{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Explainability Engine\n",
    "\n",
    "This notebook implements explainability features for the F1 Prize Picks optimization system:\n",
    "- SHAP (SHapley Additive exPlanations) analysis\n",
    "- Feature importance visualization\n",
    "- Prediction confidence analysis\n",
    "- Natural language explanations\n",
    "- Interactive dashboards for bet recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize SHAP\n",
    "shap.initjs()\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary components\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from f1db_data_loader import load_f1db_data\n",
    "\n",
    "# Load data\n",
    "print(\"Loading F1 data...\")\n",
    "f1_data = load_f1db_data(data_dir='../../data/f1db')\n",
    "\n",
    "# Try to load saved models\n",
    "try:\n",
    "    # Load fixed model\n",
    "    model_artifacts = joblib.load('f1_model_fixed_top10.pkl')\n",
    "    print(\"Loaded fixed prediction model\")\n",
    "    model = model_artifacts['model']\n",
    "    scaler = model_artifacts['scaler']\n",
    "    feature_columns = model_artifacts['feature_columns']\n",
    "except:\n",
    "    print(\"Model not found - will train a simple model for demonstration\")\n",
    "    model = None\n",
    "\n",
    "# Load optimizer configuration\n",
    "try:\n",
    "    optimizer_config = joblib.load('f1_prize_picks_optimizer.pkl')\n",
    "    print(\"Loaded Prize Picks optimizer\")\n",
    "except:\n",
    "    optimizer_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Model for Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model if none exists\n",
    "if model is None:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Prepare sample data\n",
    "    results = f1_data.get('results', pd.DataFrame())\n",
    "    races = f1_data.get('races', pd.DataFrame())\n",
    "    \n",
    "    # Merge and prepare features\n",
    "    df = results.merge(races[['raceId', 'year', 'date']], on='raceId')\n",
    "    df = df[df['year'] >= 2020]\n",
    "    \n",
    "    # Simple features\n",
    "    feature_columns = ['grid', 'laps', 'milliseconds', 'rank', 'fastestLapSpeed']\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "    \n",
    "    if len(available_features) >= 3:\n",
    "        # Prepare data\n",
    "        df_model = df.dropna(subset=available_features)\n",
    "        X = df_model[available_features]\n",
    "        y = (df_model['positionOrder'] <= 10).astype(int)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Train simple model\n",
    "        model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "        model.fit(X_scaled, y)\n",
    "        \n",
    "        feature_columns = available_features\n",
    "        print(f\"Trained simple model with features: {feature_columns}\")\n",
    "    else:\n",
    "        # Create dummy model\n",
    "        print(\"Creating dummy model for demonstration\")\n",
    "        from sklearn.datasets import make_classification\n",
    "        X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "        model.fit(X_scaled, y)\n",
    "        \n",
    "        feature_columns = [f'feature_{i}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "if hasattr(model, 'predict_proba'):\n",
    "    # Create sample data for SHAP\n",
    "    n_samples = min(100, len(X_scaled) if 'X_scaled' in locals() else 100)\n",
    "    \n",
    "    if 'X_scaled' in locals():\n",
    "        X_sample = X_scaled[:n_samples]\n",
    "    else:\n",
    "        # Create sample data\n",
    "        X_sample = np.random.randn(n_samples, len(feature_columns))\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Handle binary classification output\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Use positive class\n",
    "    \n",
    "    print(\"SHAP analysis complete\")\n",
    "    \n",
    "    # SHAP summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_columns, show=False)\n",
    "    plt.title('SHAP Feature Importance Summary')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_columns, plot_type=\"bar\", show=False)\n",
    "    plt.title('Average SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Individual Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionExplainer:\n",
    "    \"\"\"\n",
    "    Generate explanations for individual predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, model, feature_names, explainer=None):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.explainer = explainer\n",
    "    \n",
    "    def explain_prediction(self, features, driver_name=\"Driver\"):\n",
    "        \"\"\"\n",
    "        Generate comprehensive explanation for a single prediction\n",
    "        \"\"\"\n",
    "        # Get prediction\n",
    "        prob = self.model.predict_proba(features.reshape(1, -1))[0, 1]\n",
    "        \n",
    "        # Get SHAP values\n",
    "        if self.explainer:\n",
    "            shap_values = self.explainer.shap_values(features.reshape(1, -1))\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]\n",
    "            shap_values = shap_values[0]\n",
    "        else:\n",
    "            shap_values = np.zeros(len(features))\n",
    "        \n",
    "        # Create explanation\n",
    "        explanation = {\n",
    "            'driver': driver_name,\n",
    "            'probability': prob,\n",
    "            'prediction': 'Top 10' if prob > 0.5 else 'Outside Top 10',\n",
    "            'confidence': self._calculate_confidence(prob),\n",
    "            'top_factors': self._get_top_factors(features, shap_values),\n",
    "            'risk_factors': self._get_risk_factors(features, shap_values)\n",
    "        }\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _calculate_confidence(self, probability):\n",
    "        \"\"\"\n",
    "        Calculate confidence level based on probability\n",
    "        \"\"\"\n",
    "        distance_from_middle = abs(probability - 0.5)\n",
    "        if distance_from_middle > 0.3:\n",
    "            return 'High'\n",
    "        elif distance_from_middle > 0.15:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    \n",
    "    def _get_top_factors(self, features, shap_values, n=3):\n",
    "        \"\"\"\n",
    "        Get top positive contributing factors\n",
    "        \"\"\"\n",
    "        positive_indices = np.where(shap_values > 0)[0]\n",
    "        if len(positive_indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        sorted_indices = positive_indices[np.argsort(shap_values[positive_indices])[::-1]]\n",
    "        \n",
    "        factors = []\n",
    "        for idx in sorted_indices[:n]:\n",
    "            factors.append({\n",
    "                'feature': self.feature_names[idx],\n",
    "                'value': features[idx],\n",
    "                'impact': shap_values[idx]\n",
    "            })\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def _get_risk_factors(self, features, shap_values, n=3):\n",
    "        \"\"\"\n",
    "        Get top negative contributing factors\n",
    "        \"\"\"\n",
    "        negative_indices = np.where(shap_values < 0)[0]\n",
    "        if len(negative_indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        sorted_indices = negative_indices[np.argsort(shap_values[negative_indices])]\n",
    "        \n",
    "        factors = []\n",
    "        for idx in sorted_indices[:n]:\n",
    "            factors.append({\n",
    "                'feature': self.feature_names[idx],\n",
    "                'value': features[idx],\n",
    "                'impact': shap_values[idx]\n",
    "            })\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def generate_narrative(self, explanation):\n",
    "        \"\"\"\n",
    "        Generate natural language explanation\n",
    "        \"\"\"\n",
    "        narrative = f\"**{explanation['driver']} - {explanation['prediction']} Prediction**\\n\\n\"\n",
    "        narrative += f\"Probability: {explanation['probability']:.1%} (Confidence: {explanation['confidence']})\\n\\n\"\n",
    "        \n",
    "        if explanation['top_factors']:\n",
    "            narrative += \"**Key Success Factors:**\\n\"\n",
    "            for factor in explanation['top_factors']:\n",
    "                narrative += f\"- {factor['feature']}: {factor['value']:.2f} \"\n",
    "                narrative += f\"(+{factor['impact']:.3f} impact)\\n\"\n",
    "        \n",
    "        if explanation['risk_factors']:\n",
    "            narrative += \"\\n**Risk Factors:**\\n\"\n",
    "            for factor in explanation['risk_factors']:\n",
    "                narrative += f\"- {factor['feature']}: {factor['value']:.2f} \"\n",
    "                narrative += f\"({factor['impact']:.3f} impact)\\n\"\n",
    "        \n",
    "        return narrative\n",
    "\n",
    "# Create explainer\n",
    "pred_explainer = PredictionExplainer(\n",
    "    model, \n",
    "    feature_columns,\n",
    "    explainer if 'explainer' in locals() else None\n",
    ")\n",
    "\n",
    "# Generate example explanations\n",
    "print(\"\\nExample Prediction Explanations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample predictions\n",
    "sample_drivers = ['Verstappen', 'Hamilton', 'Leclerc']\n",
    "for i, driver in enumerate(sample_drivers):\n",
    "    # Generate sample features\n",
    "    if 'X_sample' in locals() and i < len(X_sample):\n",
    "        features = X_sample[i]\n",
    "    else:\n",
    "        features = np.random.randn(len(feature_columns))\n",
    "    \n",
    "    # Get explanation\n",
    "    explanation = pred_explainer.explain_prediction(features, driver)\n",
    "    narrative = pred_explainer.generate_narrative(explanation)\n",
    "    \n",
    "    print(narrative)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_explanation(explanation, shap_values=None):\n",
    "    \"\"\"\n",
    "    Create interactive Plotly visualization for prediction explanation\n",
    "    \"\"\"\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Prediction Probability', 'Feature Contributions',\n",
    "                       'Confidence Gauge', 'Risk Assessment'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'indicator'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Prediction probability bar\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=['Not Top 10', 'Top 10'],\n",
    "            y=[1 - explanation['probability'], explanation['probability']],\n",
    "            marker_color=['red', 'green'],\n",
    "            text=[f\"{(1-explanation['probability']):.1%}\", f\"{explanation['probability']:.1%}\"],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Feature contributions\n",
    "    all_factors = explanation['top_factors'] + explanation['risk_factors']\n",
    "    if all_factors:\n",
    "        features = [f['feature'] for f in all_factors]\n",
    "        impacts = [f['impact'] for f in all_factors]\n",
    "        colors = ['green' if i > 0 else 'red' for i in impacts]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=impacts,\n",
    "                y=features,\n",
    "                orientation='h',\n",
    "                marker_color=colors\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Confidence gauge\n",
    "    confidence_value = {'High': 0.9, 'Medium': 0.6, 'Low': 0.3}[explanation['confidence']]\n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number\",\n",
    "            value=confidence_value,\n",
    "            title={'text': \"Confidence Level\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [0, 1]},\n",
    "                'bar': {'color': \"darkblue\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 0.33], 'color': \"lightgray\"},\n",
    "                    {'range': [0.33, 0.67], 'color': \"gray\"},\n",
    "                    {'range': [0.67, 1], 'color': \"lightgreen\"}\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 0.5\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Risk vs Reward scatter\n",
    "    risk_score = len(explanation['risk_factors']) / (len(explanation['risk_factors']) + len(explanation['top_factors']) + 1)\n",
    "    reward_score = explanation['probability']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[risk_score],\n",
    "            y=[reward_score],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=20, color='blue'),\n",
    "            text=[explanation['driver']],\n",
    "            textposition=\"top center\"\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{explanation['driver']} - Prize Picks Analysis\",\n",
    "        showlegend=False,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Probability\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Impact\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Risk Score\", range=[0, 1], row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Reward (Win Probability)\", range=[0, 1], row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive visualization for first driver\n",
    "if 'X_sample' in locals() and len(X_sample) > 0:\n",
    "    features = X_sample[0]\n",
    "else:\n",
    "    features = np.random.randn(len(feature_columns))\n",
    "\n",
    "explanation = pred_explainer.explain_prediction(features, \"Verstappen\")\n",
    "fig = create_interactive_explanation(explanation)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prize Picks Recommendation Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrizePicksExplainer:\n",
    "    \"\"\"\n",
    "    Explain Prize Picks optimization decisions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.bet_type_descriptions = {\n",
    "            'top_10': 'Finish in top 10 positions',\n",
    "            'top_5': 'Finish in top 5 positions',\n",
    "            'top_3': 'Finish on the podium',\n",
    "            'points': 'Score championship points',\n",
    "            'beat_teammate': 'Finish ahead of teammate',\n",
    "            'h2h': 'Head-to-head matchup',\n",
    "            'dnf': 'Did not finish (DNF)'\n",
    "        }\n",
    "    \n",
    "    def explain_parlay(self, parlay):\n",
    "        \"\"\"\n",
    "        Generate comprehensive explanation for a parlay\n",
    "        \"\"\"\n",
    "        explanation = {\n",
    "            'summary': self._generate_summary(parlay),\n",
    "            'picks_analysis': self._analyze_picks(parlay),\n",
    "            'correlation_analysis': self._analyze_correlation(parlay),\n",
    "            'risk_assessment': self._assess_risk(parlay),\n",
    "            'value_proposition': self._analyze_value(parlay)\n",
    "        }\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _generate_summary(self, parlay):\n",
    "        \"\"\"\n",
    "        Generate executive summary\n",
    "        \"\"\"\n",
    "        summary = f\"**{parlay['n_picks']}-Pick Parlay**\\n\\n\"\n",
    "        summary += f\"- **Bet Amount**: ${parlay['bet_size']:.2f}\\n\"\n",
    "        summary += f\"- **Potential Payout**: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\\n\"\n",
    "        summary += f\"- **Win Probability**: {parlay['adjusted_prob']:.1%}\\n\"\n",
    "        summary += f\"- **Expected Value**: ${parlay['expected_value'] * parlay['bet_size']:.2f}\\n\"\n",
    "        summary += f\"- **Kelly Stake**: {parlay['kelly_stake']:.1%} of bankroll\\n\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _analyze_picks(self, parlay):\n",
    "        \"\"\"\n",
    "        Analyze individual picks\n",
    "        \"\"\"\n",
    "        analysis = \"**Individual Pick Analysis:**\\n\\n\"\n",
    "        \n",
    "        for i, pick in enumerate(parlay['picks'], 1):\n",
    "            analysis += f\"**Pick {i}: {pick['driver']} - {self.bet_type_descriptions.get(pick['bet_type'], pick['bet_type'])}**\\n\"\n",
    "            analysis += f\"- True Probability: {pick['true_prob']:.1%}\\n\"\n",
    "            analysis += f\"- Implied Probability: {pick['implied_prob']:.1%}\\n\"\n",
    "            analysis += f\"- Edge: +{pick['edge']:.1%}\\n\"\n",
    "            analysis += f\"- Confidence: {pick['confidence']:.1%}\\n\\n\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_correlation(self, parlay):\n",
    "        \"\"\"\n",
    "        Analyze correlation between picks\n",
    "        \"\"\"\n",
    "        analysis = \"**Correlation Analysis:**\\n\\n\"\n",
    "        \n",
    "        correlation = parlay.get('correlation', 0)\n",
    "        if correlation < 0.3:\n",
    "            risk_level = \"Low\"\n",
    "            color = \"green\"\n",
    "        elif correlation < 0.6:\n",
    "            risk_level = \"Medium\"\n",
    "            color = \"yellow\"\n",
    "        else:\n",
    "            risk_level = \"High\"\n",
    "            color = \"red\"\n",
    "        \n",
    "        analysis += f\"- Overall Correlation: {correlation:.2f} ({risk_level} risk)\\n\"\n",
    "        analysis += f\"- Diversification: {'Good' if correlation < 0.3 else 'Could be improved'}\\n\"\n",
    "        \n",
    "        # Check for same driver multiple times\n",
    "        drivers = [p['driver'] for p in parlay['picks']]\n",
    "        if len(set(drivers)) < len(drivers):\n",
    "            analysis += \"- \u26a0\ufe0f Multiple bets on same driver increases correlation risk\\n\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _assess_risk(self, parlay):\n",
    "        \"\"\"\n",
    "        Assess risk factors\n",
    "        \"\"\"\n",
    "        analysis = \"**Risk Assessment:**\\n\\n\"\n",
    "        \n",
    "        # Calculate risk score\n",
    "        risk_factors = []\n",
    "        \n",
    "        # Parlay size risk\n",
    "        if parlay['n_picks'] >= 4:\n",
    "            risk_factors.append(\"Large parlay size (4+ picks) significantly reduces win probability\")\n",
    "        \n",
    "        # Low probability picks\n",
    "        low_prob_picks = [p for p in parlay['picks'] if p['true_prob'] < 0.5]\n",
    "        if len(low_prob_picks) > 0:\n",
    "            risk_factors.append(f\"{len(low_prob_picks)} picks have <50% probability\")\n",
    "        \n",
    "        # Correlation risk\n",
    "        if parlay.get('correlation', 0) > 0.5:\n",
    "            risk_factors.append(\"High correlation between picks\")\n",
    "        \n",
    "        if risk_factors:\n",
    "            analysis += \"**Risk Factors:**\\n\"\n",
    "            for factor in risk_factors:\n",
    "                analysis += f\"- {factor}\\n\"\n",
    "        else:\n",
    "            analysis += \"\u2705 No major risk factors identified\\n\"\n",
    "        \n",
    "        # Overall risk rating\n",
    "        risk_score = len(risk_factors) / 3\n",
    "        if risk_score < 0.33:\n",
    "            risk_rating = \"Low Risk\"\n",
    "        elif risk_score < 0.67:\n",
    "            risk_rating = \"Medium Risk\"\n",
    "        else:\n",
    "            risk_rating = \"High Risk\"\n",
    "        \n",
    "        analysis += f\"\\n**Overall Risk Rating**: {risk_rating}\\n\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_value(self, parlay):\n",
    "        \"\"\"\n",
    "        Analyze value proposition\n",
    "        \"\"\"\n",
    "        analysis = \"**Value Analysis:**\\n\\n\"\n",
    "        \n",
    "        ev = parlay['expected_value']\n",
    "        roi = (ev / 1) * 100  # ROI percentage\n",
    "        \n",
    "        analysis += f\"- Expected ROI: {roi:.1f}%\\n\"\n",
    "        analysis += f\"- Breakeven Win Rate: {1/parlay['payout']:.1%}\\n\"\n",
    "        analysis += f\"- Your Win Rate: {parlay['adjusted_prob']:.1%}\\n\"\n",
    "        analysis += f\"- Edge over Breakeven: +{(parlay['adjusted_prob'] - 1/parlay['payout']):.1%}\\n\"\n",
    "        \n",
    "        if ev > 0:\n",
    "            analysis += \"\\n\u2705 **Positive Expected Value - Recommended Bet**\\n\"\n",
    "        else:\n",
    "            analysis += \"\\n\u274c **Negative Expected Value - Not Recommended**\\n\"\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Create Prize Picks explainer\n",
    "pp_explainer = PrizePicksExplainer()\n",
    "\n",
    "# Example parlay explanation\n",
    "example_parlay = {\n",
    "    'n_picks': 3,\n",
    "    'bet_size': 50,\n",
    "    'payout': 6,\n",
    "    'adjusted_prob': 0.22,\n",
    "    'expected_value': 0.32,\n",
    "    'kelly_stake': 0.08,\n",
    "    'correlation': 0.35,\n",
    "    'picks': [\n",
    "        {'driver': 'Verstappen', 'bet_type': 'top_3', 'true_prob': 0.65, 'implied_prob': 0.50, 'edge': 0.15, 'confidence': 0.85},\n",
    "        {'driver': 'Hamilton', 'bet_type': 'top_5', 'true_prob': 0.70, 'implied_prob': 0.60, 'edge': 0.10, 'confidence': 0.80},\n",
    "        {'driver': 'Leclerc', 'bet_type': 'beat_teammate', 'true_prob': 0.55, 'implied_prob': 0.50, 'edge': 0.05, 'confidence': 0.70}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate explanation\n",
    "explanation = pp_explainer.explain_parlay(example_parlay)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRIZE PICKS PARLAY EXPLANATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for section, content in explanation.items():\n",
    "    print(f\"\\n{content}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_confidence(predictions, actuals=None):\n",
    "    \"\"\"\n",
    "    Analyze prediction confidence and calibration\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Confidence distribution\n",
    "    ax = axes[0, 0]\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        probs = predictions\n",
    "    else:\n",
    "        probs = predictions['probability'] if 'probability' in predictions else np.random.random(100)\n",
    "    \n",
    "    ax.hist(probs, bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=0.5, color='red', linestyle='--', label='Decision threshold')\n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Prediction Confidence Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Calibration plot (if actuals provided)\n",
    "    ax = axes[0, 1]\n",
    "    if actuals is not None:\n",
    "        from sklearn.calibration import calibration_curve\n",
    "        fraction_pos, mean_pred = calibration_curve(actuals, probs, n_bins=10)\n",
    "        \n",
    "        ax.plot(mean_pred, fraction_pos, marker='o', label='Model')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "        ax.set_xlabel('Mean Predicted Probability')\n",
    "        ax.set_ylabel('Fraction of Positives')\n",
    "        ax.set_title('Calibration Plot')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Simulated calibration\n",
    "        ax.text(0.5, 0.5, 'Calibration plot requires actual outcomes', \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Calibration Plot (No Actuals Available)')\n",
    "    \n",
    "    # 3. Confidence vs Edge\n",
    "    ax = axes[1, 0]\n",
    "    # Simulate edge data\n",
    "    edges = (probs - 0.5) * 0.3  # Simulated edges\n",
    "    confidence = np.abs(probs - 0.5) * 2  # Distance from 0.5\n",
    "    \n",
    "    scatter = ax.scatter(edges, confidence, c=probs, cmap='RdYlGn', alpha=0.6)\n",
    "    ax.set_xlabel('Edge (True Prob - Implied Prob)')\n",
    "    ax.set_ylabel('Confidence Score')\n",
    "    ax.set_title('Confidence vs Betting Edge')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Probability')\n",
    "    \n",
    "    # 4. Confidence zones\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Define confidence zones\n",
    "    high_conf = probs[(probs > 0.7) | (probs < 0.3)]\n",
    "    med_conf = probs[(probs >= 0.3) & (probs <= 0.7) & ((probs < 0.4) | (probs > 0.6))]\n",
    "    low_conf = probs[(probs >= 0.4) & (probs <= 0.6)]\n",
    "    \n",
    "    conf_data = [len(high_conf), len(med_conf), len(low_conf)]\n",
    "    labels = ['High\\nConfidence', 'Medium\\nConfidence', 'Low\\nConfidence']\n",
    "    colors = ['green', 'yellow', 'red']\n",
    "    \n",
    "    ax.pie(conf_data, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "    ax.set_title('Prediction Confidence Zones')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print confidence statistics\n",
    "    print(\"\\nConfidence Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"High Confidence Predictions: {len(high_conf)} ({len(high_conf)/len(probs):.1%})\")\n",
    "    print(f\"Medium Confidence Predictions: {len(med_conf)} ({len(med_conf)/len(probs):.1%})\")\n",
    "    print(f\"Low Confidence Predictions: {len(low_conf)} ({len(low_conf)/len(probs):.1%})\")\n",
    "    print(f\"\\nMean Confidence: {np.mean(np.abs(probs - 0.5) * 2):.3f}\")\n",
    "    print(f\"Confidence Std Dev: {np.std(np.abs(probs - 0.5) * 2):.3f}\")\n",
    "\n",
    "# Analyze confidence for sample predictions\n",
    "sample_predictions = np.random.beta(2, 2, 200)  # Beta distribution for realistic probabilities\n",
    "analyze_prediction_confidence(sample_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_dashboard(model_metrics, feature_importance):\n",
    "    \"\"\"\n",
    "    Create comprehensive performance dashboard\n",
    "    \"\"\"\n",
    "    # Create subplot figure\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Model Accuracy Over Time', 'Feature Importance',\n",
    "                       'Prediction Distribution', 'ROI by Confidence',\n",
    "                       'Win Rate by Parlay Size', 'Risk-Adjusted Returns'),\n",
    "        specs=[[{'type': 'scatter'}, {'type': 'bar'}],\n",
    "               [{'type': 'histogram'}, {'type': 'scatter'}],\n",
    "               [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Model accuracy over time\n",
    "    dates = pd.date_range(start='2023-01-01', periods=20, freq='M')\n",
    "    accuracy = 0.65 + np.random.normal(0, 0.05, 20).cumsum() * 0.01\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dates, y=accuracy, mode='lines+markers', name='Accuracy'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Feature importance\n",
    "    features = ['Grid Position', 'Recent Form', 'Track History', 'Team Performance', 'Age Factor']\n",
    "    importance = [0.25, 0.20, 0.18, 0.15, 0.12]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=importance, y=features, orientation='h', marker_color='lightblue'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Prediction distribution\n",
    "    predictions = np.random.beta(2, 2, 500)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=predictions, nbinsx=20, marker_color='green'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. ROI by confidence\n",
    "    confidence_levels = np.linspace(0.5, 0.9, 20)\n",
    "    roi = -0.2 + confidence_levels * 0.5 + np.random.normal(0, 0.05, 20)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=confidence_levels, y=roi, mode='markers',\n",
    "                  marker=dict(size=10, color=roi, colorscale='RdYlGn')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Win rate by parlay size\n",
    "    parlay_sizes = [2, 3, 4, 5, 6]\n",
    "    win_rates = [0.45, 0.25, 0.15, 0.08, 0.04]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=parlay_sizes, y=win_rates, marker_color='purple'),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Risk-adjusted returns\n",
    "    risk = np.linspace(0, 1, 20)\n",
    "    returns = 0.3 - risk * 0.5 + np.random.normal(0, 0.05, 20)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=risk, y=returns, mode='markers+lines',\n",
    "                  marker=dict(size=8, color='blue')),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"F1 Prize Picks Model Performance Dashboard\",\n",
    "        showlegend=False,\n",
    "        height=1200\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Importance\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Probability\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Confidence\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Parlay Size\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Risk Level\", row=3, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"ROI\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Win Rate\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Returns\", row=3, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display dashboard\n",
    "dashboard = create_performance_dashboard(None, None)\n",
    "dashboard.show()\n",
    "\n",
    "print(\"\\nPerformance dashboard created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Explainability Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save explainability components\n",
    "explainability_components = {\n",
    "    'prediction_explainer': pred_explainer,\n",
    "    'prize_picks_explainer': pp_explainer,\n",
    "    'feature_importance': feature_columns if 'feature_columns' in locals() else [],\n",
    "    'shap_explainer': explainer if 'explainer' in locals() else None,\n",
    "    'metadata': {\n",
    "        'created_date': datetime.now().isoformat(),\n",
    "        'model_type': type(model).__name__ if model else 'None',\n",
    "        'n_features': len(feature_columns) if 'feature_columns' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(explainability_components, 'f1_explainability_engine.pkl')\n",
    "print(\"\\nExplainability engine saved successfully!\")\n",
    "print(f\"Components: {list(explainability_components.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The F1 Explainability Engine provides:\n",
    "\n",
    "1. **SHAP Analysis**: Feature importance and impact visualization\n",
    "2. **Individual Explanations**: Detailed breakdowns for each prediction\n",
    "3. **Natural Language**: Human-readable explanations\n",
    "4. **Interactive Visualizations**: Plotly dashboards\n",
    "5. **Confidence Analysis**: Calibration and reliability metrics\n",
    "\n",
    "### Key Features:\n",
    "- Explains why specific drivers are recommended\n",
    "- Shows which features drive predictions\n",
    "- Analyzes risk factors and correlations\n",
    "- Provides confidence levels for each bet\n",
    "- Generates comprehensive parlay explanations\n",
    "\n",
    "### Usage:\n",
    "- Use SHAP values to understand model behavior\n",
    "- Generate explanations for user trust\n",
    "- Monitor prediction confidence\n",
    "- Identify high-risk recommendations\n",
    "- Debug unexpected predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}