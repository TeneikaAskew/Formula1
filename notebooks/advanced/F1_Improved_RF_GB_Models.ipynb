{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved F1 Random Forest and Gradient Boosting Models\n",
    "\n",
    "This notebook improves upon the original Random Forest and Gradient Boosting models with:\n",
    "- Top 5 predictions alongside Top 3 and Top 10\n",
    "- Race name validation in outputs\n",
    "- Comprehensive overfitting prevention\n",
    "- Temporal validation splits\n",
    "- Better feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n# Remove imblearn import if not available - we'll implement SMOTE alternative if needed\ntry:\n    from imblearn.over_sampling import SMOTE\n    SMOTE_AVAILABLE = True\nexcept ImportError:\n    print(\"Warning: imblearn not available. SMOTE functionality will be disabled.\")\n    SMOTE_AVAILABLE = False\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\n# Load data using the correct paths\ndata_path = '/app/data/f1db/'\n\nprint(\"Loading F1 data...\")\ndrivers = pd.read_csv(f'{data_path}drivers.csv')\nqualifying = pd.read_csv(f'{data_path}qualifying.csv')\nraces = pd.read_csv(f'{data_path}races.csv')\nlap_times = pd.read_csv(f'{data_path}lap_times.csv')\npit_stop_times = pd.read_csv(f'{data_path}pit_stops.csv')\ndriver_standings = pd.read_csv(f'{data_path}driver_standings.csv')\nresults = pd.read_csv(f'{data_path}results.csv')\nstatus = pd.read_csv(f'{data_path}status.csv')\nconstructor_results = pd.read_csv(f'{data_path}constructor_results.csv')\nconstructor_standings = pd.read_csv(f'{data_path}constructor_standings.csv')\n\nprint(f\"Data loaded successfully!\")\nprint(f\"Total races: {len(races)}\")\nprint(f\"Date range: {races['date'].min()} to {races['date'].max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation with Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify drivers who finished the races\n",
    "finished_status_ids = status[status['status'].str.lower().isin(['finished', '+1 lap', '+2 laps', '+3 laps', '+4 laps', '+5 laps'])]['statusId'].unique()\n",
    "finished_results = results[results['statusId'].isin(finished_status_ids)]\n",
    "\n",
    "# Merge races and results to get race year and date\n",
    "results = results.merge(races[['raceId', 'year', 'date', 'name', 'round']], on='raceId', how='left')\n",
    "\n",
    "# Convert date to datetime\n",
    "results['date'] = pd.to_datetime(results['date'])\n",
    "races['date'] = pd.to_datetime(races['date'])\n",
    "\n",
    "# Sort by date for temporal consistency\n",
    "results = results.sort_values('date')\n",
    "\n",
    "# Create target variables\n",
    "results['top_10'] = results['positionOrder'] <= 10\n",
    "results['top_5'] = results['positionOrder'] <= 5  # NEW: Top 5 predictions\n",
    "results['top_3'] = results['positionOrder'] <= 3\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"Top 10 finishes: {results['top_10'].sum()} ({results['top_10'].mean()*100:.1f}%)\")\n",
    "print(f\"Top 5 finishes: {results['top_5'].sum()} ({results['top_5'].mean()*100:.1f}%)\")\n",
    "print(f\"Top 3 finishes: {results['top_3'].sum()} ({results['top_3'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering with Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, window_sizes=[3, 5, 10]):\n",
    "    \"\"\"Create rolling features to capture recent form\"\"\"\n",
    "    df = df.sort_values(['driverId', 'date'])\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        # Rolling average position\n",
    "        df[f'avg_position_last_{window}'] = df.groupby('driverId')['positionOrder'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling points\n",
    "        df[f'avg_points_last_{window}'] = df.groupby('driverId')['points'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling DNF rate\n",
    "        df['dnf'] = ~df['statusId'].isin(finished_status_ids)\n",
    "        df[f'dnf_rate_last_{window}'] = df.groupby('driverId')['dnf'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply rolling features\n",
    "results = create_rolling_features(results)\n",
    "\n",
    "# Calculate seasonal performance (with shift to prevent leakage)\n",
    "seasonal_performance = results.groupby(['driverId', 'year'])['positionOrder'].transform('mean')\n",
    "results['avg_position_per_season'] = results.groupby('driverId')['positionOrder'].shift(1)\n",
    "\n",
    "# Calculate qualifying features\n",
    "def process_qualifying_times(qual_str):\n",
    "    \"\"\"Convert qualifying time string to seconds\"\"\"\n",
    "    if pd.isna(qual_str) or qual_str == '\\\\N' or qual_str == '':\n",
    "        return np.nan\n",
    "    try:\n",
    "        parts = qual_str.split(':')\n",
    "        if len(parts) == 2:\n",
    "            return int(parts[0]) * 60 + float(parts[1])\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Process qualifying times\n",
    "for col in ['q1', 'q2', 'q3']:\n",
    "    qualifying[col] = qualifying[col].apply(process_qualifying_times)\n",
    "\n",
    "# Best qualifying time\n",
    "qualifying['best_quali_time'] = qualifying[['q1', 'q2', 'q3']].min(axis=1)\n",
    "\n",
    "# Merge qualifying data\n",
    "results = results.merge(\n",
    "    qualifying[['raceId', 'driverId', 'position', 'best_quali_time']],\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left',\n",
    "    suffixes=('', '_quali')\n",
    ")\n",
    "\n",
    "# Constructor features\n",
    "constructor_avg_points = constructor_results.groupby('constructorId')['points'].mean().reset_index()\n",
    "constructor_avg_points.columns = ['constructorId', 'avg_constructor_points']\n",
    "results = results.merge(constructor_avg_points, on='constructorId', how='left')\n",
    "\n",
    "# Track-specific features\n",
    "track_performance = results.groupby(['driverId', 'name'])['positionOrder'].agg(['mean', 'std']).reset_index()\n",
    "track_performance.columns = ['driverId', 'track_name', 'avg_position_at_track', 'std_position_at_track']\n",
    "results = results.merge(\n",
    "    track_performance,\n",
    "    left_on=['driverId', 'name'],\n",
    "    right_on=['driverId', 'track_name'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"Total features created: {len([col for col in results.columns if col not in ['raceId', 'driverId', 'date']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Train-Test Split for Overfitting Prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only recent data (2018 onwards) for better relevance\n",
    "recent_results = results[results['year'] >= 2018].copy()\n",
    "\n",
    "# Define temporal split points\n",
    "train_end_date = pd.Timestamp('2022-12-31')\n",
    "val_end_date = pd.Timestamp('2023-12-31')\n",
    "\n",
    "# Split data temporally\n",
    "train_data = recent_results[recent_results['date'] <= train_end_date]\n",
    "val_data = recent_results[(recent_results['date'] > train_end_date) & (recent_results['date'] <= val_end_date)]\n",
    "test_data = recent_results[recent_results['date'] > val_end_date]\n",
    "\n",
    "print(f\"\\nTemporal split:\")\n",
    "print(f\"Train: {train_data['date'].min()} to {train_data['date'].max()} ({len(train_data)} samples)\")\n",
    "print(f\"Val: {val_data['date'].min()} to {val_data['date'].max()} ({len(val_data)} samples)\")\n",
    "print(f\"Test: {test_data['date'].min()} to {test_data['date'].max()} ({len(test_data)} samples)\")\n",
    "\n",
    "# Feature selection\n",
    "feature_cols = [\n",
    "    'grid', 'position_quali', 'avg_constructor_points',\n",
    "    'avg_position_last_3', 'avg_position_last_5', 'avg_position_last_10',\n",
    "    'avg_points_last_3', 'avg_points_last_5', 'avg_points_last_10',\n",
    "    'dnf_rate_last_3', 'dnf_rate_last_5', 'dnf_rate_last_10',\n",
    "    'avg_position_at_track', 'std_position_at_track'\n",
    "]\n",
    "\n",
    "# Remove features with too many NaN values\n",
    "feature_cols = [col for col in feature_cols if col in recent_results.columns]\n",
    "\n",
    "# Prepare features and targets\n",
    "def prepare_data(df, feature_cols):\n",
    "    X = df[feature_cols].fillna(df[feature_cols].mean())\n",
    "    y_top_10 = df['top_10']\n",
    "    y_top_5 = df['top_5']\n",
    "    y_top_3 = df['top_3']\n",
    "    return X, y_top_10, y_top_5, y_top_3\n",
    "\n",
    "X_train, y_train_top_10, y_train_top_5, y_train_top_3 = prepare_data(train_data, feature_cols)\n",
    "X_val, y_val_top_10, y_val_top_5, y_val_top_3 = prepare_data(val_data, feature_cols)\n",
    "X_test, y_test_top_10, y_test_top_5, y_test_top_3 = prepare_data(test_data, feature_cols)\n",
    "\n",
    "print(f\"\\nFeatures used: {feature_cols}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regularized models to prevent overfitting\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,  # Shallow trees\n",
    "    'min_samples_split': 50,  # Require many samples to split\n",
    "    'min_samples_leaf': 20,   # Require many samples in leaves\n",
    "    'max_features': 'sqrt',   # Use subset of features\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.05,    # Small learning rate\n",
    "    'max_depth': 4,           # Very shallow trees\n",
    "    'min_samples_split': 50,\n",
    "    'min_samples_leaf': 20,\n",
    "    'subsample': 0.8,         # Use subset of data\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "gb_model = GradientBoostingClassifier(**gb_params)\n",
    "\n",
    "# Train models for each target\n",
    "print(\"Training models...\")\n",
    "\n",
    "models = {}\n",
    "for target_name, y_train, y_val in [\n",
    "    ('top_10', y_train_top_10, y_val_top_10),\n",
    "    ('top_5', y_train_top_5, y_val_top_5),\n",
    "    ('top_3', y_train_top_3, y_val_top_3)\n",
    "]:\n",
    "    print(f\"\\nTraining {target_name} models...\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Train Gradient Boosting\n",
    "    gb = GradientBoostingClassifier(**gb_params)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    # Ensemble\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[('rf', rf), ('gb', gb)],\n",
    "        voting='soft'\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_pred = ensemble.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    val_prec = precision_score(y_val, val_pred)\n",
    "    val_rec = recall_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred)\n",
    "    \n",
    "    print(f\"{target_name} Validation Metrics:\")\n",
    "    print(f\"  Accuracy: {val_acc:.3f}\")\n",
    "    print(f\"  Precision: {val_prec:.3f}\")\n",
    "    print(f\"  Recall: {val_rec:.3f}\")\n",
    "    print(f\"  F1: {val_f1:.3f}\")\n",
    "    \n",
    "    models[target_name] = ensemble\n",
    "    \n",
    "    # Check for overfitting\n",
    "    train_pred = ensemble.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
    "    print(f\"  Overfitting ratio: {train_acc/val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Tests and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_analysis(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"Comprehensive overfitting analysis\"\"\"\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    train_auc = roc_auc_score(y_train, train_proba)\n",
    "    val_auc = roc_auc_score(y_val, val_proba)\n",
    "    \n",
    "    # Plot learning curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # ROC curves\n",
    "    ax = axes[0]\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, train_proba)\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val, val_proba)\n",
    "    ax.plot(fpr_train, tpr_train, label=f'Train (AUC={train_auc:.3f})')\n",
    "    ax.plot(fpr_val, tpr_val, label=f'Val (AUC={val_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'{model_name} ROC Curves')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Confusion matrices\n",
    "    ax = axes[1]\n",
    "    cm = confusion_matrix(y_val, val_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'{model_name} Validation Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print overfitting metrics\n",
    "    print(f\"\\n{model_name} Overfitting Analysis:\")\n",
    "    print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.3f}\")\n",
    "    print(f\"Accuracy Gap: {train_acc - val_acc:.3f}\")\n",
    "    print(f\"Train AUC: {train_auc:.3f}\")\n",
    "    print(f\"Val AUC: {val_auc:.3f}\")\n",
    "    print(f\"AUC Gap: {train_auc - val_auc:.3f}\")\n",
    "    \n",
    "    if train_acc - val_acc > 0.1:\n",
    "        print(\"⚠️ WARNING: Significant overfitting detected!\")\n",
    "    elif train_acc - val_acc > 0.05:\n",
    "        print(\"⚠️ WARNING: Moderate overfitting detected!\")\n",
    "    else:\n",
    "        print(\"✅ Model shows good generalization\")\n",
    "    \n",
    "    return train_acc, val_acc, train_auc, val_auc\n",
    "\n",
    "# Analyze overfitting for each model\n",
    "overfitting_results = {}\n",
    "for target_name in ['top_10', 'top_5', 'top_3']:\n",
    "    y_train = locals()[f'y_train_{target_name}']\n",
    "    y_val = locals()[f'y_val_{target_name}']\n",
    "    \n",
    "    results = overfitting_analysis(\n",
    "        models[target_name], X_train, y_train, X_val, y_val, \n",
    "        f'{target_name.upper()} Model'\n",
    "    )\n",
    "    overfitting_results[target_name] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for Next Race with Race Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get the next race information\nlatest_race_date = races['date'].max()\nnext_race = races[races['date'] > latest_race_date].iloc[0] if len(races[races['date'] > latest_race_date]) > 0 else races.iloc[-1]\nnext_race_name = next_race['name']\nnext_race_year = next_race['year']\nnext_race_round = next_race['round']\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PREDICTIONS FOR NEXT RACE: {next_race_name} {next_race_year}\")\nprint(f\"Round {next_race_round} - Scheduled for {next_race['date']}\")\nprint(f\"{'='*60}\\n\")\n\n# Get current drivers dynamically from recent results\n# Find drivers who have participated in recent races\nrecent_year = results['year'].max()\nrecent_races = results[results['year'] >= recent_year - 1]  # Last 2 years\n\n# Get active drivers based on recent participation\nactive_driver_ids = recent_races.groupby('driverId').size()\nactive_driver_ids = active_driver_ids[active_driver_ids >= 5].index  # At least 5 races\n\n# Get driver information\ndriver_info = drivers[drivers['driverId'].isin(active_driver_ids)][['driverId', 'surname', 'forename']]\n\n# If we don't have enough active drivers, get the most recent participants\nif len(driver_info) < 10:\n    print(f\"Found only {len(driver_info)} active drivers, expanding search...\")\n    # Get drivers from the most recent races\n    latest_races = results.nlargest(200, 'date')  # Last ~10 races\n    active_driver_ids = latest_races['driverId'].value_counts().head(20).index\n    driver_info = drivers[drivers['driverId'].isin(active_driver_ids)][['driverId', 'surname', 'forename']]\n\nprint(f\"Found {len(driver_info)} active drivers for predictions\")\n\n# Prepare features for current drivers\npredictions_data = []\n\nfor _, driver in driver_info.iterrows():\n    driver_id = driver['driverId']\n    driver_name = driver['surname']\n    \n    # Get latest features for this driver\n    driver_results = recent_results[recent_results['driverId'] == driver_id]\n    \n    if len(driver_results) > 0:\n        # Get most recent data\n        # Fix: Convert pandas Series to numpy array properly\n        latest_row = driver_results.iloc[-1]\n        # Extract only the feature columns and convert to numpy array\n        latest_features_values = latest_row[feature_cols].values\n        latest_features = latest_features_values.reshape(1, -1)\n        \n        # Fill NaN values\n        latest_features = pd.DataFrame(latest_features, columns=feature_cols).fillna(X_train[feature_cols].mean())\n        \n        # Make predictions\n        pred_top_10 = models['top_10'].predict(latest_features)[0]\n        pred_top_5 = models['top_5'].predict(latest_features)[0]\n        pred_top_3 = models['top_3'].predict(latest_features)[0]\n        \n        # Get probabilities\n        prob_top_10 = models['top_10'].predict_proba(latest_features)[0][1]\n        prob_top_5 = models['top_5'].predict_proba(latest_features)[0][1]\n        prob_top_3 = models['top_3'].predict_proba(latest_features)[0][1]\n        \n        predictions_data.append({\n            'Driver': driver_name,\n            'Top_10': pred_top_10,\n            'Top_10_Prob': prob_top_10,\n            'Top_5': pred_top_5,\n            'Top_5_Prob': prob_top_5,\n            'Top_3': pred_top_3,\n            'Top_3_Prob': prob_top_3\n        })\n\n# Create predictions DataFrame\npredictions_df = pd.DataFrame(predictions_data)\n\nif len(predictions_df) == 0:\n    print(\"No predictions available - insufficient driver data\")\nelse:\n    # Sort by probability for better display\n    predictions_df = predictions_df.sort_values('Top_3_Prob', ascending=False)\n    \n    # Display predictions\n    print(\"\\n🏎️ TOP 10 PREDICTIONS:\")\n    print(\"-\" * 40)\n    for _, row in predictions_df.iterrows():\n        # Fix boolean comparison for pandas Series\n        emoji = \"✅\" if bool(row['Top_10']) else \"❌\"\n        print(f\"{emoji} {row['Driver']:15} - {row['Top_10']} (Confidence: {row['Top_10_Prob']:.1%})\")\n    \n    print(\"\\n🏎️ TOP 5 PREDICTIONS:\")\n    print(\"-\" * 40)\n    for _, row in predictions_df.iterrows():\n        # Fix boolean comparison for pandas Series\n        emoji = \"✅\" if bool(row['Top_5']) else \"❌\"\n        print(f\"{emoji} {row['Driver']:15} - {row['Top_5']} (Confidence: {row['Top_5_Prob']:.1%})\")\n    \n    print(\"\\n🏎️ TOP 3 (PODIUM) PREDICTIONS:\")\n    print(\"-\" * 40)\n    for _, row in predictions_df.iterrows():\n        # Fix boolean comparison for pandas Series\n        emoji = \"✅\" if bool(row['Top_3']) else \"❌\"\n        print(f\"{emoji} {row['Driver']:15} - {row['Top_3']} (Confidence: {row['Top_3_Prob']:.1%})\")\n    \n    # Summary statistics\n    print(f\"\\n📊 SUMMARY FOR {next_race_name}:\")\n    print(f\"Drivers predicted to finish Top 10: {predictions_df['Top_10'].sum()}\")\n    print(f\"Drivers predicted to finish Top 5: {predictions_df['Top_5'].sum()}\")\n    print(f\"Drivers predicted to finish Top 3: {predictions_df['Top_3'].sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from ensemble models\n",
    "def get_ensemble_importance(ensemble_model, feature_names):\n",
    "    \"\"\"Get averaged feature importance from ensemble\"\"\"\n",
    "    rf_importance = ensemble_model.estimators_[0][1].feature_importances_\n",
    "    gb_importance = ensemble_model.estimators_[1][1].feature_importances_\n",
    "    \n",
    "    # Average the importances\n",
    "    avg_importance = (rf_importance + gb_importance) / 2\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': avg_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Plot feature importance for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (target_name, ax) in enumerate(zip(['top_10', 'top_5', 'top_3'], axes)):\n",
    "    importance_df = get_ensemble_importance(models[target_name], feature_cols)\n",
    "    \n",
    "    # Plot top 10 features\n",
    "    top_features = importance_df.head(10)\n",
    "    ax.barh(top_features['Feature'], top_features['Importance'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{target_name.upper()} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Most Important Features by Model:\")\n",
    "for target_name in ['top_10', 'top_5', 'top_3']:\n",
    "    importance_df = get_ensemble_importance(models[target_name], feature_cols)\n",
    "    print(f\"\\n{target_name.upper()}:\")\n",
    "    for idx, row in importance_df.head(5).iterrows():\n",
    "        print(f\"  {row['Feature']}: {row['Importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results = {}\n",
    "for target_name in ['top_10', 'top_5', 'top_3']:\n",
    "    y_test = locals()[f'y_test_{target_name}']\n",
    "    model = models[target_name]\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    print(f\"\\n{target_name.upper()} Model Performance:\")\n",
    "    print(f\"  Accuracy:  {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall:    {rec:.3f}\")\n",
    "    print(f\"  F1 Score:  {f1:.3f}\")\n",
    "    print(f\"  AUC-ROC:   {auc:.3f}\")\n",
    "    \n",
    "    # Check if performance is realistic\n",
    "    if acc > 0.95:\n",
    "        print(\"  ⚠️ WARNING: Accuracy may be too high - check for overfitting!\")\n",
    "    elif acc > 0.85:\n",
    "        print(\"  ⚠️ Note: High accuracy - monitor for overfitting\")\n",
    "    else:\n",
    "        print(\"  ✅ Accuracy appears realistic\")\n",
    "    \n",
    "    test_results[target_name] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(test_results).T\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models for Production Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create model directory\n",
    "model_dir = '/app/notebooks/advanced/models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "for target_name, model in models.items():\n",
    "    model_path = f'{model_dir}/f1_{target_name}_model.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {target_name} model to {model_path}\")\n",
    "\n",
    "# Save feature names and preprocessing info\n",
    "model_info = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'train_date_range': (train_data['date'].min().isoformat(), train_data['date'].max().isoformat()),\n",
    "    'val_date_range': (val_data['date'].min().isoformat(), val_data['date'].max().isoformat()),\n",
    "    'test_date_range': (test_data['date'].min().isoformat(), test_data['date'].max().isoformat()),\n",
    "    'performance_metrics': test_results,\n",
    "    'next_race': next_race_name,\n",
    "    'model_params': {\n",
    "        'rf': rf_params,\n",
    "        'gb': gb_params\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{model_dir}/model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "    \n",
    "print(f\"\\nModel information saved to {model_dir}/model_info.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}