{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Pipeline Integration - Master Controller\n",
    "\n",
    "This notebook orchestrates ALL components of the F1 Prize Picks optimization system.\n",
    "It can run other notebooks programmatically to ensure proper initialization of all components.\n",
    "\n",
    "Key Features:\n",
    "- Automatically runs prerequisite notebooks in correct order\n",
    "- Handles missing models/modules gracefully\n",
    "- Uses the correct f1db_data_loader.py\n",
    "- Provides a single entry point for the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 16:25:09,071 - F1Pipeline - INFO - Working directory: c:\\Users\\tenei\\Documents\\GitHub\\Formula1\\notebooks\\advanced\n",
      "2025-07-23 16:25:09,073 - F1Pipeline - INFO - ✓ Loaded f1db_data_loader\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('F1Pipeline')\n",
    "\n",
    "# Set working directory to notebook location (works on both Windows and Linux)\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name != 'advanced':\n",
    "    # Try to find the advanced directory\n",
    "    if (notebook_dir / 'advanced').exists():\n",
    "        os.chdir(notebook_dir / 'advanced')\n",
    "    elif (notebook_dir / 'notebooks' / 'advanced').exists():\n",
    "        os.chdir(notebook_dir / 'notebooks' / 'advanced')\n",
    "    elif notebook_dir.parent.name == 'notebooks' and notebook_dir.name != 'advanced':\n",
    "        advanced_dir = notebook_dir.parent / 'advanced'\n",
    "        if advanced_dir.exists():\n",
    "            os.chdir(advanced_dir)\n",
    "\n",
    "# Add current directory to Python path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "logger.info(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Import F1DBDataLoader early to ensure it's available\n",
    "try:\n",
    "    from f1db_data_loader import F1DBDataLoader, load_f1db_data\n",
    "    logger.info(\"✓ Loaded f1db_data_loader\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"✗ Could not import f1db_data_loader: {e}\")\n",
    "    F1DBDataLoader = None\n",
    "    load_f1db_data = None\n",
    "\n",
    "# Import other components that might be available\n",
    "try:\n",
    "    from F1_Feature_Store import F1FeatureStore\n",
    "    logger.info(\"✓ Loaded F1FeatureStore\")\n",
    "except ImportError:\n",
    "    logger.debug(\"F1FeatureStore not available yet\")\n",
    "    F1FeatureStore = None\n",
    "\n",
    "try:\n",
    "    from F1_Explainability_Engine import PredictionExplainer, PrizePicksExplainer\n",
    "    logger.info(\"✓ Loaded Explainability components\")\n",
    "except ImportError:\n",
    "    logger.debug(\"Explainability components not available yet\")\n",
    "    PredictionExplainer = None\n",
    "    PrizePicksExplainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NotebookRunner:\n    \"\"\"\n    Utility to run other Jupyter notebooks programmatically\n    \"\"\"\n    def __init__(self):\n        self.executed_notebooks = []\n        self.failed_notebooks = []\n    \n    def run_notebook(self, notebook_path, timeout=600):\n        \"\"\"\n        Execute a Jupyter notebook and return success status\n        \"\"\"\n        logger.info(f\"Running notebook: {notebook_path}\")\n        \n        try:\n            # Alternative method: Use papermill if available\n            try:\n                import papermill as pm\n                pm.execute_notebook(\n                    input_path=notebook_path,\n                    output_path=notebook_path,\n                    kernel_name='python3',\n                    timeout=timeout\n                )\n                logger.info(f\"✓ Successfully executed: {notebook_path}\")\n                self.executed_notebooks.append(notebook_path)\n                return True\n            except ImportError:\n                logger.info(\"Papermill not available, using nbconvert method\")\n            \n            # Original nbconvert method but without --inplace to avoid modifying the original\n            result = subprocess.run(\n                [\n                    sys.executable, '-m', 'jupyter', 'nbconvert',\n                    '--to', 'notebook',\n                    '--execute',\n                    '--ExecutePreprocessor.timeout=' + str(timeout),\n                    '--ExecutePreprocessor.allow_errors=True',  # Continue on errors\n                    '--output', 'temp_' + Path(notebook_path).name,\n                    notebook_path\n                ],\n                capture_output=True,\n                text=True\n            )\n            \n            # Check if the notebook created expected outputs\n            if result.returncode == 0:\n                # Check if expected files were created\n                expected_outputs = {\n                    'F1_Core_Models.ipynb': 'f1_position_prediction_model.pkl',\n                    'F1_Integrated_Driver_Evaluation.ipynb': 'f1_integrated_evaluation_model.pkl',\n                    'F1_Prize_Picks_Optimizer.ipynb': 'f1_prize_picks_optimizer.pkl'\n                }\n                \n                notebook_name = Path(notebook_path).name\n                if notebook_name in expected_outputs:\n                    output_file = expected_outputs[notebook_name]\n                    if Path(output_file).exists():\n                        logger.info(f\"✓ Successfully created: {output_file}\")\n                        self.executed_notebooks.append(notebook_path)\n                        return True\n                    else:\n                        logger.warning(f\"⚠ Notebook ran but didn't create expected output: {output_file}\")\n                        # Still mark as success if it's not a critical notebook\n                        if 'Evaluation' not in notebook_name:\n                            self.executed_notebooks.append(notebook_path)\n                            return True\n                else:\n                    # Non-critical notebook\n                    logger.info(f\"✓ Successfully executed: {notebook_path}\")\n                    self.executed_notebooks.append(notebook_path)\n                    return True\n            \n            logger.error(f\"✗ Failed to execute: {notebook_path}\")\n            if result.stderr:\n                logger.error(f\"Error: {result.stderr}\")\n            self.failed_notebooks.append(notebook_path)\n            return False\n                \n        except Exception as e:\n            logger.error(f\"✗ Exception running {notebook_path}: {str(e)}\")\n            self.failed_notebooks.append(notebook_path)\n            return False\n        finally:\n            # Clean up temp files\n            temp_file = Path('temp_' + Path(notebook_path).name)\n            if temp_file.exists():\n                temp_file.unlink()\n    \n    def run_notebooks_in_order(self, notebook_list):\n        \"\"\"\n        Run a list of notebooks in order\n        \"\"\"\n        results = []\n        for notebook in notebook_list:\n            if Path(notebook).exists():\n                success = self.run_notebook(notebook)\n                results.append((notebook, success))\n                # Don't stop on failure for non-critical notebooks\n                if not success and any(critical in notebook for critical in ['Core_Models', 'Feature_Store']):\n                    logger.warning(f\"Stopping execution due to failure in critical notebook: {notebook}\")\n                    break\n            else:\n                logger.warning(f\"Notebook not found: {notebook}\")\n                results.append((notebook, False))\n        \n        return results\n    \n    def get_summary(self):\n        \"\"\"\n        Get execution summary\n        \"\"\"\n        return {\n            'executed': self.executed_notebooks,\n            'failed': self.failed_notebooks,\n            'success_rate': len(self.executed_notebooks) / (len(self.executed_notebooks) + len(self.failed_notebooks)) if (self.executed_notebooks or self.failed_notebooks) else 0\n        }\n\n# Initialize runner\nrunner = NotebookRunner()"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookRunner:\n",
    "    \"\"\"\n",
    "    Utility to run other Jupyter notebooks programmatically\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.executed_notebooks = []\n",
    "        self.failed_notebooks = []\n",
    "    \n",
    "    def run_notebook(self, notebook_path, timeout=600):\n",
    "        \"\"\"\n",
    "        Execute a Jupyter notebook and return success status\n",
    "        \"\"\"\n",
    "        logger.info(f\"Running notebook: {notebook_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use nbconvert to execute the notebook\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "                    '--to', 'notebook',\n",
    "                    '--execute',\n",
    "                    '--ExecutePreprocessor.timeout=' + str(timeout),\n",
    "                    '--inplace',\n",
    "                    '--clear-output',\n",
    "                    notebook_path\n",
    "                ],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                self.executed_notebooks.append(notebook_path)\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"✗ Failed to execute: {notebook_path}\")\n",
    "                logger.error(f\"Error: {result.stderr}\")\n",
    "                self.failed_notebooks.append(notebook_path)\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Exception running {notebook_path}: {str(e)}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "    \n",
    "    def run_notebooks_in_order(self, notebook_list):\n",
    "        \"\"\"\n",
    "        Run a list of notebooks in order\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for notebook in notebook_list:\n",
    "            if Path(notebook).exists():\n",
    "                success = self.run_notebook(notebook)\n",
    "                results.append((notebook, success))\n",
    "                if not success:\n",
    "                    logger.warning(f\"Stopping execution due to failure in {notebook}\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(f\"Notebook not found: {notebook}\")\n",
    "                results.append((notebook, False))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get execution summary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'executed': self.executed_notebooks,\n",
    "            'failed': self.failed_notebooks,\n",
    "            'success_rate': len(self.executed_notebooks) / (len(self.executed_notebooks) + len(self.failed_notebooks)) if (self.executed_notebooks or self.failed_notebooks) else 0\n",
    "        }\n",
    "\n",
    "# Initialize runner\n",
    "runner = NotebookRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define notebook execution order\nNOTEBOOK_PIPELINE = [\n    {\n        'name': 'Core Models',\n        'notebook': 'F1_Core_Models.ipynb',\n        'creates': ['f1_position_prediction_model.pkl'],\n        'required': True\n    },\n    {\n        'name': 'Feature Store',\n        'notebook': 'F1_Feature_Store.ipynb',\n        'creates': ['F1_Feature_Store.py'],  # Creates a module\n        'required': True\n    },\n    {\n        'name': 'Integrated Driver Evaluation',\n        'notebook': 'F1_Integrated_Driver_Evaluation.ipynb',\n        'creates': ['f1_integrated_evaluation_model.pkl'],\n        'required': True\n    },\n    {\n        'name': 'Prize Picks Optimizer',\n        'notebook': 'F1_Prize_Picks_Optimizer.ipynb',\n        'creates': ['f1_prize_picks_optimizer.pkl'],\n        'required': True\n    },\n    {\n        'name': 'Explainability Engine',\n        'notebook': 'F1_Explainability_Engine.ipynb',\n        'creates': ['F1_Explainability_Engine.py'],  # Creates a module\n        'required': True\n    },\n    {\n        'name': 'MLflow Tracking',\n        'notebook': 'F1_MLflow_Tracking.ipynb',\n        'creates': [],  # Optional tracking\n        'required': False\n    }\n]\n\n# Option to force rerun all notebooks (set to True if needed)\nFORCE_RERUN = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def initialize_pipeline_components(force_rerun=False):\n    \"\"\"\n    Initialize all pipeline components by running required notebooks\n    ALL notebooks must succeed - no fallbacks or compromises\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"INITIALIZING F1 PIPELINE COMPONENTS\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    # ALL notebooks are required for production\n    required_outputs = {\n        'F1_Core_Models.ipynb': 'f1_position_prediction_model.pkl',\n        'F1_Feature_Store.ipynb': None,  # Creates module, not a file\n        'F1_Integrated_Driver_Evaluation.ipynb': 'f1_integrated_evaluation_model.pkl',\n        'F1_Prize_Picks_Optimizer.ipynb': 'f1_prize_picks_optimizer.pkl',\n        'F1_Explainability_Engine.ipynb': None,  # Creates module\n        'F1_MLflow_Tracking.ipynb': None  # Optional tracking\n    }\n    \n    notebooks_to_run = []\n    \n    for notebook, output_file in required_outputs.items():\n        needs_run = force_rerun\n        \n        if not force_rerun and output_file:\n            # Check if output file exists\n            if not Path(output_file).exists():\n                needs_run = True\n        \n        if needs_run:\n            component = next((c for c in NOTEBOOK_PIPELINE if c['notebook'] == notebook), None)\n            if component:\n                notebooks_to_run.append(component)\n                print(f\"📋 Will run: {component['name']}\")\n        else:\n            print(f\"✓ {notebook}: outputs exist\")\n    \n    if not notebooks_to_run:\n        print(\"\\n✅ All models are trained and ready!\")\n        return True\n    \n    # Run the notebooks - ALL must succeed\n    print(f\"\\nRunning {len(notebooks_to_run)} notebooks to create models...\")\n    print(\"This is a production system - all models must be properly trained\")\n    print(\"-\" * 60)\n    \n    for component in notebooks_to_run:\n        print(f\"\\nRunning: {component['name']}...\")\n        success = runner.run_notebook(component['notebook'], timeout=1200)  # 20 min timeout\n        \n        if not success:\n            print(f\"\\n❌ FAILED: {component['name']}\")\n            print(\"The pipeline cannot proceed without all models properly trained.\")\n            print(\"\\nTo fix this:\")\n            print(f\"1. Open {component['notebook']} in Jupyter\")\n            print(\"2. Run all cells manually to see the specific error\")\n            print(\"3. Fix any data path or dependency issues\")\n            print(\"4. Ensure the model saves successfully\")\n            print(\"\\nThen run this pipeline again.\")\n            return False\n        else:\n            print(f\"✅ SUCCESS: {component['name']}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"✅ ALL MODELS SUCCESSFULLY TRAINED!\")\n    print(\"=\" * 60)\n    return True\n\n# Initialize components - must succeed\ninitialization_success = initialize_pipeline_components(force_rerun=FORCE_RERUN)\n\nif not initialization_success:\n    raise RuntimeError(\n        \"Pipeline initialization failed. All models must be properly trained for production use. \"\n        \"Please fix the failing notebooks and try again.\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 16:25:09,242 - F1Pipeline - INFO - Data directory: C:\\Users\\tenei\\Documents\\GitHub\\Formula1\\data\\f1db\n",
      "2025-07-23 16:25:09,244 - F1Pipeline - INFO - Pipeline configuration initialized\n"
     ]
    }
   ],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the F1 pipeline - Windows/Linux compatible\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths - use relative paths that work on both Windows and Linux\n",
    "        current_dir = Path.cwd()\n",
    "        \n",
    "        # Find the data directory relative to current location\n",
    "        if (current_dir / '../../data/f1db').exists():\n",
    "            self.data_dir = (current_dir / '../../data/f1db').resolve()\n",
    "        elif (current_dir.parent.parent / 'data' / 'f1db').exists():\n",
    "            self.data_dir = (current_dir.parent.parent / 'data' / 'f1db').resolve()\n",
    "        else:\n",
    "            # Try to find it from the root\n",
    "            possible_paths = [\n",
    "                current_dir / 'data' / 'f1db',\n",
    "                current_dir.parent / 'data' / 'f1db',\n",
    "                current_dir.parent.parent / 'data' / 'f1db',\n",
    "                Path('data/f1db'),\n",
    "                Path('../data/f1db'),\n",
    "                Path('../../data/f1db')\n",
    "            ]\n",
    "            for p in possible_paths:\n",
    "                if p.exists():\n",
    "                    self.data_dir = p.resolve()\n",
    "                    break\n",
    "            else:\n",
    "                # Default to relative path\n",
    "                self.data_dir = Path('../../data/f1db').resolve()\n",
    "        \n",
    "        self.model_dir = Path('.')\n",
    "        self.output_dir = Path('pipeline_outputs')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Model settings\n",
    "        self.use_cached_data = True\n",
    "        self.auto_sync = True\n",
    "        self.cache_expiry_hours = 24\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.bankroll = 1000\n",
    "        self.kelly_fraction = 0.25\n",
    "        self.max_correlation = 0.5\n",
    "        self.min_edge = 0.05\n",
    "        self.max_exposure = 0.25\n",
    "        \n",
    "        # Constraints\n",
    "        self.constraints = {\n",
    "            'max_per_driver': 2,\n",
    "            'max_per_type': 3,\n",
    "            'min_avg_edge': 0.08\n",
    "        }\n",
    "        \n",
    "        # Pipeline settings\n",
    "        self.generate_report = True\n",
    "        self.save_predictions = True\n",
    "        self.mlflow_tracking = False\n",
    "        \n",
    "        logger.info(f\"Data directory: {self.data_dir}\")\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return {\n",
    "            'data_dir': str(self.data_dir),\n",
    "            'model_dir': str(self.model_dir),\n",
    "            'output_dir': str(self.output_dir),\n",
    "            'bankroll': self.bankroll,\n",
    "            'kelly_fraction': self.kelly_fraction,\n",
    "            'max_correlation': self.max_correlation,\n",
    "            'min_edge': self.min_edge,\n",
    "            'max_exposure': self.max_exposure,\n",
    "            'constraints': self.constraints\n",
    "        }\n",
    "    \n",
    "    def save(self, path='pipeline_config.json'):\n",
    "        \"\"\"Save configuration\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path='pipeline_config.json'):\n",
    "        \"\"\"Load configuration\"\"\"\n",
    "        config = cls()\n",
    "        if Path(path).exists():\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for key, value in data.items():\n",
    "                    if hasattr(config, key):\n",
    "                        if key.endswith('_dir'):\n",
    "                            setattr(config, key, Path(value))\n",
    "                        else:\n",
    "                            setattr(config, key, value)\n",
    "        return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = PipelineConfig()\n",
    "config.save()\n",
    "logger.info(f\"Pipeline configuration initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class F1PrizePipeline:\n    \"\"\"\n    Production-grade pipeline orchestrating all components\n    No fallbacks - all models must be properly trained\n    \"\"\"\n    def __init__(self, config: PipelineConfig):\n        self.config = config\n        self.data_loader = None\n        self.feature_store = None\n        self.predictor = None\n        self.optimizer = None\n        self.explainer = None\n        self.results = {}\n        \n        # Initialize all components - fail if any are missing\n        self._initialize_components()\n    \n    def _initialize_components(self):\n        \"\"\"Initialize all pipeline components - all are required\"\"\"\n        logger.info(\"Initializing production pipeline components...\")\n        \n        # Data loader - REQUIRED\n        if not F1DBDataLoader:\n            raise ImportError(\"F1DBDataLoader not available. Cannot proceed without data loader.\")\n        \n        self.data_loader = F1DBDataLoader(\n            data_dir=str(self.config.data_dir)  # Changed from base_path to data_dir\n        )\n        logger.info(\"✓ Initialized F1DBDataLoader\")\n        \n        # Load integrated predictor - REQUIRED\n        try:\n            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n            logger.info(\"✓ Loaded integrated predictor\")\n        except FileNotFoundError:\n            raise FileNotFoundError(\n                \"Integrated evaluation model not found. \"\n                \"Run F1_Integrated_Driver_Evaluation.ipynb to create it.\"\n            )\n        \n        # Load Prize Picks optimizer - REQUIRED\n        try:\n            optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n            self.optimizer = optimizer_config['optimizer']\n            logger.info(\"✓ Loaded Prize Picks optimizer\")\n        except FileNotFoundError:\n            raise FileNotFoundError(\n                \"Prize Picks optimizer not found. \"\n                \"Run F1_Prize_Picks_Optimizer.ipynb to create it.\"\n            )\n        \n        # Feature store - REQUIRED\n        if not F1FeatureStore:\n            raise ImportError(\n                \"F1FeatureStore not available. \"\n                \"Run F1_Feature_Store.ipynb to create the module.\"\n            )\n        self.feature_store = F1FeatureStore()\n        logger.info(\"✓ Initialized Feature Store\")\n        \n        # Explainers - REQUIRED for production\n        if not PredictionExplainer or not PrizePicksExplainer:\n            raise ImportError(\n                \"Explainability components not available. \"\n                \"Run F1_Explainability_Engine.ipynb to create them.\"\n            )\n        \n        # Initialize explainers with loaded models\n        self.prediction_explainer = PredictionExplainer(self.predictor, self.feature_store.get_feature_names())\n        self.pp_explainer = PrizePicksExplainer()\n        logger.info(\"✓ Initialized Explainability components\")\n        \n        # Verify base position prediction model exists\n        if not Path(self.config.model_dir / 'f1_position_prediction_model.pkl').exists():\n            raise FileNotFoundError(\n                \"Base position prediction model not found. \"\n                \"Run F1_Core_Models.ipynb to create it.\"\n            )\n        \n        logger.info(\"✅ All production components initialized successfully\")\n    \n    def load_data(self, force_update=False):\n        \"\"\"Load and prepare F1 data\"\"\"\n        logger.info(\"Loading F1 data...\")\n        \n        # Load data using f1db_data_loader\n        self.data = load_f1db_data(data_dir=str(self.config.data_dir))  # Changed from base_path\n        \n        if not self.data:\n            raise ValueError(\"Failed to load F1 data. Check data directory and connection.\")\n        \n        logger.info(f\"Loaded {len(self.data)} datasets\")\n        \n        # Validate critical datasets exist\n        required_datasets = ['races', 'drivers', 'results', 'constructors']\n        missing = [ds for ds in required_datasets if ds not in self.data or self.data[ds].empty]\n        \n        if missing:\n            # Check with alternative names\n            alt_names = {\n                'results': ['races_race_results', 'race_results'],\n                'races': ['races'],\n                'drivers': ['drivers'],\n                'constructors': ['constructors']\n            }\n            \n            for dataset in missing[:]:\n                for alt_name in alt_names.get(dataset, []):\n                    if alt_name in self.data and not self.data[alt_name].empty:\n                        self.data[dataset] = self.data[alt_name]\n                        missing.remove(dataset)\n                        break\n        \n        if missing:\n            raise ValueError(f\"Missing required datasets: {missing}\")\n        \n        return self.data\n    \n    def prepare_features(self, race_id=None):\n        \"\"\"Prepare features for prediction using Feature Store\"\"\"\n        logger.info(\"Preparing features with Feature Store...\")\n        \n        # Get upcoming race if no race_id specified\n        if race_id is None:\n            races = self.data.get('races', pd.DataFrame())\n            if 'date' in races.columns:\n                races['date'] = pd.to_datetime(races['date'])\n                upcoming = races[races['date'] > datetime.now()]\n                if not upcoming.empty:\n                    upcoming = upcoming.iloc[0]\n                    race_id = upcoming.get('id', upcoming.get('raceId'))\n                    race_name = upcoming.get('officialName', upcoming.get('name', 'Unknown'))\n                    logger.info(f\"Preparing for upcoming race: {race_name} (ID: {race_id})\")\n            \n            if race_id is None:\n                logger.warning(\"No upcoming race found - using latest race\")\n                race_id = races['id'].max() if 'id' in races.columns else races['raceId'].max()\n        \n        # Use Feature Store to engineer features\n        features = self.feature_store.engineer_features(self.data, race_id)\n        \n        if features.empty:\n            raise ValueError(\"Feature engineering failed. No features generated.\")\n        \n        self.results['features'] = features\n        self.results['race_id'] = race_id\n        return features\n    \n    def generate_predictions(self):\n        \"\"\"Generate predictions using trained models\"\"\"\n        logger.info(\"Generating predictions with trained models...\")\n        \n        if 'features' not in self.results:\n            raise ValueError(\"No features available. Run prepare_features first.\")\n        \n        features = self.results['features']\n        \n        # Load base prediction model\n        base_model_data = joblib.load(self.config.model_dir / 'f1_position_prediction_model.pkl')\n        base_model = base_model_data['model']\n        scaler = base_model_data['scaler']\n        feature_columns = base_model_data['feature_columns']\n        \n        # Get predictions from integrated predictor\n        predictions = self.predictor.predict_with_evaluation(\n            features, \n            self.data.get('driver_evaluation', pd.DataFrame()),\n            self.data.get('constructor_compatibility', pd.DataFrame())\n        )\n        \n        # Structure predictions for Prize Picks\n        predictions_df = pd.DataFrame(predictions)\n        \n        # Ensure required columns exist\n        required_cols = ['driver', 'driverId', 'top10_prob', 'top5_prob', \n                        'top3_prob', 'points_prob', 'confidence']\n        \n        missing_cols = [col for col in required_cols if col not in predictions_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Predictions missing required columns: {missing_cols}\")\n        \n        self.results['predictions'] = predictions_df\n        logger.info(f\"Generated predictions for {len(predictions_df)} drivers\")\n        return predictions_df\n    \n    def optimize_picks(self):\n        \"\"\"Optimize Prize Picks selections using trained optimizer\"\"\"\n        logger.info(\"Optimizing Prize Picks with trained optimizer...\")\n        \n        if 'predictions' not in self.results:\n            raise ValueError(\"No predictions available. Run generate_predictions first.\")\n        \n        predictions = self.results['predictions']\n        \n        if predictions.empty:\n            raise ValueError(\"No predictions to optimize\")\n        \n        # Generate all possible picks\n        all_picks = self.optimizer.generate_all_picks(\n            predictions,\n            min_edge=self.config.min_edge\n        )\n        \n        if all_picks.empty:\n            logger.warning(\"No picks with positive edge found\")\n            return []\n        \n        # Optimize portfolio\n        portfolio = self.optimizer.optimize_portfolio(\n            all_picks,\n            bankroll=self.config.bankroll,\n            constraints=self.config.constraints\n        )\n        \n        if not portfolio:\n            logger.warning(\"Optimizer returned empty portfolio\")\n            return []\n        \n        self.results['portfolio'] = portfolio\n        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n        return portfolio\n    \n    def generate_explanations(self):\n        \"\"\"Generate explanations for recommendations\"\"\"\n        logger.info(\"Generating explanations...\")\n        \n        if 'portfolio' not in self.results:\n            raise ValueError(\"No portfolio to explain. Run optimize_picks first.\")\n        \n        explanations = []\n        \n        for parlay in self.results['portfolio']:\n            explanation = self.pp_explainer.explain_parlay(parlay)\n            explanations.append(explanation)\n        \n        self.results['explanations'] = explanations\n        return explanations\n    \n    def generate_report(self, save_path=None):\n        \"\"\"Generate comprehensive report\"\"\"\n        logger.info(\"Generating production report...\")\n        \n        # Ensure we have all required components\n        if 'predictions' not in self.results:\n            raise ValueError(\"No predictions available for report\")\n        \n        if 'portfolio' not in self.results:\n            logger.warning(\"No portfolio generated - report will be limited\")\n        \n        report = {\n            'generated_at': datetime.now().isoformat(),\n            'pipeline_version': '1.0.0',\n            'race_id': self.results.get('race_id'),\n            'config': self.config.to_dict(),\n            'summary': self._generate_summary(),\n            'predictions': self.results['predictions'].to_dict('records'),\n            'portfolio': self._serialize_portfolio(),\n            'explanations': self.results.get('explanations', []),\n            'risk_metrics': self._calculate_risk_metrics(),\n            'model_info': {\n                'integrated_predictor': str(type(self.predictor)),\n                'optimizer': str(type(self.optimizer)),\n                'feature_store': str(type(self.feature_store))\n            }\n        }\n        \n        if save_path is None:\n            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        \n        with open(save_path, 'w') as f:\n            json.dump(report, f, indent=2)\n        \n        logger.info(f\"Report saved to {save_path}\")\n        return report\n    \n    def _generate_summary(self):\n        \"\"\"Generate summary statistics\"\"\"\n        summary = {\n            'pipeline_status': 'success',\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        if 'predictions' in self.results:\n            summary['n_drivers'] = len(self.results['predictions'])\n            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n            summary['top_drivers'] = self.results['predictions'].nlargest(5, 'confidence')['driver'].tolist()\n        \n        if 'portfolio' in self.results:\n            portfolio = self.results['portfolio']\n            summary['n_parlays'] = len(portfolio)\n            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio]) if portfolio else 0\n        \n        return summary\n    \n    def _serialize_portfolio(self):\n        \"\"\"Serialize portfolio for JSON\"\"\"\n        if 'portfolio' not in self.results:\n            return []\n        \n        serialized = []\n        for parlay in self.results['portfolio']:\n            parlay_data = {\n                'n_picks': parlay['n_picks'],\n                'bet_size': parlay['bet_size'],\n                'payout': parlay['payout'],\n                'adjusted_prob': parlay['adjusted_prob'],\n                'expected_value': parlay['expected_value'],\n                'kelly_stake': parlay['kelly_stake'],\n                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n            }\n            serialized.append(parlay_data)\n        \n        return serialized\n    \n    def _calculate_risk_metrics(self):\n        \"\"\"Calculate risk metrics for portfolio\"\"\"\n        if 'portfolio' not in self.results or not self.results['portfolio']:\n            return {\n                'total_exposure': 0,\n                'exposure_pct': 0,\n                'n_bets': 0,\n                'status': 'no_portfolio'\n            }\n        \n        portfolio = self.results['portfolio']\n        total_exposure = sum(p['bet_size'] for p in portfolio)\n        \n        metrics = {\n            'total_exposure': total_exposure,\n            'exposure_pct': total_exposure / self.config.bankroll,\n            'n_bets': len(portfolio),\n            'avg_bet_size': total_exposure / len(portfolio),\n            'max_bet_size': max(p['bet_size'] for p in portfolio),\n            'min_bet_size': min(p['bet_size'] for p in portfolio),\n            'status': 'calculated'\n        }\n        \n        return metrics\n    \n    def run(self, race_id=None):\n        \"\"\"Run complete production pipeline - no compromises\"\"\"\n        logger.info(\"Starting F1 Prize Picks production pipeline...\")\n        logger.info(\"All models must be properly trained - no fallbacks\")\n        \n        try:\n            # Step 1: Load data - REQUIRED\n            self.load_data()\n            \n            # Step 2: Prepare features - REQUIRED\n            features = self.prepare_features(race_id)\n            \n            # Step 3: Generate predictions - REQUIRED\n            predictions = self.generate_predictions()\n            \n            # Step 4: Optimize picks - REQUIRED\n            portfolio = self.optimize_picks()\n            \n            # Step 5: Generate explanations - REQUIRED\n            if self.config.generate_report:\n                self.generate_explanations()\n            \n            # Step 6: Generate report - REQUIRED\n            if self.config.save_predictions:\n                report = self.generate_report()\n            \n            logger.info(\"✅ Production pipeline completed successfully!\")\n            return self.results\n            \n        except Exception as e:\n            logger.error(f\"Production pipeline failed: {str(e)}\")\n            logger.error(\"This is a production system - all components must work properly\")\n            raise  # Re-raise the exception - no silent failures in production"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 16:25:09,386 - F1Pipeline - INFO - Initializing production pipeline components...\n",
      "2025-07-23 16:25:09,388 - F1Pipeline - INFO - ✓ Initialized F1DBDataLoader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Pipeline execution failed: Can't get attribute 'IntegratedF1Predictor' on <module '__main__'>\n",
      "\n",
      "This is a production system. Please ensure:\n",
      "1. All notebooks have been run successfully\n",
      "2. All models are properly trained and saved\n",
      "3. Data is available in the correct format\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'IntegratedF1Predictor' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initialization_success:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      4\u001b[39m         \u001b[38;5;66;03m# Initialize production pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         pipeline = \u001b[43mF1PrizePipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mProduction pipeline initialized successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m         \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mF1PrizePipeline.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.results = {}\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize all components - fail if any are missing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mF1PrizePipeline._initialize_components\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Load integrated predictor - REQUIRED\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1_integrated_evaluation_model.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m✓ Loaded integrated predictor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tenei\\Documents\\GitHub\\Formula1\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:749\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    744\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m    746\u001b[39m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[32m    747\u001b[39m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[32m    748\u001b[39m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tenei\\Documents\\GitHub\\Formula1\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:626\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[39m\n\u001b[32m    624\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    628\u001b[39m         warnings.warn(\n\u001b[32m    629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\pickle.py:1213\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1211\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1212\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\pickle.py:1538\u001b[39m, in \u001b[36m_Unpickler.load_stack_global\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[33m\"\u001b[39m\u001b[33mSTACK_GLOBAL requires str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\pickle.py:1582\u001b[39m, in \u001b[36m_Unpickler.find_class\u001b[39m\u001b[34m(self, module, name)\u001b[39m\n\u001b[32m   1580\u001b[39m \u001b[38;5;28m__import__\u001b[39m(module, level=\u001b[32m0\u001b[39m)\n\u001b[32m   1581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m4\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys.modules[module], name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\pickle.py:331\u001b[39m, in \u001b[36m_getattribute\u001b[39m\u001b[34m(obj, name)\u001b[39m\n\u001b[32m    329\u001b[39m         obj = \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    332\u001b[39m                              .format(name, obj)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'IntegratedF1Predictor' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# Check if all components are initialized successfully\n",
    "if initialization_success:\n",
    "    try:\n",
    "        # Initialize production pipeline\n",
    "        pipeline = F1PrizePipeline(config)\n",
    "        logger.info(\"Production pipeline initialized successfully\")\n",
    "        \n",
    "        # Run the pipeline\n",
    "        logger.info(\"Running production pipeline...\")\n",
    "        results = pipeline.run()\n",
    "        \n",
    "        # Display results\n",
    "        if results and 'portfolio' in results:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"F1 PRIZE PICKS RECOMMENDATIONS - PRODUCTION\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            portfolio = results['portfolio']\n",
    "            \n",
    "            if portfolio:\n",
    "                for i, parlay in enumerate(portfolio, 1):\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"PARLAY {i}: {parlay['n_picks']}-PICK ENTRY\")\n",
    "                    print(f\"{'='*60}\")\n",
    "                    print(f\"Bet Amount: ${parlay['bet_size']:.2f}\")\n",
    "                    print(f\"Potential Payout: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\")\n",
    "                    print(f\"Win Probability: {parlay['adjusted_prob']:.1%}\")\n",
    "                    print(f\"Expected Value: +{parlay['expected_value']:.1%}\")\n",
    "                    print(f\"\\nPicks:\")\n",
    "                    \n",
    "                    picks = parlay['picks']\n",
    "                    if hasattr(picks, 'iterrows'):\n",
    "                        for j, (_, pick) in enumerate(picks.iterrows(), 1):\n",
    "                            print(f\"  {j}. {pick['driver']} - {pick['bet_type']}\")\n",
    "                            print(f\"     Probability: {pick['probability']:.1%}\")\n",
    "                            print(f\"     Edge: +{pick['edge']:.1%}\")\n",
    "                \n",
    "                # Summary\n",
    "                summary = pipeline._generate_summary()\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"PORTFOLIO SUMMARY\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"Total Wagered: ${summary.get('total_wagered', 0):.2f}\")\n",
    "                print(f\"Expected Profit: ${summary.get('expected_profit', 0):.2f}\")\n",
    "                print(f\"Number of Parlays: {summary.get('n_parlays', 0)}\")\n",
    "                print(f\"Average Win Probability: {summary.get('avg_win_probability', 0):.1%}\")\n",
    "                print(f\"\\nTop 5 Drivers by Confidence:\")\n",
    "                for driver in summary.get('top_drivers', []):\n",
    "                    print(f\"  - {driver}\")\n",
    "            else:\n",
    "                print(\"\\nNo parlays generated. This could mean:\")\n",
    "                print(\"- No bets met the minimum edge requirement\")\n",
    "                print(\"- Risk constraints prevented bet placement\")\n",
    "                print(\"- Try adjusting config.min_edge or config.kelly_fraction\")\n",
    "        else:\n",
    "            print(\"\\n❌ Pipeline completed but no portfolio was generated.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Pipeline execution failed: {str(e)}\")\n",
    "        print(\"\\nThis is a production system. Please ensure:\")\n",
    "        print(\"1. All notebooks have been run successfully\")\n",
    "        print(\"2. All models are properly trained and saved\")\n",
    "        print(\"3. Data is available in the correct format\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n❌ Cannot run pipeline - initialization failed\")\n",
    "    print(\"Please fix the failing notebooks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a production-grade F1 Prize Picks pipeline that:\n",
    "- Automatically runs prerequisite notebooks in correct order\n",
    "- Ensures all models are properly trained with no fallbacks\n",
    "- Uses real F1DB data from the correct data path\n",
    "- Provides comprehensive race weekend automation and performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceWeekendAutomation:\n",
    "    \"\"\"\n",
    "    Automate pipeline execution for race weekends\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: F1PrizePipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.schedule = []\n",
    "    \n",
    "    def get_race_schedule(self):\n",
    "        \"\"\"Get upcoming race schedule\"\"\"\n",
    "        races = self.pipeline.data.get('races', pd.DataFrame())\n",
    "        if races.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get future races\n",
    "        races['date'] = pd.to_datetime(races['date'])\n",
    "        future_races = races[races['date'] > datetime.now()]\n",
    "        \n",
    "        return future_races.sort_values('date')\n",
    "    \n",
    "    def schedule_race_analysis(self, race_id, race_date):\n",
    "        \"\"\"Schedule analysis for a specific race\"\"\"\n",
    "        # Run at different times\n",
    "        schedule_times = [\n",
    "            (race_date - timedelta(days=3), 'Initial Analysis'),\n",
    "            (race_date - timedelta(days=1), 'Pre-Qualifying Update'),\n",
    "            (race_date - timedelta(hours=4), 'Final Predictions')\n",
    "        ]\n",
    "        \n",
    "        for run_time, description in schedule_times:\n",
    "            self.schedule.append({\n",
    "                'race_id': race_id,\n",
    "                'run_time': run_time,\n",
    "                'description': description,\n",
    "                'status': 'scheduled'\n",
    "            })\n",
    "    \n",
    "    def execute_scheduled_runs(self):\n",
    "        \"\"\"Execute scheduled pipeline runs\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        for task in self.schedule:\n",
    "            if task['status'] == 'scheduled' and task['run_time'] <= current_time:\n",
    "                logger.info(f\"Executing {task['description']} for race {task['race_id']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Update config based on timing\n",
    "                    if 'Final' in task['description']:\n",
    "                        self.pipeline.config.kelly_fraction = 0.20  # More conservative\n",
    "                    \n",
    "                    # Run pipeline\n",
    "                    results = self.pipeline.run(task['race_id'])\n",
    "                    \n",
    "                    # Save results with timestamp\n",
    "                    output_name = f\"race_{task['race_id']}_{task['description'].replace(' ', '_')}_{current_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "                    self.pipeline.generate_report(\n",
    "                        self.pipeline.config.output_dir / f\"{output_name}.json\"\n",
    "                    )\n",
    "                    \n",
    "                    task['status'] = 'completed'\n",
    "                    task['completed_at'] = current_time.isoformat()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to execute {task['description']}: {str(e)}\")\n",
    "                    task['status'] = 'failed'\n",
    "                    task['error'] = str(e)\n",
    "    \n",
    "    def generate_weekend_summary(self):\n",
    "        \"\"\"Generate summary of all analyses for a race weekend\"\"\"\n",
    "        completed_tasks = [t for t in self.schedule if t['status'] == 'completed']\n",
    "        \n",
    "        if not completed_tasks:\n",
    "            return None\n",
    "        \n",
    "        summary = {\n",
    "            'race_id': completed_tasks[0]['race_id'],\n",
    "            'analyses_completed': len(completed_tasks),\n",
    "            'final_recommendations': None\n",
    "        }\n",
    "        \n",
    "        # Get final predictions\n",
    "        final_task = next((t for t in completed_tasks if 'Final' in t['description']), None)\n",
    "        if final_task:\n",
    "            # Load the report\n",
    "            report_files = list(self.pipeline.config.output_dir.glob(f\"race_{final_task['race_id']}_Final*.json\"))\n",
    "            if report_files:\n",
    "                with open(report_files[-1], 'r') as f:\n",
    "                    final_report = json.load(f)\n",
    "                    summary['final_recommendations'] = final_report.get('portfolio', [])\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Example usage for race weekend automation\n",
    "if 'pipeline' in locals() and hasattr(pipeline, 'data') and pipeline.data:\n",
    "    automation = RaceWeekendAutomation(pipeline)\n",
    "    \n",
    "    # Get upcoming races\n",
    "    upcoming_races = automation.get_race_schedule()\n",
    "    if not upcoming_races.empty:\n",
    "        print(\"\\nUpcoming Races:\")\n",
    "        print(\"=\" * 60)\n",
    "        for idx, race in upcoming_races.head(3).iterrows():\n",
    "            print(f\"{race['date'].strftime('%Y-%m-%d')}: {race['name']} (Round {race['round']})\")\n",
    "            \n",
    "            # Schedule analysis for next race\n",
    "            if idx == upcoming_races.index[0]:  # First race\n",
    "                automation.schedule_race_analysis(race['raceId'], race['date'])\n",
    "        \n",
    "        print(f\"\\nScheduled {len(automation.schedule)} analyses for next race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Monitoring (From Original Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor pipeline and prediction performance\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = []\n",
    "    \n",
    "    def track_predictions(self, predictions, actuals=None):\n",
    "        \"\"\"Track prediction accuracy\"\"\"\n",
    "        metric = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_predictions': len(predictions),\n",
    "            'avg_confidence': predictions['confidence'].mean() if 'confidence' in predictions else 0\n",
    "        }\n",
    "        \n",
    "        if actuals is not None:\n",
    "            # Calculate accuracy metrics\n",
    "            metric['accuracy'] = self._calculate_accuracy(predictions, actuals)\n",
    "        \n",
    "        self.metrics.append(metric)\n",
    "    \n",
    "    def _calculate_accuracy(self, predictions, actuals):\n",
    "        \"\"\"Calculate prediction accuracy\"\"\"\n",
    "        # Implementation depends on actual data format\n",
    "        # This is a placeholder - implement based on your needs\n",
    "        return 0.0\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"Generate performance report\"\"\"\n",
    "        if not self.metrics:\n",
    "            return None\n",
    "        \n",
    "        report = {\n",
    "            'period': {\n",
    "                'start': self.metrics[0]['timestamp'],\n",
    "                'end': self.metrics[-1]['timestamp']\n",
    "            },\n",
    "            'total_predictions': sum(m['n_predictions'] for m in self.metrics),\n",
    "            'avg_confidence': np.mean([m['avg_confidence'] for m in self.metrics]),\n",
    "            'runs_completed': len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def plot_performance_trends(self):\n",
    "        \"\"\"Plot performance trends over time\"\"\"\n",
    "        if not self.metrics:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        timestamps = [pd.to_datetime(m['timestamp']) for m in self.metrics]\n",
    "        confidences = [m['avg_confidence'] for m in self.metrics]\n",
    "        \n",
    "        ax.plot(timestamps, confidences, marker='o')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Average Confidence')\n",
    "        ax.set_title('Model Confidence Over Time')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save metrics to file\"\"\"\n",
    "        metrics_file = self.output_dir / 'performance_metrics.json'\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        logger.info(f\"Saved performance metrics to {metrics_file}\")\n",
    "    \n",
    "    def load_metrics(self):\n",
    "        \"\"\"Load existing metrics\"\"\"\n",
    "        metrics_file = self.output_dir / 'performance_metrics.json'\n",
    "        if metrics_file.exists():\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                self.metrics = json.load(f)\n",
    "            logger.info(f\"Loaded {len(self.metrics)} historical metrics\")\n",
    "\n",
    "# Initialize performance monitor\n",
    "if 'config' in locals():\n",
    "    monitor = PerformanceMonitor(config.output_dir)\n",
    "    monitor.load_metrics()  # Load any existing metrics\n",
    "    \n",
    "    # Track current predictions if available\n",
    "    if 'pipeline' in locals() and 'predictions' in pipeline.results:\n",
    "        monitor.track_predictions(pipeline.results['predictions'])\n",
    "        monitor.save_metrics()\n",
    "        \n",
    "        performance_report = monitor.generate_performance_report()\n",
    "        print(\"\\nPerformance Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        for key, value in performance_report.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline state saved to pipeline_outputs\\pipeline_state.json\n",
      "\n",
      "Standalone run script created: run_f1_pipeline.py\n",
      "\n",
      "Usage examples:\n",
      "  python run_f1_pipeline.py                  # Run for upcoming race\n",
      "  python run_f1_pipeline.py --race-id 1234   # Run for specific race\n",
      "  python run_f1_pipeline.py --backtest       # Run backtesting\n",
      "  python run_f1_pipeline.py --schedule       # Schedule race weekend automation\n"
     ]
    }
   ],
   "source": [
    "# Save complete pipeline state for future use\n",
    "pipeline_state = {\n",
    "    'config': config.to_dict(),\n",
    "    'last_run': datetime.now().isoformat(),\n",
    "    'results_summary': pipeline._generate_summary() if 'pipeline' in locals() else {},\n",
    "    'automation_schedule': automation.schedule if 'automation' in locals() else [],\n",
    "    'performance_metrics': monitor.metrics[-10:] if 'monitor' in locals() else [],  # Last 10 metrics\n",
    "    'notebook_execution': runner.get_summary() if 'runner' in locals() else {}\n",
    "}\n",
    "\n",
    "# Save state\n",
    "state_path = config.output_dir / 'pipeline_state.json'\n",
    "with open(state_path, 'w') as f:\n",
    "    json.dump(pipeline_state, f, indent=2)\n",
    "\n",
    "print(f\"\\nPipeline state saved to {state_path}\")\n",
    "\n",
    "# Create standalone run script for easy execution\n",
    "run_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run F1 Prize Picks Pipeline\n",
    "\n",
    "Usage:\n",
    "    python run_f1_pipeline.py              # Run for upcoming race\n",
    "    python run_f1_pipeline.py --race-id 1234  # Run for specific race\n",
    "    python run_f1_pipeline.py --backtest   # Run backtesting\n",
    "    python run_f1_pipeline.py --schedule   # Schedule race weekend automation\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Add notebook directory to path\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "\n",
    "def run_master_notebook(race_id=None, mode='predict'):\n",
    "    \"\"\"Run the master pipeline notebook\"\"\"\n",
    "    # Convert notebook to script first\n",
    "    subprocess.run([\n",
    "        sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "        '--to', 'script',\n",
    "        'F1_Pipeline_Integration_Master.ipynb'\n",
    "    ])\n",
    "    \n",
    "    # Import and run\n",
    "    from F1_Pipeline_Integration_Master import F1PrizePipeline, PipelineConfig\n",
    "    from F1_Pipeline_Integration_Master import RaceWeekendAutomation, PerformanceMonitor\n",
    "    \n",
    "    # Load configuration\n",
    "    config = PipelineConfig.load()\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    \n",
    "    if mode == 'schedule':\n",
    "        # Run automation\n",
    "        automation = RaceWeekendAutomation(pipeline)\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        upcoming = automation.get_race_schedule()\n",
    "        if not upcoming.empty:\n",
    "            next_race = upcoming.iloc[0]\n",
    "            automation.schedule_race_analysis(next_race['raceId'], next_race['date'])\n",
    "            print(f\"Scheduled analyses for {next_race['name']}\")\n",
    "            automation.execute_scheduled_runs()\n",
    "    elif mode == 'backtest':\n",
    "        print(\"Running backtesting...\")\n",
    "        # Import and run backtesting notebook\n",
    "        subprocess.run([\n",
    "            sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "            '--to', 'notebook',\n",
    "            '--execute',\n",
    "            'F1_Backtesting_Framework.ipynb'\n",
    "        ])\n",
    "    else:\n",
    "        # Normal prediction mode\n",
    "        results = pipeline.run(race_id)\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\\\nPipeline completed successfully!\")\n",
    "            print(f\"Results saved to {config.output_dir}\")\n",
    "            \n",
    "            # Track performance\n",
    "            monitor = PerformanceMonitor(config.output_dir)\n",
    "            monitor.load_metrics()\n",
    "            monitor.track_predictions(results.get('predictions', pd.DataFrame()))\n",
    "            monitor.save_metrics()\n",
    "        else:\n",
    "            print(\"\\\\nPipeline failed. Check logs for details.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Run F1 Prize Picks Pipeline')\n",
    "    parser.add_argument('--race-id', type=int, help='Specific race ID to analyze')\n",
    "    parser.add_argument('--backtest', action='store_true', \n",
    "                       help='Run backtesting instead of predictions')\n",
    "    parser.add_argument('--schedule', action='store_true',\n",
    "                       help='Schedule automated race weekend analyses')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.backtest:\n",
    "        run_master_notebook(mode='backtest')\n",
    "    elif args.schedule:\n",
    "        run_master_notebook(mode='schedule')\n",
    "    else:\n",
    "        run_master_notebook(race_id=args.race_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save run script\n",
    "script_path = Path('run_f1_pipeline.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(run_script)\n",
    "\n",
    "# Make it executable on Unix-like systems\n",
    "import os\n",
    "if os.name != 'nt':  # Not Windows\n",
    "    os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"\\nStandalone run script created: {script_path}\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  python run_f1_pipeline.py                  # Run for upcoming race\")\n",
    "print(\"  python run_f1_pipeline.py --race-id 1234   # Run for specific race\")\n",
    "print(\"  python run_f1_pipeline.py --backtest       # Run backtesting\")\n",
    "print(\"  python run_f1_pipeline.py --schedule       # Schedule race weekend automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Components (Using Correct Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1PrizePipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrating all components\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = None\n",
    "        self.feature_store = None\n",
    "        self.predictor = None\n",
    "        self.optimizer = None\n",
    "        self.explainer = None\n",
    "        self.results = {}\n",
    "        \n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all pipeline components\"\"\"\n",
    "        logger.info(\"Initializing pipeline components...\")\n",
    "        \n",
    "        # Data loader - Using f1db_data_loader\n",
    "        if F1DBDataLoader:\n",
    "            self.data_loader = F1DBDataLoader(\n",
    "                data_dir=str(self.config.data_dir)  # Changed from base_path\n",
    "            )\n",
    "            logger.info(\"✓ Initialized F1DBDataLoader\")\n",
    "        else:\n",
    "            logger.error(\"✗ F1DBDataLoader not available\")\n",
    "        \n",
    "        # Load saved models if available\n",
    "        try:\n",
    "            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n",
    "            logger.info(\"✓ Loaded integrated predictor\")\n",
    "        except:\n",
    "            logger.warning(\"✗ Could not load integrated predictor\")\n",
    "            self.predictor = None\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if PrizePicksOptimizer:\n",
    "            try:\n",
    "                optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n",
    "                self.optimizer = optimizer_config['optimizer']\n",
    "                logger.info(\"✓ Loaded Prize Picks optimizer\")\n",
    "            except:\n",
    "                logger.warning(\"Creating new optimizer\")\n",
    "                self.optimizer = PrizePicksOptimizer(\n",
    "                    kelly_fraction=self.config.kelly_fraction,\n",
    "                    max_correlation=self.config.max_correlation\n",
    "                )\n",
    "        \n",
    "        # Feature store\n",
    "        if F1FeatureStore:\n",
    "            self.feature_store = F1FeatureStore()\n",
    "            logger.info(\"✓ Initialized Feature Store\")\n",
    "        \n",
    "        # Explainers\n",
    "        if PredictionExplainer:\n",
    "            self.prediction_explainer = PredictionExplainer(None, [])\n",
    "        if PrizePicksExplainer:\n",
    "            self.pp_explainer = PrizePicksExplainer()\n",
    "        \n",
    "        logger.info(\"Pipeline components initialization complete\")\n",
    "    \n",
    "    def load_data(self, force_update=False):\n",
    "        \"\"\"Load and prepare F1 data\"\"\"\n",
    "        logger.info(\"Loading F1 data...\")\n",
    "        \n",
    "        if not self.data_loader:\n",
    "            logger.error(\"Data loader not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Load data using f1db_data_loader\n",
    "        if load_f1db_data:\n",
    "            self.data = load_f1db_data(data_dir=str(self.config.data_dir))  # Changed from base_path\n",
    "        else:\n",
    "            # Fallback to loading core datasets\n",
    "            self.data = self.data_loader.get_core_datasets()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} datasets\")\n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, race_id=None):\n",
    "        \"\"\"Prepare features for prediction\"\"\"\n",
    "        logger.info(\"Preparing features...\")\n",
    "        \n",
    "        # Get upcoming race if no race_id specified\n",
    "        if race_id is None:\n",
    "            races = self.data.get('races', pd.DataFrame())\n",
    "            if not races.empty:\n",
    "                races['date'] = pd.to_datetime(races['date'])\n",
    "                upcoming = races[races['date'] > datetime.now()].iloc[0]\n",
    "                race_id = upcoming['raceId']\n",
    "                logger.info(f\"Preparing for upcoming race: {upcoming['name']}\")\n",
    "            else:\n",
    "                logger.warning(\"No upcoming race found\")\n",
    "                return None\n",
    "        \n",
    "        # Build feature set\n",
    "        if self.feature_store and hasattr(self.feature_store, 'engineer_features'):\n",
    "            features = self.feature_store.engineer_features(self.data)\n",
    "        else:\n",
    "            # Basic feature preparation\n",
    "            features = self._create_basic_features()\n",
    "        \n",
    "        self.results['features'] = features\n",
    "        return features\n",
    "    \n",
    "    def _create_basic_features(self):\n",
    "        \"\"\"Create basic features if feature store not available\"\"\"\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        if results.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Simple feature engineering\n",
    "        driver_stats = results.groupby('driverId').agg({\n",
    "            'positionOrder': ['mean', 'std'],\n",
    "            'points': ['mean', 'sum'],\n",
    "            'grid': 'mean'\n",
    "        })\n",
    "        \n",
    "        driver_stats.columns = ['avg_position', 'position_std', \n",
    "                               'avg_points', 'total_points', 'avg_grid']\n",
    "        \n",
    "        return driver_stats\n",
    "    \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions for all drivers\"\"\"\n",
    "        logger.info(\"Generating predictions...\")\n",
    "        \n",
    "        # Get active drivers\n",
    "        drivers = self.data.get('drivers', pd.DataFrame())\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        \n",
    "        if drivers.empty or results.empty:\n",
    "            logger.error(\"No driver or results data available\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get drivers who raced recently\n",
    "        recent_drivers = results[results['year'] >= 2023]['driverId'].unique()\n",
    "        active_drivers = drivers[drivers['driverId'].isin(recent_drivers)]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for _, driver in active_drivers.iterrows():\n",
    "            # Get driver stats\n",
    "            driver_results = results[results['driverId'] == driver['driverId']].tail(10)\n",
    "            \n",
    "            if len(driver_results) >= 3:\n",
    "                # Calculate probabilities\n",
    "                top10_prob = (driver_results['positionOrder'] <= 10).mean()\n",
    "                top5_prob = (driver_results['positionOrder'] <= 5).mean()\n",
    "                top3_prob = (driver_results['positionOrder'] <= 3).mean()\n",
    "                points_prob = (driver_results['points'] > 0).mean()\n",
    "                \n",
    "                # Adjust with model if available\n",
    "                confidence = 0.7 + 0.05 * len(driver_results) / 10\n",
    "                \n",
    "                predictions.append({\n",
    "                    'driver': driver['surname'],\n",
    "                    'driverId': driver['driverId'],\n",
    "                    'top10_prob': min(0.95, top10_prob * 1.1),\n",
    "                    'top5_prob': min(0.85, top5_prob * 1.1),\n",
    "                    'top3_prob': min(0.70, top3_prob * 1.1),\n",
    "                    'points_prob': min(0.95, points_prob * 1.05),\n",
    "                    'beat_teammate_prob': 0.5,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        self.results['predictions'] = pd.DataFrame(predictions)\n",
    "        logger.info(f\"Generated predictions for {len(predictions)} drivers\")\n",
    "        return self.results['predictions']\n",
    "    \n",
    "    def optimize_picks(self):\n",
    "        \"\"\"Optimize Prize Picks selections\"\"\"\n",
    "        logger.info(\"Optimizing Prize Picks...\")\n",
    "        \n",
    "        if 'predictions' not in self.results or self.results['predictions'].empty:\n",
    "            logger.error(\"No predictions available\")\n",
    "            return None\n",
    "        \n",
    "        if not self.optimizer:\n",
    "            logger.error(\"Optimizer not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Generate all possible picks\n",
    "        all_picks = self.optimizer.generate_all_picks(\n",
    "            self.results['predictions'],\n",
    "            min_edge=self.config.min_edge\n",
    "        )\n",
    "        \n",
    "        if all_picks.empty:\n",
    "            logger.warning(\"No picks with positive edge found\")\n",
    "            return None\n",
    "        \n",
    "        # Optimize portfolio\n",
    "        portfolio = self.optimizer.optimize_portfolio(\n",
    "            all_picks,\n",
    "            bankroll=self.config.bankroll,\n",
    "            constraints=self.config.constraints\n",
    "        )\n",
    "        \n",
    "        self.results['portfolio'] = portfolio\n",
    "        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n",
    "        return portfolio\n",
    "    \n",
    "    def generate_report(self, save_path=None):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        logger.info(\"Generating report...\")\n",
    "        \n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'config': self.config.to_dict(),\n",
    "            'summary': self._generate_summary(),\n",
    "            'predictions': self.results.get('predictions', pd.DataFrame()).to_dict('records'),\n",
    "            'portfolio': self._serialize_portfolio(),\n",
    "            'risk_metrics': self._calculate_risk_metrics()\n",
    "        }\n",
    "        \n",
    "        if save_path is None:\n",
    "            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Report saved to {save_path}\")\n",
    "        return report\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if 'predictions' in self.results and not self.results['predictions'].empty:\n",
    "            summary['n_drivers'] = len(self.results['predictions'])\n",
    "            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n",
    "        \n",
    "        if 'portfolio' in self.results:\n",
    "            portfolio = self.results['portfolio']\n",
    "            summary['n_parlays'] = len(portfolio)\n",
    "            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n",
    "            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n",
    "            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _serialize_portfolio(self):\n",
    "        \"\"\"Serialize portfolio for JSON\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return []\n",
    "        \n",
    "        serialized = []\n",
    "        for parlay in self.results['portfolio']:\n",
    "            parlay_data = {\n",
    "                'n_picks': parlay['n_picks'],\n",
    "                'bet_size': parlay['bet_size'],\n",
    "                'payout': parlay['payout'],\n",
    "                'adjusted_prob': parlay['adjusted_prob'],\n",
    "                'expected_value': parlay['expected_value'],\n",
    "                'kelly_stake': parlay['kelly_stake'],\n",
    "                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n",
    "            }\n",
    "            serialized.append(parlay_data)\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _calculate_risk_metrics(self):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return {}\n",
    "        \n",
    "        portfolio = self.results['portfolio']\n",
    "        \n",
    "        total_exposure = sum(p['bet_size'] for p in portfolio)\n",
    "        \n",
    "        metrics = {\n",
    "            'total_exposure': total_exposure,\n",
    "            'exposure_pct': total_exposure / self.config.bankroll if self.config.bankroll > 0 else 0,\n",
    "            'n_bets': len(portfolio),\n",
    "            'avg_bet_size': total_exposure / len(portfolio) if portfolio else 0,\n",
    "            'max_bet_size': max(p['bet_size'] for p in portfolio) if portfolio else 0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run(self, race_id=None):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        logger.info(\"Starting F1 Prize Picks pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data\n",
    "            self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features\n",
    "            features = self.prepare_features(race_id)\n",
    "            \n",
    "            # Step 3: Generate predictions\n",
    "            predictions = self.generate_predictions()\n",
    "            \n",
    "            # Step 4: Optimize picks\n",
    "            portfolio = self.optimize_picks()\n",
    "            \n",
    "            # Step 5: Generate report\n",
    "            if self.config.save_predictions:\n",
    "                report = self.generate_report()\n",
    "            \n",
    "            logger.info(\"Pipeline completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 16:24:48,797 - F1Pipeline - INFO - Initializing pipeline components...\n",
      "2025-07-23 16:24:48,799 - F1Pipeline - INFO - ✓ Initialized F1DBDataLoader\n",
      "2025-07-23 16:24:48,836 - F1Pipeline - WARNING - ✗ Could not load integrated predictor\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PrizePicksOptimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check if all components are initialized\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initialization_success:\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Initialize and run pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     pipeline = \u001b[43mF1PrizePipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mPipeline initialized\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mF1PrizePipeline.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.explainer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.results = {}\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mF1PrizePipeline._initialize_components\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Initialize optimizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPrizePicksOptimizer\u001b[49m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m         optimizer_config = joblib.load(\u001b[38;5;28mself\u001b[39m.config.model_dir / \u001b[33m'\u001b[39m\u001b[33mf1_prize_picks_optimizer.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'PrizePicksOptimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if all components are initialized\n",
    "if initialization_success:\n",
    "    # Initialize and run pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    logger.info(\"Pipeline initialized\")\n",
    "    \n",
    "    # Run the pipeline\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    # Display results\n",
    "    if results and 'portfolio' in results:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"F1 PRIZE PICKS RECOMMENDATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        portfolio = results['portfolio']\n",
    "        \n",
    "        for i, parlay in enumerate(portfolio, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"PARLAY {i}: {parlay['n_picks']}-PICK ENTRY\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Bet Amount: ${parlay['bet_size']:.2f}\")\n",
    "            print(f\"Potential Payout: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\")\n",
    "            print(f\"Win Probability: {parlay['adjusted_prob']:.1%}\")\n",
    "            print(f\"Expected Value: ${parlay['expected_value'] * parlay['bet_size']:.2f}\")\n",
    "            print(f\"\\nPicks:\")\n",
    "            \n",
    "            picks = parlay['picks']\n",
    "            if hasattr(picks, 'iterrows'):\n",
    "                for j, (_, pick) in enumerate(picks.iterrows(), 1):\n",
    "                    print(f\"  {j}. {pick['driver']} - {pick['bet_type']}\")\n",
    "                    print(f\"     Edge: +{pick['edge']:.1%}\")\n",
    "        \n",
    "        # Summary\n",
    "        summary = pipeline._generate_summary()\n",
    "        if summary:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Total Wagered: ${summary.get('total_wagered', 0):.2f}\")\n",
    "            print(f\"Expected Profit: ${summary.get('expected_profit', 0):.2f}\")\n",
    "            print(f\"Number of Parlays: {summary.get('n_parlays', 0)}\")\n",
    "            print(f\"Average Win Probability: {summary.get('avg_win_probability', 0):.1%}\")\n",
    "    else:\n",
    "        print(\"\\nNo recommendations generated. Check logs for details.\")\n",
    "else:\n",
    "    print(\"\\n❌ Pipeline initialization failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run_pipeline(bankroll=1000, kelly_fraction=0.25):\n",
    "    \"\"\"\n",
    "    Quick function to run the pipeline with custom parameters\n",
    "    \"\"\"\n",
    "    # Update config\n",
    "    config.bankroll = bankroll\n",
    "    config.kelly_fraction = kelly_fraction\n",
    "    \n",
    "    # Initialize and run\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_all_notebooks_fresh():\n",
    "    \"\"\"\n",
    "    Run all notebooks from scratch\n",
    "    \"\"\"\n",
    "    print(\"Running all notebooks from scratch...\")\n",
    "    print(\"This may take 10-15 minutes...\\n\")\n",
    "    \n",
    "    # Force rerun all components\n",
    "    success = initialize_pipeline_components(force_rerun=True)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✅ All notebooks executed successfully!\")\n",
    "        print(\"Now running the main pipeline...\\n\")\n",
    "        \n",
    "        # Run pipeline\n",
    "        pipeline = F1PrizePipeline(config)\n",
    "        results = pipeline.run()\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"\\n❌ Some notebooks failed. Check the logs.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# results = quick_run_pipeline(bankroll=500, kelly_fraction=0.20)\n",
    "# results = run_all_notebooks_fresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Run Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Master Pipeline Integration notebook:\n",
    "\n",
    "1. **Automatically runs prerequisite notebooks** in the correct order\n",
    "2. **Uses the correct f1db_data_loader.py** (not enhanced version)\n",
    "3. **Handles missing components gracefully** with informative error messages\n",
    "4. **Provides a single entry point** for the entire F1 Prize Picks pipeline\n",
    "\n",
    "### Usage:\n",
    "- **First time**: The notebook will automatically run all required notebooks\n",
    "- **Subsequent runs**: It will skip notebooks whose outputs already exist\n",
    "- **Force fresh run**: Use `run_all_notebooks_fresh()` to rebuild everything\n",
    "\n",
    "### Key Functions:\n",
    "- `initialize_pipeline_components()` - Run required notebooks\n",
    "- `quick_run_pipeline()` - Run with custom parameters\n",
    "- `run_all_notebooks_fresh()` - Rebuild everything from scratch\n",
    "\n",
    "The pipeline is now self-contained and can orchestrate the entire F1 prediction system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}