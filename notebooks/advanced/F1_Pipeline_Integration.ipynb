{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Pipeline Integration - Master Controller\n",
    "\n",
    "This notebook orchestrates ALL components of the F1 Prize Picks optimization system.\n",
    "It can run other notebooks programmatically to ensure proper initialization of all components.\n",
    "\n",
    "Key Features:\n",
    "- Automatically runs prerequisite notebooks in correct order\n",
    "- Handles missing models/modules gracefully\n",
    "- Uses the correct f1db_data_loader.py\n",
    "- Provides a single entry point for the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:20:18,945 - F1Pipeline - INFO - Working directory: c:\\Users\\tenei\\Documents\\GitHub\\Formula1\\notebooks\\advanced\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('F1Pipeline')\n",
    "\n",
    "# Set working directory to notebook location (works on both Windows and Linux)\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name != 'advanced':\n",
    "    # Try to find the advanced directory\n",
    "    if (notebook_dir / 'advanced').exists():\n",
    "        os.chdir(notebook_dir / 'advanced')\n",
    "    elif (notebook_dir / 'notebooks' / 'advanced').exists():\n",
    "        os.chdir(notebook_dir / 'notebooks' / 'advanced')\n",
    "    elif notebook_dir.parent.name == 'notebooks' and notebook_dir.name != 'advanced':\n",
    "        advanced_dir = notebook_dir.parent / 'advanced'\n",
    "        if advanced_dir.exists():\n",
    "            os.chdir(advanced_dir)\n",
    "\n",
    "# Add current directory to Python path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "logger.info(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookRunner:\n",
    "    \"\"\"\n",
    "    Utility to run other Jupyter notebooks programmatically\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.executed_notebooks = []\n",
    "        self.failed_notebooks = []\n",
    "    \n",
    "    def run_notebook(self, notebook_path, timeout=600):\n",
    "        \"\"\"\n",
    "        Execute a Jupyter notebook and return success status\n",
    "        \"\"\"\n",
    "        logger.info(f\"Running notebook: {notebook_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Alternative method: Use papermill if available\n",
    "            try:\n",
    "                import papermill as pm\n",
    "                pm.execute_notebook(\n",
    "                    input_path=notebook_path,\n",
    "                    output_path=notebook_path,\n",
    "                    kernel_name='python3',\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                self.executed_notebooks.append(notebook_path)\n",
    "                return True\n",
    "            except ImportError:\n",
    "                logger.info(\"Papermill not available, using nbconvert method\")\n",
    "            \n",
    "            # Original nbconvert method but without --inplace to avoid modifying the original\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "                    '--to', 'notebook',\n",
    "                    '--execute',\n",
    "                    '--ExecutePreprocessor.timeout=' + str(timeout),\n",
    "                    '--ExecutePreprocessor.allow_errors=True',  # Continue on errors\n",
    "                    '--output', 'temp_' + Path(notebook_path).name,\n",
    "                    notebook_path\n",
    "                ],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            # Check if the notebook created expected outputs\n",
    "            if result.returncode == 0:\n",
    "                # Check if expected files were created\n",
    "                expected_outputs = {\n",
    "                    'F1_Model_Fixes_and_Validation.ipynb': 'f1_position_prediction_model.pkl',\n",
    "                    'F1_Integrated_Driver_Evaluation.ipynb': 'f1_integrated_evaluation_model.pkl',\n",
    "                    'F1_Prize_Picks_Optimizer.ipynb': 'f1_prize_picks_optimizer.pkl'\n",
    "                }\n",
    "                \n",
    "                notebook_name = Path(notebook_path).name\n",
    "                if notebook_name in expected_outputs:\n",
    "                    output_file = expected_outputs[notebook_name]\n",
    "                    if Path(output_file).exists():\n",
    "                        logger.info(f\"✓ Successfully created: {output_file}\")\n",
    "                        self.executed_notebooks.append(notebook_path)\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(f\"⚠ Notebook ran but didn't create expected output: {output_file}\")\n",
    "                        # Still mark as success if it's not a critical notebook\n",
    "                        if 'Evaluation' not in notebook_name:\n",
    "                            self.executed_notebooks.append(notebook_path)\n",
    "                            return True\n",
    "                else:\n",
    "                    # Non-critical notebook\n",
    "                    logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                    self.executed_notebooks.append(notebook_path)\n",
    "                    return True\n",
    "            \n",
    "            logger.error(f\"✗ Failed to execute: {notebook_path}\")\n",
    "            if result.stderr:\n",
    "                logger.error(f\"Error: {result.stderr}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Exception running {notebook_path}: {str(e)}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "        finally:\n",
    "            # Clean up temp files\n",
    "            temp_file = Path('temp_' + Path(notebook_path).name)\n",
    "            if temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "    \n",
    "    def run_notebooks_in_order(self, notebook_list):\n",
    "        \"\"\"\n",
    "        Run a list of notebooks in order\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for notebook in notebook_list:\n",
    "            if Path(notebook).exists():\n",
    "                success = self.run_notebook(notebook)\n",
    "                results.append((notebook, success))\n",
    "                # Don't stop on failure for non-critical notebooks\n",
    "                if not success and any(critical in notebook for critical in ['Model_Fixes', 'Feature_Store']):\n",
    "                    logger.warning(f\"Stopping execution due to failure in critical notebook: {notebook}\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(f\"Notebook not found: {notebook}\")\n",
    "                results.append((notebook, False))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get execution summary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'executed': self.executed_notebooks,\n",
    "            'failed': self.failed_notebooks,\n",
    "            'success_rate': len(self.executed_notebooks) / (len(self.executed_notebooks) + len(self.failed_notebooks)) if (self.executed_notebooks or self.failed_notebooks) else 0\n",
    "        }\n",
    "\n",
    "# Initialize runner\n",
    "runner = NotebookRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookRunner:\n",
    "    \"\"\"\n",
    "    Utility to run other Jupyter notebooks programmatically\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.executed_notebooks = []\n",
    "        self.failed_notebooks = []\n",
    "    \n",
    "    def run_notebook(self, notebook_path, timeout=600):\n",
    "        \"\"\"\n",
    "        Execute a Jupyter notebook and return success status\n",
    "        \"\"\"\n",
    "        logger.info(f\"Running notebook: {notebook_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use nbconvert to execute the notebook\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "                    '--to', 'notebook',\n",
    "                    '--execute',\n",
    "                    '--ExecutePreprocessor.timeout=' + str(timeout),\n",
    "                    '--inplace',\n",
    "                    '--clear-output',\n",
    "                    notebook_path\n",
    "                ],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                self.executed_notebooks.append(notebook_path)\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"✗ Failed to execute: {notebook_path}\")\n",
    "                logger.error(f\"Error: {result.stderr}\")\n",
    "                self.failed_notebooks.append(notebook_path)\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Exception running {notebook_path}: {str(e)}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "    \n",
    "    def run_notebooks_in_order(self, notebook_list):\n",
    "        \"\"\"\n",
    "        Run a list of notebooks in order\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for notebook in notebook_list:\n",
    "            if Path(notebook).exists():\n",
    "                success = self.run_notebook(notebook)\n",
    "                results.append((notebook, success))\n",
    "                if not success:\n",
    "                    logger.warning(f\"Stopping execution due to failure in {notebook}\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(f\"Notebook not found: {notebook}\")\n",
    "                results.append((notebook, False))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get execution summary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'executed': self.executed_notebooks,\n",
    "            'failed': self.failed_notebooks,\n",
    "            'success_rate': len(self.executed_notebooks) / (len(self.executed_notebooks) + len(self.failed_notebooks)) if (self.executed_notebooks or self.failed_notebooks) else 0\n",
    "        }\n",
    "\n",
    "# Initialize runner\n",
    "runner = NotebookRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define notebook execution order\n",
    "NOTEBOOK_PIPELINE = [\n",
    "    {\n",
    "        'name': 'Model Fixes and Validation',\n",
    "        'notebook': 'F1_Model_Fixes_and_Validation.ipynb',\n",
    "        'creates': ['f1_position_prediction_model.pkl'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Feature Store',\n",
    "        'notebook': 'F1_Feature_Store.ipynb',\n",
    "        'creates': ['F1_Feature_Store.py'],  # Creates a module\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Integrated Driver Evaluation',\n",
    "        'notebook': 'F1_Integrated_Driver_Evaluation.ipynb',\n",
    "        'creates': ['f1_integrated_evaluation_model.pkl'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Prize Picks Optimizer',\n",
    "        'notebook': 'F1_Prize_Picks_Optimizer.ipynb',\n",
    "        'creates': ['f1_prize_picks_optimizer.pkl'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Explainability Engine',\n",
    "        'notebook': 'F1_Explainability_Engine.ipynb',\n",
    "        'creates': ['F1_Explainability_Engine.py'],  # Creates a module\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'MLflow Tracking',\n",
    "        'notebook': 'F1_MLflow_Tracking.ipynb',\n",
    "        'creates': [],  # Optional tracking\n",
    "        'required': False\n",
    "    }\n",
    "]\n",
    "\n",
    "# Option to force rerun all notebooks (set to True if needed)\n",
    "FORCE_RERUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:20:19,038 - F1Pipeline - INFO - Running notebook: F1_Integrated_Driver_Evaluation.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING F1 PIPELINE COMPONENTS\n",
      "============================================================\n",
      "\n",
      "✓ F1_Model_Fixes_and_Validation.ipynb: outputs exist\n",
      "✓ F1_Feature_Store.ipynb: outputs exist\n",
      "📋 Will run: Integrated Driver Evaluation\n",
      "📋 Will run: Prize Picks Optimizer\n",
      "✓ F1_Explainability_Engine.ipynb: outputs exist\n",
      "✓ F1_MLflow_Tracking.ipynb: outputs exist\n",
      "\n",
      "Running 2 notebooks to create models...\n",
      "This is a production system - all models must be properly trained\n",
      "------------------------------------------------------------\n",
      "\n",
      "Running: Integrated Driver Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:20:33,713 - F1Pipeline - ERROR - ✗ Failed to execute: F1_Integrated_Driver_Evaluation.ipynb\n",
      "2025-07-22 10:20:33,719 - F1Pipeline - ERROR - Error: [NbConvertApp] Converting notebook F1_Integrated_Driver_Evaluation.ipynb to notebook\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\Scripts\\jupyter-nbconvert-script.py\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jupyter_core\\application.py\", line 264, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 346, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 518, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 483, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 412, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 181, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 199, in from_file\n",
      "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\notebook.py\", line 32, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 143, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 318, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\preprocessors\\base.py\", line 47, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 84, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 105, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbclient\\util.py\", line 74, in wrapped\n",
      "    return just_run(coro(*args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbclient\\util.py\", line 53, in just_run\n",
      "    return loop.run_until_complete(coro)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 642, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbclient\\client.py\", line 857, in async_execute_cell\n",
      "    self._check_raise_for_error(cell, exec_reply)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbclient\\client.py\", line 760, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "def identify_prize_picks_value(predictions, driver_metrics, development_df):\n",
      "    \"\"\"\n",
      "    Identify high-value Prize Picks opportunities based on integrated analysis\n",
      "    \"\"\"\n",
      "    # Merge all driver information\n",
      "    value_analysis = predictions.merge(\n",
      "        driver_metrics[['driverId', 'surname', 'avg_points', 'consistency_score', 'recent_form_trend']], \n",
      "        on='driverId'\n",
      "    )\n",
      "    value_analysis = value_analysis.merge(\n",
      "        development_df[['driverId', 'development_phase']], \n",
      "        on='driverId'\n",
      "    )\n",
      "    \n",
      "    # Calculate value scores\n",
      "    value_analysis['value_score'] = (\n",
      "        value_analysis['adjusted_probability'] * \n",
      "        value_analysis['consistency_score'] * \n",
      "        value_analysis['confidence']\n",
      "    )\n",
      "    \n",
      "    # Categorize opportunities\n",
      "    value_analysis['opportunity_type'] = 'Standard'\n",
      "    \n",
      "    # Young talent (pre-peak with good form)\n",
      "    young_talent = (\n",
      "        (value_analysis['development_phase'] == 'pre-peak') & \n",
      "        (value_analysis['recent_form_trend'] > 0)\n",
      "    )\n",
      "    value_analysis.loc[young_talent, 'opportunity_type'] = 'Rising Star'\n",
      "    \n",
      "    # Peak performers with high consistency\n",
      "    peak_performers = (\n",
      "        (value_analysis['development_phase'] == 'peak') & \n",
      "        (value_analysis['consistency_score'] > 0.7)\n",
      "    )\n",
      "    value_analysis.loc[peak_performers, 'opportunity_type'] = 'Safe Bet'\n",
      "    \n",
      "    # Value picks (good probability but potentially overlooked)\n",
      "    value_picks = (\n",
      "        (value_analysis['adjusted_probability'] > 0.6) & \n",
      "        (value_analysis['avg_points'] < 10)\n",
      "    )\n",
      "    value_analysis.loc[value_picks, 'opportunity_type'] = 'Hidden Value'\n",
      "    \n",
      "    # Sort by value score\n",
      "    value_analysis = value_analysis.sort_values('value_score', ascending=False)\n",
      "    \n",
      "    # Visualize opportunities\n",
      "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
      "    \n",
      "    # Value vs Probability scatter\n",
      "    opportunity_colors = {\n",
      "        'Rising Star': 'green',\n",
      "        'Safe Bet': 'blue',\n",
      "        'Hidden Value': 'orange',\n",
      "        'Standard': 'gray'\n",
      "    }\n",
      "    \n",
      "    for opp_type, color in opportunity_colors.items():\n",
      "        mask = value_analysis['opportunity_type'] == opp_type\n",
      "        ax1.scatter(\n",
      "            value_analysis[mask]['adjusted_probability'],\n",
      "            value_analysis[mask]['value_score'],\n",
      "            c=color, label=opp_type, alpha=0.7, s=100\n",
      "        )\n",
      "    \n",
      "    ax1.set_xlabel('Adjusted Probability')\n",
      "    ax1.set_ylabel('Value Score')\n",
      "    ax1.set_title('Prize Picks Value Opportunities')\n",
      "    ax1.legend()\n",
      "    ax1.grid(True, alpha=0.3)\n",
      "    \n",
      "    # Top opportunities by type\n",
      "    opportunity_counts = value_analysis['opportunity_type'].value_counts()\n",
      "    ax2.bar(opportunity_counts.index, opportunity_counts.values)\n",
      "    ax2.set_xlabel('Opportunity Type')\n",
      "    ax2.set_ylabel('Count')\n",
      "    ax2.set_title('Distribution of Betting Opportunities')\n",
      "    ax2.grid(True, alpha=0.3)\n",
      "    \n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "    \n",
      "    return value_analysis\n",
      "\n",
      "# Create sample predictions for demonstration\n",
      "sample_features = pd.DataFrame({\n",
      "    'driverId': driver_metrics['driverId'].head(20),\n",
      "    'grid': np.random.randint(1, 20, 20),\n",
      "    'avg_position_3': np.random.uniform(5, 15, 20)\n",
      "})\n",
      "\n",
      "# Generate predictions\n",
      "predictions = predictor.predict_with_evaluation(\n",
      "    sample_features, \n",
      "    development_df,\n",
      "    compatibility_df\n",
      ")\n",
      "\n",
      "# Identify value opportunities\n",
      "value_opportunities = identify_prize_picks_value(\n",
      "    predictions,\n",
      "    driver_metrics,\n",
      "    development_df\n",
      ")\n",
      "\n",
      "print(\"\\nTop 10 Prize Picks Value Opportunities:\")\n",
      "print(value_opportunities.head(10)[[\n",
      "    'surname', 'adjusted_probability', 'value_score', \n",
      "    'opportunity_type', 'development_phase', 'confidence'\n",
      "]].round(3))\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23776/3237918173.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     94\u001b[0m \u001b[1;31m# Generate predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 95\u001b[1;33m predictions = predictor.predict_with_evaluation(\n",
      "\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0msample_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     97\u001b[0m     \u001b[0mdevelopment_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23776/3268078425.py\u001b[0m in \u001b[0;36mpredict_with_evaluation\u001b[1;34m(self, race_features, driver_evaluation, compatibility)\u001b[0m\n",
      "\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     46\u001b[0m             \u001b[1;31m# Get base prediction (if model is loaded)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m                     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    163\u001b[0m         \u001b[1;34m\"\"\"Return the number of estimators in the ensemble.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'estimators_'\n",
      "AttributeError: 'RandomForestClassifier' object has no attribute 'estimators_'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ FAILED: Integrated Driver Evaluation\n",
      "The pipeline cannot proceed without all models properly trained.\n",
      "\n",
      "To fix this:\n",
      "1. Open F1_Integrated_Driver_Evaluation.ipynb in Jupyter\n",
      "2. Run all cells manually to see the specific error\n",
      "3. Fix any data path or dependency issues\n",
      "4. Ensure the model saves successfully\n",
      "\n",
      "Then run this pipeline again.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pipeline initialization failed. All models must be properly trained for production use. Please fix the failing notebooks and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15176/2287798396.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minitialization_success\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     raise RuntimeError(\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;34m\"Pipeline initialization failed. All models must be properly trained for production use. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;34m\"Please fix the failing notebooks and try again.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Pipeline initialization failed. All models must be properly trained for production use. Please fix the failing notebooks and try again."
     ]
    }
   ],
   "source": [
    "def initialize_pipeline_components(force_rerun=False):\n",
    "    \"\"\"\n",
    "    Initialize all pipeline components by running required notebooks\n",
    "    ALL notebooks must succeed - no fallbacks or compromises\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INITIALIZING F1 PIPELINE COMPONENTS\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # ALL notebooks are required for production\n",
    "    required_outputs = {\n",
    "        'F1_Model_Fixes_and_Validation.ipynb': 'f1_position_prediction_model.pkl',\n",
    "        'F1_Feature_Store.ipynb': None,  # Creates module, not a file\n",
    "        'F1_Integrated_Driver_Evaluation.ipynb': 'f1_integrated_evaluation_model.pkl',\n",
    "        'F1_Prize_Picks_Optimizer.ipynb': 'f1_prize_picks_optimizer.pkl',\n",
    "        'F1_Explainability_Engine.ipynb': None,  # Creates module\n",
    "        'F1_MLflow_Tracking.ipynb': None  # Optional tracking\n",
    "    }\n",
    "    \n",
    "    notebooks_to_run = []\n",
    "    \n",
    "    for notebook, output_file in required_outputs.items():\n",
    "        needs_run = force_rerun\n",
    "        \n",
    "        if not force_rerun and output_file:\n",
    "            # Check if output file exists\n",
    "            if not Path(output_file).exists():\n",
    "                needs_run = True\n",
    "        \n",
    "        if needs_run:\n",
    "            component = next((c for c in NOTEBOOK_PIPELINE if c['notebook'] == notebook), None)\n",
    "            if component:\n",
    "                notebooks_to_run.append(component)\n",
    "                print(f\"📋 Will run: {component['name']}\")\n",
    "        else:\n",
    "            print(f\"✓ {notebook}: outputs exist\")\n",
    "    \n",
    "    if not notebooks_to_run:\n",
    "        print(\"\\n✅ All models are trained and ready!\")\n",
    "        return True\n",
    "    \n",
    "    # Run the notebooks - ALL must succeed\n",
    "    print(f\"\\nRunning {len(notebooks_to_run)} notebooks to create models...\")\n",
    "    print(\"This is a production system - all models must be properly trained\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for component in notebooks_to_run:\n",
    "        print(f\"\\nRunning: {component['name']}...\")\n",
    "        success = runner.run_notebook(component['notebook'], timeout=1200)  # 20 min timeout\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"\\n❌ FAILED: {component['name']}\")\n",
    "            print(\"The pipeline cannot proceed without all models properly trained.\")\n",
    "            print(\"\\nTo fix this:\")\n",
    "            print(f\"1. Open {component['notebook']} in Jupyter\")\n",
    "            print(\"2. Run all cells manually to see the specific error\")\n",
    "            print(\"3. Fix any data path or dependency issues\")\n",
    "            print(\"4. Ensure the model saves successfully\")\n",
    "            print(\"\\nThen run this pipeline again.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"✅ SUCCESS: {component['name']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ ALL MODELS SUCCESSFULLY TRAINED!\")\n",
    "    print(\"=\" * 60)\n",
    "    return True\n",
    "\n",
    "# Initialize components - must succeed\n",
    "initialization_success = initialize_pipeline_components(force_rerun=FORCE_RERUN)\n",
    "\n",
    "if not initialization_success:\n",
    "    raise RuntimeError(\n",
    "        \"Pipeline initialization failed. All models must be properly trained for production use. \"\n",
    "        \"Please fix the failing notebooks and try again.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the F1 pipeline - Windows/Linux compatible\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths - use relative paths that work on both Windows and Linux\n",
    "        current_dir = Path.cwd()\n",
    "        \n",
    "        # Find the data directory relative to current location\n",
    "        if (current_dir / '../../data/f1db').exists():\n",
    "            self.data_dir = (current_dir / '../../data/f1db').resolve()\n",
    "        elif (current_dir.parent.parent / 'data' / 'f1db').exists():\n",
    "            self.data_dir = (current_dir.parent.parent / 'data' / 'f1db').resolve()\n",
    "        else:\n",
    "            # Try to find it from the root\n",
    "            possible_paths = [\n",
    "                current_dir / 'data' / 'f1db',\n",
    "                current_dir.parent / 'data' / 'f1db',\n",
    "                current_dir.parent.parent / 'data' / 'f1db',\n",
    "                Path('data/f1db'),\n",
    "                Path('../data/f1db'),\n",
    "                Path('../../data/f1db')\n",
    "            ]\n",
    "            for p in possible_paths:\n",
    "                if p.exists():\n",
    "                    self.data_dir = p.resolve()\n",
    "                    break\n",
    "            else:\n",
    "                # Default to relative path\n",
    "                self.data_dir = Path('../../data/f1db').resolve()\n",
    "        \n",
    "        self.model_dir = Path('.')\n",
    "        self.output_dir = Path('pipeline_outputs')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Model settings\n",
    "        self.use_cached_data = True\n",
    "        self.auto_sync = True\n",
    "        self.cache_expiry_hours = 24\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.bankroll = 1000\n",
    "        self.kelly_fraction = 0.25\n",
    "        self.max_correlation = 0.5\n",
    "        self.min_edge = 0.05\n",
    "        self.max_exposure = 0.25\n",
    "        \n",
    "        # Constraints\n",
    "        self.constraints = {\n",
    "            'max_per_driver': 2,\n",
    "            'max_per_type': 3,\n",
    "            'min_avg_edge': 0.08\n",
    "        }\n",
    "        \n",
    "        # Pipeline settings\n",
    "        self.generate_report = True\n",
    "        self.save_predictions = True\n",
    "        self.mlflow_tracking = False\n",
    "        \n",
    "        logger.info(f\"Data directory: {self.data_dir}\")\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return {\n",
    "            'data_dir': str(self.data_dir),\n",
    "            'model_dir': str(self.model_dir),\n",
    "            'output_dir': str(self.output_dir),\n",
    "            'bankroll': self.bankroll,\n",
    "            'kelly_fraction': self.kelly_fraction,\n",
    "            'max_correlation': self.max_correlation,\n",
    "            'min_edge': self.min_edge,\n",
    "            'max_exposure': self.max_exposure,\n",
    "            'constraints': self.constraints\n",
    "        }\n",
    "    \n",
    "    def save(self, path='pipeline_config.json'):\n",
    "        \"\"\"Save configuration\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path='pipeline_config.json'):\n",
    "        \"\"\"Load configuration\"\"\"\n",
    "        config = cls()\n",
    "        if Path(path).exists():\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for key, value in data.items():\n",
    "                    if hasattr(config, key):\n",
    "                        if key.endswith('_dir'):\n",
    "                            setattr(config, key, Path(value))\n",
    "                        else:\n",
    "                            setattr(config, key, value)\n",
    "        return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = PipelineConfig()\n",
    "config.save()\n",
    "logger.info(f\"Pipeline configuration initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class F1PrizePipeline:\n",
    "    \"\"\"\n",
    "    Production-grade pipeline orchestrating all components\n",
    "    No fallbacks - all models must be properly trained\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = None\n",
    "        self.feature_store = None\n",
    "        self.predictor = None\n",
    "        self.optimizer = None\n",
    "        self.explainer = None\n",
    "        self.results = {}\n",
    "        \n",
    "        # Initialize all components - fail if any are missing\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all pipeline components - all are required\"\"\"\n",
    "        logger.info(\"Initializing production pipeline components...\")\n",
    "        \n",
    "        # Data loader - REQUIRED\n",
    "        if not F1DBDataLoader:\n",
    "            raise ImportError(\"F1DBDataLoader not available. Cannot proceed without data loader.\")\n",
    "        \n",
    "        self.data_loader = F1DBDataLoader(\n",
    "            data_dir=str(self.config.data_dir)  # Changed from base_path to data_dir\n",
    "        )\n",
    "        logger.info(\"✓ Initialized F1DBDataLoader\")\n",
    "        \n",
    "        # Load integrated predictor - REQUIRED\n",
    "        try:\n",
    "            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n",
    "            logger.info(\"✓ Loaded integrated predictor\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                \"Integrated evaluation model not found. \"\n",
    "                \"Run F1_Integrated_Driver_Evaluation.ipynb to create it.\"\n",
    "            )\n",
    "        \n",
    "        # Load Prize Picks optimizer - REQUIRED\n",
    "        try:\n",
    "            optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n",
    "            self.optimizer = optimizer_config['optimizer']\n",
    "            logger.info(\"✓ Loaded Prize Picks optimizer\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                \"Prize Picks optimizer not found. \"\n",
    "                \"Run F1_Prize_Picks_Optimizer.ipynb to create it.\"\n",
    "            )\n",
    "        \n",
    "        # Feature store - REQUIRED\n",
    "        if not F1FeatureStore:\n",
    "            raise ImportError(\n",
    "                \"F1FeatureStore not available. \"\n",
    "                \"Run F1_Feature_Store.ipynb to create the module.\"\n",
    "            )\n",
    "        self.feature_store = F1FeatureStore()\n",
    "        logger.info(\"✓ Initialized Feature Store\")\n",
    "        \n",
    "        # Explainers - REQUIRED for production\n",
    "        if not PredictionExplainer or not PrizePicksExplainer:\n",
    "            raise ImportError(\n",
    "                \"Explainability components not available. \"\n",
    "                \"Run F1_Explainability_Engine.ipynb to create them.\"\n",
    "            )\n",
    "        \n",
    "        # Initialize explainers with loaded models\n",
    "        self.prediction_explainer = PredictionExplainer(self.predictor, self.feature_store.get_feature_names())\n",
    "        self.pp_explainer = PrizePicksExplainer()\n",
    "        logger.info(\"✓ Initialized Explainability components\")\n",
    "        \n",
    "        # Verify base position prediction model exists\n",
    "        if not Path(self.config.model_dir / 'f1_position_prediction_model.pkl').exists():\n",
    "            raise FileNotFoundError(\n",
    "                \"Base position prediction model not found. \"\n",
    "                \"Run F1_Model_Fixes_and_Validation.ipynb to create it.\"\n",
    "            )\n",
    "        \n",
    "        logger.info(\"✅ All production components initialized successfully\")\n",
    "    \n",
    "    def load_data(self, force_update=False):\n",
    "        \"\"\"Load and prepare F1 data\"\"\"\n",
    "        logger.info(\"Loading F1 data...\")\n",
    "        \n",
    "        # Load data using f1db_data_loader\n",
    "        self.data = load_f1db_data(data_dir=str(self.config.data_dir))  # Changed from base_path\n",
    "        \n",
    "        if not self.data:\n",
    "            raise ValueError(\"Failed to load F1 data. Check data directory and connection.\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} datasets\")\n",
    "        \n",
    "        # Validate critical datasets exist\n",
    "        required_datasets = ['races', 'drivers', 'results', 'constructors']\n",
    "        missing = [ds for ds in required_datasets if ds not in self.data or self.data[ds].empty]\n",
    "        \n",
    "        if missing:\n",
    "            # Check with alternative names\n",
    "            alt_names = {\n",
    "                'results': ['races_race_results', 'race_results'],\n",
    "                'races': ['races'],\n",
    "                'drivers': ['drivers'],\n",
    "                'constructors': ['constructors']\n",
    "            }\n",
    "            \n",
    "            for dataset in missing[:]:\n",
    "                for alt_name in alt_names.get(dataset, []):\n",
    "                    if alt_name in self.data and not self.data[alt_name].empty:\n",
    "                        self.data[dataset] = self.data[alt_name]\n",
    "                        missing.remove(dataset)\n",
    "                        break\n",
    "        \n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required datasets: {missing}\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, race_id=None):\n",
    "        \"\"\"Prepare features for prediction using Feature Store\"\"\"\n",
    "        logger.info(\"Preparing features with Feature Store...\")\n",
    "        \n",
    "        # Get upcoming race if no race_id specified\n",
    "        if race_id is None:\n",
    "            races = self.data.get('races', pd.DataFrame())\n",
    "            if 'date' in races.columns:\n",
    "                races['date'] = pd.to_datetime(races['date'])\n",
    "                upcoming = races[races['date'] > datetime.now()]\n",
    "                if not upcoming.empty:\n",
    "                    upcoming = upcoming.iloc[0]\n",
    "                    race_id = upcoming.get('id', upcoming.get('raceId'))\n",
    "                    race_name = upcoming.get('officialName', upcoming.get('name', 'Unknown'))\n",
    "                    logger.info(f\"Preparing for upcoming race: {race_name} (ID: {race_id})\")\n",
    "            \n",
    "            if race_id is None:\n",
    "                logger.warning(\"No upcoming race found - using latest race\")\n",
    "                race_id = races['id'].max() if 'id' in races.columns else races['raceId'].max()\n",
    "        \n",
    "        # Use Feature Store to engineer features\n",
    "        features = self.feature_store.engineer_features(self.data, race_id)\n",
    "        \n",
    "        if features.empty:\n",
    "            raise ValueError(\"Feature engineering failed. No features generated.\")\n",
    "        \n",
    "        self.results['features'] = features\n",
    "        self.results['race_id'] = race_id\n",
    "        return features\n",
    "    \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions using trained models\"\"\"\n",
    "        logger.info(\"Generating predictions with trained models...\")\n",
    "        \n",
    "        if 'features' not in self.results:\n",
    "            raise ValueError(\"No features available. Run prepare_features first.\")\n",
    "        \n",
    "        features = self.results['features']\n",
    "        \n",
    "        # Load base prediction model\n",
    "        base_model_data = joblib.load(self.config.model_dir / 'f1_position_prediction_model.pkl')\n",
    "        base_model = base_model_data['model']\n",
    "        scaler = base_model_data['scaler']\n",
    "        feature_columns = base_model_data['feature_columns']\n",
    "        \n",
    "        # Get predictions from integrated predictor\n",
    "        predictions = self.predictor.predict_with_evaluation(\n",
    "            features, \n",
    "            self.data.get('driver_evaluation', pd.DataFrame()),\n",
    "            self.data.get('constructor_compatibility', pd.DataFrame())\n",
    "        )\n",
    "        \n",
    "        # Structure predictions for Prize Picks\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['driver', 'driverId', 'top10_prob', 'top5_prob', \n",
    "                        'top3_prob', 'points_prob', 'confidence']\n",
    "        \n",
    "        missing_cols = [col for col in required_cols if col not in predictions_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Predictions missing required columns: {missing_cols}\")\n",
    "        \n",
    "        self.results['predictions'] = predictions_df\n",
    "        logger.info(f\"Generated predictions for {len(predictions_df)} drivers\")\n",
    "        return predictions_df\n",
    "    \n",
    "    def optimize_picks(self):\n",
    "        \"\"\"Optimize Prize Picks selections using trained optimizer\"\"\"\n",
    "        logger.info(\"Optimizing Prize Picks with trained optimizer...\")\n",
    "        \n",
    "        if 'predictions' not in self.results:\n",
    "            raise ValueError(\"No predictions available. Run generate_predictions first.\")\n",
    "        \n",
    "        predictions = self.results['predictions']\n",
    "        \n",
    "        if predictions.empty:\n",
    "            raise ValueError(\"No predictions to optimize\")\n",
    "        \n",
    "        # Generate all possible picks\n",
    "        all_picks = self.optimizer.generate_all_picks(\n",
    "            predictions,\n",
    "            min_edge=self.config.min_edge\n",
    "        )\n",
    "        \n",
    "        if all_picks.empty:\n",
    "            logger.warning(\"No picks with positive edge found\")\n",
    "            return []\n",
    "        \n",
    "        # Optimize portfolio\n",
    "        portfolio = self.optimizer.optimize_portfolio(\n",
    "            all_picks,\n",
    "            bankroll=self.config.bankroll,\n",
    "            constraints=self.config.constraints\n",
    "        )\n",
    "        \n",
    "        if not portfolio:\n",
    "            logger.warning(\"Optimizer returned empty portfolio\")\n",
    "            return []\n",
    "        \n",
    "        self.results['portfolio'] = portfolio\n",
    "        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n",
    "        return portfolio\n",
    "    \n",
    "    def generate_explanations(self):\n",
    "        \"\"\"Generate explanations for recommendations\"\"\"\n",
    "        logger.info(\"Generating explanations...\")\n",
    "        \n",
    "        if 'portfolio' not in self.results:\n",
    "            raise ValueError(\"No portfolio to explain. Run optimize_picks first.\")\n",
    "        \n",
    "        explanations = []\n",
    "        \n",
    "        for parlay in self.results['portfolio']:\n",
    "            explanation = self.pp_explainer.explain_parlay(parlay)\n",
    "            explanations.append(explanation)\n",
    "        \n",
    "        self.results['explanations'] = explanations\n",
    "        return explanations\n",
    "    \n",
    "    def generate_report(self, save_path=None):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        logger.info(\"Generating production report...\")\n",
    "        \n",
    "        # Ensure we have all required components\n",
    "        if 'predictions' not in self.results:\n",
    "            raise ValueError(\"No predictions available for report\")\n",
    "        \n",
    "        if 'portfolio' not in self.results:\n",
    "            logger.warning(\"No portfolio generated - report will be limited\")\n",
    "        \n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'pipeline_version': '1.0.0',\n",
    "            'race_id': self.results.get('race_id'),\n",
    "            'config': self.config.to_dict(),\n",
    "            'summary': self._generate_summary(),\n",
    "            'predictions': self.results['predictions'].to_dict('records'),\n",
    "            'portfolio': self._serialize_portfolio(),\n",
    "            'explanations': self.results.get('explanations', []),\n",
    "            'risk_metrics': self._calculate_risk_metrics(),\n",
    "            'model_info': {\n",
    "                'integrated_predictor': str(type(self.predictor)),\n",
    "                'optimizer': str(type(self.optimizer)),\n",
    "                'feature_store': str(type(self.feature_store))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if save_path is None:\n",
    "            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Report saved to {save_path}\")\n",
    "        return report\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        summary = {\n",
    "            'pipeline_status': 'success',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if 'predictions' in self.results:\n",
    "            summary['n_drivers'] = len(self.results['predictions'])\n",
    "            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n",
    "            summary['top_drivers'] = self.results['predictions'].nlargest(5, 'confidence')['driver'].tolist()\n",
    "        \n",
    "        if 'portfolio' in self.results:\n",
    "            portfolio = self.results['portfolio']\n",
    "            summary['n_parlays'] = len(portfolio)\n",
    "            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n",
    "            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n",
    "            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio]) if portfolio else 0\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _serialize_portfolio(self):\n",
    "        \"\"\"Serialize portfolio for JSON\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return []\n",
    "        \n",
    "        serialized = []\n",
    "        for parlay in self.results['portfolio']:\n",
    "            parlay_data = {\n",
    "                'n_picks': parlay['n_picks'],\n",
    "                'bet_size': parlay['bet_size'],\n",
    "                'payout': parlay['payout'],\n",
    "                'adjusted_prob': parlay['adjusted_prob'],\n",
    "                'expected_value': parlay['expected_value'],\n",
    "                'kelly_stake': parlay['kelly_stake'],\n",
    "                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n",
    "            }\n",
    "            serialized.append(parlay_data)\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _calculate_risk_metrics(self):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        if 'portfolio' not in self.results or not self.results['portfolio']:\n",
    "            return {\n",
    "                'total_exposure': 0,\n",
    "                'exposure_pct': 0,\n",
    "                'n_bets': 0,\n",
    "                'status': 'no_portfolio'\n",
    "            }\n",
    "        \n",
    "        portfolio = self.results['portfolio']\n",
    "        total_exposure = sum(p['bet_size'] for p in portfolio)\n",
    "        \n",
    "        metrics = {\n",
    "            'total_exposure': total_exposure,\n",
    "            'exposure_pct': total_exposure / self.config.bankroll,\n",
    "            'n_bets': len(portfolio),\n",
    "            'avg_bet_size': total_exposure / len(portfolio),\n",
    "            'max_bet_size': max(p['bet_size'] for p in portfolio),\n",
    "            'min_bet_size': min(p['bet_size'] for p in portfolio),\n",
    "            'status': 'calculated'\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run(self, race_id=None):\n",
    "        \"\"\"Run complete production pipeline - no compromises\"\"\"\n",
    "        logger.info(\"Starting F1 Prize Picks production pipeline...\")\n",
    "        logger.info(\"All models must be properly trained - no fallbacks\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data - REQUIRED\n",
    "            self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features - REQUIRED\n",
    "            features = self.prepare_features(race_id)\n",
    "            \n",
    "            # Step 3: Generate predictions - REQUIRED\n",
    "            predictions = self.generate_predictions()\n",
    "            \n",
    "            # Step 4: Optimize picks - REQUIRED\n",
    "            portfolio = self.optimize_picks()\n",
    "            \n",
    "            # Step 5: Generate explanations - REQUIRED\n",
    "            if self.config.generate_report:\n",
    "                self.generate_explanations()\n",
    "            \n",
    "            # Step 6: Generate report - REQUIRED\n",
    "            if self.config.save_predictions:\n",
    "                report = self.generate_report()\n",
    "            \n",
    "            logger.info(\"✅ Production pipeline completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Production pipeline failed: {str(e)}\")\n",
    "            logger.error(\"This is a production system - all components must work properly\")\n",
    "            raise  # Re-raise the exception - no silent failures in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1PrizePipeline:\n",
    "    \"\"\"\n",
    "    Production-grade pipeline orchestrating all components\n",
    "    No fallbacks - all models must be properly trained\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = None\n",
    "        self.feature_store = None\n",
    "        self.predictor = None\n",
    "        self.optimizer = None\n",
    "        self.explainer = None\n",
    "        self.results = {}\n",
    "        \n",
    "        # Initialize all components - fail if any are missing\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all pipeline components - all are required\"\"\"\n",
    "        logger.info(\"Initializing production pipeline components...\")\n",
    "        \n",
    "        # Data loader - REQUIRED\n",
    "        if not F1DBDataLoader:\n",
    "            raise ImportError(\"F1DBDataLoader not available. Cannot proceed without data loader.\")\n",
    "        \n",
    "        self.data_loader = F1DBDataLoader(\n",
    "            base_path=str(self.config.data_dir)\n",
    "        )\n",
    "        logger.info(\"✓ Initialized F1DBDataLoader\")\n",
    "        \n",
    "        # Load integrated predictor - REQUIRED\n",
    "        try:\n",
    "            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n",
    "            logger.info(\"✓ Loaded integrated predictor\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                \"Integrated evaluation model not found. \"\n",
    "                \"Run F1_Integrated_Driver_Evaluation.ipynb to create it.\"\n",
    "            )\n",
    "        \n",
    "        # Load Prize Picks optimizer - REQUIRED\n",
    "        try:\n",
    "            optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n",
    "            self.optimizer = optimizer_config['optimizer']\n",
    "            logger.info(\"✓ Loaded Prize Picks optimizer\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                \"Prize Picks optimizer not found. \"\n",
    "                \"Run F1_Prize_Picks_Optimizer.ipynb to create it.\"\n",
    "            )\n",
    "        \n",
    "        # Feature store - REQUIRED\n",
    "        if not F1FeatureStore:\n",
    "            raise ImportError(\n",
    "                \"F1FeatureStore not available. \"\n",
    "                \"Run F1_Feature_Store.ipynb to create the module.\"\n",
    "            )\n",
    "        self.feature_store = F1FeatureStore()\n",
    "        logger.info(\"✓ Initialized Feature Store\")\n",
    "        \n",
    "        # Explainers - REQUIRED for production\n",
    "        if not PredictionExplainer or not PrizePicksExplainer:\n",
    "            raise ImportError(\n",
    "                \"Explainability components not available. \"\n",
    "                \"Run F1_Explainability_Engine.ipynb to create them.\"\n",
    "            )\n",
    "        \n",
    "        # Initialize explainers with loaded models\n",
    "        self.prediction_explainer = PredictionExplainer(self.predictor, self.feature_store.get_feature_names())\n",
    "        self.pp_explainer = PrizePicksExplainer()\n",
    "        logger.info(\"✓ Initialized Explainability components\")\n",
    "        \n",
    "        # Verify base position prediction model exists\n",
    "        if not Path(self.config.model_dir / 'f1_position_prediction_model.pkl').exists():\n",
    "            raise FileNotFoundError(\n",
    "                \"Base position prediction model not found. \"\n",
    "                \"Run F1_Model_Fixes_and_Validation.ipynb to create it.\"\n",
    "            )\n",
    "        \n",
    "        logger.info(\"✅ All production components initialized successfully\")\n",
    "    \n",
    "    def load_data(self, force_update=False):\n",
    "        \"\"\"Load and prepare F1 data\"\"\"\n",
    "        logger.info(\"Loading F1 data...\")\n",
    "        \n",
    "        # Load data using f1db_data_loader\n",
    "        self.data = load_f1db_data(base_path=str(self.config.data_dir))\n",
    "        \n",
    "        if not self.data:\n",
    "            raise ValueError(\"Failed to load F1 data. Check data directory and connection.\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} datasets\")\n",
    "        \n",
    "        # Validate critical datasets exist\n",
    "        required_datasets = ['races', 'drivers', 'results', 'constructors']\n",
    "        missing = [ds for ds in required_datasets if ds not in self.data or self.data[ds].empty]\n",
    "        \n",
    "        if missing:\n",
    "            # Check with alternative names\n",
    "            alt_names = {\n",
    "                'results': ['races_race_results', 'race_results'],\n",
    "                'races': ['races'],\n",
    "                'drivers': ['drivers'],\n",
    "                'constructors': ['constructors']\n",
    "            }\n",
    "            \n",
    "            for dataset in missing[:]:\n",
    "                for alt_name in alt_names.get(dataset, []):\n",
    "                    if alt_name in self.data and not self.data[alt_name].empty:\n",
    "                        self.data[dataset] = self.data[alt_name]\n",
    "                        missing.remove(dataset)\n",
    "                        break\n",
    "        \n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required datasets: {missing}\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, race_id=None):\n",
    "        \"\"\"Prepare features for prediction using Feature Store\"\"\"\n",
    "        logger.info(\"Preparing features with Feature Store...\")\n",
    "        \n",
    "        # Get upcoming race if no race_id specified\n",
    "        if race_id is None:\n",
    "            races = self.data.get('races', pd.DataFrame())\n",
    "            if 'date' in races.columns:\n",
    "                races['date'] = pd.to_datetime(races['date'])\n",
    "                upcoming = races[races['date'] > datetime.now()]\n",
    "                if not upcoming.empty:\n",
    "                    upcoming = upcoming.iloc[0]\n",
    "                    race_id = upcoming.get('id', upcoming.get('raceId'))\n",
    "                    race_name = upcoming.get('officialName', upcoming.get('name', 'Unknown'))\n",
    "                    logger.info(f\"Preparing for upcoming race: {race_name} (ID: {race_id})\")\n",
    "            \n",
    "            if race_id is None:\n",
    "                logger.warning(\"No upcoming race found - using latest race\")\n",
    "                race_id = races['id'].max() if 'id' in races.columns else races['raceId'].max()\n",
    "        \n",
    "        # Use Feature Store to engineer features\n",
    "        features = self.feature_store.engineer_features(self.data, race_id)\n",
    "        \n",
    "        if features.empty:\n",
    "            raise ValueError(\"Feature engineering failed. No features generated.\")\n",
    "        \n",
    "        self.results['features'] = features\n",
    "        self.results['race_id'] = race_id\n",
    "        return features\n",
    "    \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions using trained models\"\"\"\n",
    "        logger.info(\"Generating predictions with trained models...\")\n",
    "        \n",
    "        if 'features' not in self.results:\n",
    "            raise ValueError(\"No features available. Run prepare_features first.\")\n",
    "        \n",
    "        features = self.results['features']\n",
    "        \n",
    "        # Load base prediction model\n",
    "        base_model_data = joblib.load(self.config.model_dir / 'f1_position_prediction_model.pkl')\n",
    "        base_model = base_model_data['model']\n",
    "        scaler = base_model_data['scaler']\n",
    "        feature_columns = base_model_data['feature_columns']\n",
    "        \n",
    "        # Get predictions from integrated predictor\n",
    "        predictions = self.predictor.predict_with_evaluation(\n",
    "            features, \n",
    "            self.data.get('driver_evaluation', pd.DataFrame()),\n",
    "            self.data.get('constructor_compatibility', pd.DataFrame())\n",
    "        )\n",
    "        \n",
    "        # Structure predictions for Prize Picks\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['driver', 'driverId', 'top10_prob', 'top5_prob', \n",
    "                        'top3_prob', 'points_prob', 'confidence']\n",
    "        \n",
    "        missing_cols = [col for col in required_cols if col not in predictions_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Predictions missing required columns: {missing_cols}\")\n",
    "        \n",
    "        self.results['predictions'] = predictions_df\n",
    "        logger.info(f\"Generated predictions for {len(predictions_df)} drivers\")\n",
    "        return predictions_df\n",
    "    \n",
    "    def optimize_picks(self):\n",
    "        \"\"\"Optimize Prize Picks selections using trained optimizer\"\"\"\n",
    "        logger.info(\"Optimizing Prize Picks with trained optimizer...\")\n",
    "        \n",
    "        if 'predictions' not in self.results:\n",
    "            raise ValueError(\"No predictions available. Run generate_predictions first.\")\n",
    "        \n",
    "        predictions = self.results['predictions']\n",
    "        \n",
    "        if predictions.empty:\n",
    "            raise ValueError(\"No predictions to optimize\")\n",
    "        \n",
    "        # Generate all possible picks\n",
    "        all_picks = self.optimizer.generate_all_picks(\n",
    "            predictions,\n",
    "            min_edge=self.config.min_edge\n",
    "        )\n",
    "        \n",
    "        if all_picks.empty:\n",
    "            logger.warning(\"No picks with positive edge found\")\n",
    "            return []\n",
    "        \n",
    "        # Optimize portfolio\n",
    "        portfolio = self.optimizer.optimize_portfolio(\n",
    "            all_picks,\n",
    "            bankroll=self.config.bankroll,\n",
    "            constraints=self.config.constraints\n",
    "        )\n",
    "        \n",
    "        if not portfolio:\n",
    "            logger.warning(\"Optimizer returned empty portfolio\")\n",
    "            return []\n",
    "        \n",
    "        self.results['portfolio'] = portfolio\n",
    "        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n",
    "        return portfolio\n",
    "    \n",
    "    def generate_explanations(self):\n",
    "        \"\"\"Generate explanations for recommendations\"\"\"\n",
    "        logger.info(\"Generating explanations...\")\n",
    "        \n",
    "        if 'portfolio' not in self.results:\n",
    "            raise ValueError(\"No portfolio to explain. Run optimize_picks first.\")\n",
    "        \n",
    "        explanations = []\n",
    "        \n",
    "        for parlay in self.results['portfolio']:\n",
    "            explanation = self.pp_explainer.explain_parlay(parlay)\n",
    "            explanations.append(explanation)\n",
    "        \n",
    "        self.results['explanations'] = explanations\n",
    "        return explanations\n",
    "    \n",
    "    def generate_report(self, save_path=None):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        logger.info(\"Generating production report...\")\n",
    "        \n",
    "        # Ensure we have all required components\n",
    "        if 'predictions' not in self.results:\n",
    "            raise ValueError(\"No predictions available for report\")\n",
    "        \n",
    "        if 'portfolio' not in self.results:\n",
    "            logger.warning(\"No portfolio generated - report will be limited\")\n",
    "        \n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'pipeline_version': '1.0.0',\n",
    "            'race_id': self.results.get('race_id'),\n",
    "            'config': self.config.to_dict(),\n",
    "            'summary': self._generate_summary(),\n",
    "            'predictions': self.results['predictions'].to_dict('records'),\n",
    "            'portfolio': self._serialize_portfolio(),\n",
    "            'explanations': self.results.get('explanations', []),\n",
    "            'risk_metrics': self._calculate_risk_metrics(),\n",
    "            'model_info': {\n",
    "                'integrated_predictor': str(type(self.predictor)),\n",
    "                'optimizer': str(type(self.optimizer)),\n",
    "                'feature_store': str(type(self.feature_store))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if save_path is None:\n",
    "            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Report saved to {save_path}\")\n",
    "        return report\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        summary = {\n",
    "            'pipeline_status': 'success',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if 'predictions' in self.results:\n",
    "            summary['n_drivers'] = len(self.results['predictions'])\n",
    "            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n",
    "            summary['top_drivers'] = self.results['predictions'].nlargest(5, 'confidence')['driver'].tolist()\n",
    "        \n",
    "        if 'portfolio' in self.results:\n",
    "            portfolio = self.results['portfolio']\n",
    "            summary['n_parlays'] = len(portfolio)\n",
    "            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n",
    "            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n",
    "            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio]) if portfolio else 0\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _serialize_portfolio(self):\n",
    "        \"\"\"Serialize portfolio for JSON\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return []\n",
    "        \n",
    "        serialized = []\n",
    "        for parlay in self.results['portfolio']:\n",
    "            parlay_data = {\n",
    "                'n_picks': parlay['n_picks'],\n",
    "                'bet_size': parlay['bet_size'],\n",
    "                'payout': parlay['payout'],\n",
    "                'adjusted_prob': parlay['adjusted_prob'],\n",
    "                'expected_value': parlay['expected_value'],\n",
    "                'kelly_stake': parlay['kelly_stake'],\n",
    "                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n",
    "            }\n",
    "            serialized.append(parlay_data)\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _calculate_risk_metrics(self):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        if 'portfolio' not in self.results or not self.results['portfolio']:\n",
    "            return {\n",
    "                'total_exposure': 0,\n",
    "                'exposure_pct': 0,\n",
    "                'n_bets': 0,\n",
    "                'status': 'no_portfolio'\n",
    "            }\n",
    "        \n",
    "        portfolio = self.results['portfolio']\n",
    "        total_exposure = sum(p['bet_size'] for p in portfolio)\n",
    "        \n",
    "        metrics = {\n",
    "            'total_exposure': total_exposure,\n",
    "            'exposure_pct': total_exposure / self.config.bankroll,\n",
    "            'n_bets': len(portfolio),\n",
    "            'avg_bet_size': total_exposure / len(portfolio),\n",
    "            'max_bet_size': max(p['bet_size'] for p in portfolio),\n",
    "            'min_bet_size': min(p['bet_size'] for p in portfolio),\n",
    "            'status': 'calculated'\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run(self, race_id=None):\n",
    "        \"\"\"Run complete production pipeline - no compromises\"\"\"\n",
    "        logger.info(\"Starting F1 Prize Picks production pipeline...\")\n",
    "        logger.info(\"All models must be properly trained - no fallbacks\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data - REQUIRED\n",
    "            self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features - REQUIRED\n",
    "            features = self.prepare_features(race_id)\n",
    "            \n",
    "            # Step 3: Generate predictions - REQUIRED\n",
    "            predictions = self.generate_predictions()\n",
    "            \n",
    "            # Step 4: Optimize picks - REQUIRED\n",
    "            portfolio = self.optimize_picks()\n",
    "            \n",
    "            # Step 5: Generate explanations - REQUIRED\n",
    "            if self.config.generate_report:\n",
    "                self.generate_explanations()\n",
    "            \n",
    "            # Step 6: Generate report - REQUIRED\n",
    "            if self.config.save_predictions:\n",
    "                report = self.generate_report()\n",
    "            \n",
    "            logger.info(\"✅ Production pipeline completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Production pipeline failed: {str(e)}\")\n",
    "            logger.error(\"This is a production system - all components must work properly\")\n",
    "            raise  # Re-raise the exception - no silent failures in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all components are initialized successfully\n",
    "if initialization_success:\n",
    "    try:\n",
    "        # Initialize production pipeline\n",
    "        pipeline = F1PrizePipeline(config)\n",
    "        logger.info(\"Production pipeline initialized successfully\")\n",
    "        \n",
    "        # Run the pipeline\n",
    "        logger.info(\"Running production pipeline...\")\n",
    "        results = pipeline.run()\n",
    "        \n",
    "        # Display results\n",
    "        if results and 'portfolio' in results:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"F1 PRIZE PICKS RECOMMENDATIONS - PRODUCTION\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            portfolio = results['portfolio']\n",
    "            \n",
    "            if portfolio:\n",
    "                for i, parlay in enumerate(portfolio, 1):\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"PARLAY {i}: {parlay['n_picks']}-PICK ENTRY\")\n",
    "                    print(f\"{'='*60}\")\n",
    "                    print(f\"Bet Amount: ${parlay['bet_size']:.2f}\")\n",
    "                    print(f\"Potential Payout: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\")\n",
    "                    print(f\"Win Probability: {parlay['adjusted_prob']:.1%}\")\n",
    "                    print(f\"Expected Value: +{parlay['expected_value']:.1%}\")\n",
    "                    print(f\"\\nPicks:\")\n",
    "                    \n",
    "                    picks = parlay['picks']\n",
    "                    if hasattr(picks, 'iterrows'):\n",
    "                        for j, (_, pick) in enumerate(picks.iterrows(), 1):\n",
    "                            print(f\"  {j}. {pick['driver']} - {pick['bet_type']}\")\n",
    "                            print(f\"     Probability: {pick['probability']:.1%}\")\n",
    "                            print(f\"     Edge: +{pick['edge']:.1%}\")\n",
    "                \n",
    "                # Summary\n",
    "                summary = pipeline._generate_summary()\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"PORTFOLIO SUMMARY\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"Total Wagered: ${summary.get('total_wagered', 0):.2f}\")\n",
    "                print(f\"Expected Profit: ${summary.get('expected_profit', 0):.2f}\")\n",
    "                print(f\"Number of Parlays: {summary.get('n_parlays', 0)}\")\n",
    "                print(f\"Average Win Probability: {summary.get('avg_win_probability', 0):.1%}\")\n",
    "                print(f\"\\nTop 5 Drivers by Confidence:\")\n",
    "                for driver in summary.get('top_drivers', []):\n",
    "                    print(f\"  - {driver}\")\n",
    "            else:\n",
    "                print(\"\\nNo parlays generated. This could mean:\")\n",
    "                print(\"- No bets met the minimum edge requirement\")\n",
    "                print(\"- Risk constraints prevented bet placement\")\n",
    "                print(\"- Try adjusting config.min_edge or config.kelly_fraction\")\n",
    "        else:\n",
    "            print(\"\\n❌ Pipeline completed but no portfolio was generated.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Pipeline execution failed: {str(e)}\")\n",
    "        print(\"\\nThis is a production system. Please ensure:\")\n",
    "        print(\"1. All notebooks have been run successfully\")\n",
    "        print(\"2. All models are properly trained and saved\")\n",
    "        print(\"3. Data is available in the correct format\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n❌ Cannot run pipeline - initialization failed\")\n",
    "    print(\"Please fix the failing notebooks first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a production-grade F1 Prize Picks pipeline that:\n",
    "- Automatically runs prerequisite notebooks in correct order\n",
    "- Ensures all models are properly trained with no fallbacks\n",
    "- Uses real F1DB data from the correct data path\n",
    "- Provides comprehensive race weekend automation and performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceWeekendAutomation:\n",
    "    \"\"\"\n",
    "    Automate pipeline execution for race weekends\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: F1PrizePipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.schedule = []\n",
    "    \n",
    "    def get_race_schedule(self):\n",
    "        \"\"\"Get upcoming race schedule\"\"\"\n",
    "        races = self.pipeline.data.get('races', pd.DataFrame())\n",
    "        if races.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get future races\n",
    "        races['date'] = pd.to_datetime(races['date'])\n",
    "        future_races = races[races['date'] > datetime.now()]\n",
    "        \n",
    "        return future_races.sort_values('date')\n",
    "    \n",
    "    def schedule_race_analysis(self, race_id, race_date):\n",
    "        \"\"\"Schedule analysis for a specific race\"\"\"\n",
    "        # Run at different times\n",
    "        schedule_times = [\n",
    "            (race_date - timedelta(days=3), 'Initial Analysis'),\n",
    "            (race_date - timedelta(days=1), 'Pre-Qualifying Update'),\n",
    "            (race_date - timedelta(hours=4), 'Final Predictions')\n",
    "        ]\n",
    "        \n",
    "        for run_time, description in schedule_times:\n",
    "            self.schedule.append({\n",
    "                'race_id': race_id,\n",
    "                'run_time': run_time,\n",
    "                'description': description,\n",
    "                'status': 'scheduled'\n",
    "            })\n",
    "    \n",
    "    def execute_scheduled_runs(self):\n",
    "        \"\"\"Execute scheduled pipeline runs\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        for task in self.schedule:\n",
    "            if task['status'] == 'scheduled' and task['run_time'] <= current_time:\n",
    "                logger.info(f\"Executing {task['description']} for race {task['race_id']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Update config based on timing\n",
    "                    if 'Final' in task['description']:\n",
    "                        self.pipeline.config.kelly_fraction = 0.20  # More conservative\n",
    "                    \n",
    "                    # Run pipeline\n",
    "                    results = self.pipeline.run(task['race_id'])\n",
    "                    \n",
    "                    # Save results with timestamp\n",
    "                    output_name = f\"race_{task['race_id']}_{task['description'].replace(' ', '_')}_{current_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "                    self.pipeline.generate_report(\n",
    "                        self.pipeline.config.output_dir / f\"{output_name}.json\"\n",
    "                    )\n",
    "                    \n",
    "                    task['status'] = 'completed'\n",
    "                    task['completed_at'] = current_time.isoformat()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to execute {task['description']}: {str(e)}\")\n",
    "                    task['status'] = 'failed'\n",
    "                    task['error'] = str(e)\n",
    "    \n",
    "    def generate_weekend_summary(self):\n",
    "        \"\"\"Generate summary of all analyses for a race weekend\"\"\"\n",
    "        completed_tasks = [t for t in self.schedule if t['status'] == 'completed']\n",
    "        \n",
    "        if not completed_tasks:\n",
    "            return None\n",
    "        \n",
    "        summary = {\n",
    "            'race_id': completed_tasks[0]['race_id'],\n",
    "            'analyses_completed': len(completed_tasks),\n",
    "            'final_recommendations': None\n",
    "        }\n",
    "        \n",
    "        # Get final predictions\n",
    "        final_task = next((t for t in completed_tasks if 'Final' in t['description']), None)\n",
    "        if final_task:\n",
    "            # Load the report\n",
    "            report_files = list(self.pipeline.config.output_dir.glob(f\"race_{final_task['race_id']}_Final*.json\"))\n",
    "            if report_files:\n",
    "                with open(report_files[-1], 'r') as f:\n",
    "                    final_report = json.load(f)\n",
    "                    summary['final_recommendations'] = final_report.get('portfolio', [])\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Example usage for race weekend automation\n",
    "if 'pipeline' in locals() and hasattr(pipeline, 'data') and pipeline.data:\n",
    "    automation = RaceWeekendAutomation(pipeline)\n",
    "    \n",
    "    # Get upcoming races\n",
    "    upcoming_races = automation.get_race_schedule()\n",
    "    if not upcoming_races.empty:\n",
    "        print(\"\\nUpcoming Races:\")\n",
    "        print(\"=\" * 60)\n",
    "        for idx, race in upcoming_races.head(3).iterrows():\n",
    "            print(f\"{race['date'].strftime('%Y-%m-%d')}: {race['name']} (Round {race['round']})\")\n",
    "            \n",
    "            # Schedule analysis for next race\n",
    "            if idx == upcoming_races.index[0]:  # First race\n",
    "                automation.schedule_race_analysis(race['raceId'], race['date'])\n",
    "        \n",
    "        print(f\"\\nScheduled {len(automation.schedule)} analyses for next race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Monitoring (From Original Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor pipeline and prediction performance\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = []\n",
    "    \n",
    "    def track_predictions(self, predictions, actuals=None):\n",
    "        \"\"\"Track prediction accuracy\"\"\"\n",
    "        metric = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_predictions': len(predictions),\n",
    "            'avg_confidence': predictions['confidence'].mean() if 'confidence' in predictions else 0\n",
    "        }\n",
    "        \n",
    "        if actuals is not None:\n",
    "            # Calculate accuracy metrics\n",
    "            metric['accuracy'] = self._calculate_accuracy(predictions, actuals)\n",
    "        \n",
    "        self.metrics.append(metric)\n",
    "    \n",
    "    def _calculate_accuracy(self, predictions, actuals):\n",
    "        \"\"\"Calculate prediction accuracy\"\"\"\n",
    "        # Implementation depends on actual data format\n",
    "        # This is a placeholder - implement based on your needs\n",
    "        return 0.0\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"Generate performance report\"\"\"\n",
    "        if not self.metrics:\n",
    "            return None\n",
    "        \n",
    "        report = {\n",
    "            'period': {\n",
    "                'start': self.metrics[0]['timestamp'],\n",
    "                'end': self.metrics[-1]['timestamp']\n",
    "            },\n",
    "            'total_predictions': sum(m['n_predictions'] for m in self.metrics),\n",
    "            'avg_confidence': np.mean([m['avg_confidence'] for m in self.metrics]),\n",
    "            'runs_completed': len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def plot_performance_trends(self):\n",
    "        \"\"\"Plot performance trends over time\"\"\"\n",
    "        if not self.metrics:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        timestamps = [pd.to_datetime(m['timestamp']) for m in self.metrics]\n",
    "        confidences = [m['avg_confidence'] for m in self.metrics]\n",
    "        \n",
    "        ax.plot(timestamps, confidences, marker='o')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Average Confidence')\n",
    "        ax.set_title('Model Confidence Over Time')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save metrics to file\"\"\"\n",
    "        metrics_file = self.output_dir / 'performance_metrics.json'\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        logger.info(f\"Saved performance metrics to {metrics_file}\")\n",
    "    \n",
    "    def load_metrics(self):\n",
    "        \"\"\"Load existing metrics\"\"\"\n",
    "        metrics_file = self.output_dir / 'performance_metrics.json'\n",
    "        if metrics_file.exists():\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                self.metrics = json.load(f)\n",
    "            logger.info(f\"Loaded {len(self.metrics)} historical metrics\")\n",
    "\n",
    "# Initialize performance monitor\n",
    "if 'config' in locals():\n",
    "    monitor = PerformanceMonitor(config.output_dir)\n",
    "    monitor.load_metrics()  # Load any existing metrics\n",
    "    \n",
    "    # Track current predictions if available\n",
    "    if 'pipeline' in locals() and 'predictions' in pipeline.results:\n",
    "        monitor.track_predictions(pipeline.results['predictions'])\n",
    "        monitor.save_metrics()\n",
    "        \n",
    "        performance_report = monitor.generate_performance_report()\n",
    "        print(\"\\nPerformance Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        for key, value in performance_report.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline state saved to pipeline_outputs\\pipeline_state.json\n",
      "\n",
      "Standalone run script created: run_f1_pipeline.py\n",
      "\n",
      "Usage examples:\n",
      "  python run_f1_pipeline.py                  # Run for upcoming race\n",
      "  python run_f1_pipeline.py --race-id 1234   # Run for specific race\n",
      "  python run_f1_pipeline.py --backtest       # Run backtesting\n",
      "  python run_f1_pipeline.py --schedule       # Schedule race weekend automation\n"
     ]
    }
   ],
   "source": [
    "# Save complete pipeline state for future use\n",
    "pipeline_state = {\n",
    "    'config': config.to_dict(),\n",
    "    'last_run': datetime.now().isoformat(),\n",
    "    'results_summary': pipeline._generate_summary() if 'pipeline' in locals() else {},\n",
    "    'automation_schedule': automation.schedule if 'automation' in locals() else [],\n",
    "    'performance_metrics': monitor.metrics[-10:] if 'monitor' in locals() else [],  # Last 10 metrics\n",
    "    'notebook_execution': runner.get_summary() if 'runner' in locals() else {}\n",
    "}\n",
    "\n",
    "# Save state\n",
    "state_path = config.output_dir / 'pipeline_state.json'\n",
    "with open(state_path, 'w') as f:\n",
    "    json.dump(pipeline_state, f, indent=2)\n",
    "\n",
    "print(f\"\\nPipeline state saved to {state_path}\")\n",
    "\n",
    "# Create standalone run script for easy execution\n",
    "run_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run F1 Prize Picks Pipeline\n",
    "\n",
    "Usage:\n",
    "    python run_f1_pipeline.py              # Run for upcoming race\n",
    "    python run_f1_pipeline.py --race-id 1234  # Run for specific race\n",
    "    python run_f1_pipeline.py --backtest   # Run backtesting\n",
    "    python run_f1_pipeline.py --schedule   # Schedule race weekend automation\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Add notebook directory to path\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "\n",
    "def run_master_notebook(race_id=None, mode='predict'):\n",
    "    \"\"\"Run the master pipeline notebook\"\"\"\n",
    "    # Convert notebook to script first\n",
    "    subprocess.run([\n",
    "        sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "        '--to', 'script',\n",
    "        'F1_Pipeline_Integration_Master.ipynb'\n",
    "    ])\n",
    "    \n",
    "    # Import and run\n",
    "    from F1_Pipeline_Integration_Master import F1PrizePipeline, PipelineConfig\n",
    "    from F1_Pipeline_Integration_Master import RaceWeekendAutomation, PerformanceMonitor\n",
    "    \n",
    "    # Load configuration\n",
    "    config = PipelineConfig.load()\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    \n",
    "    if mode == 'schedule':\n",
    "        # Run automation\n",
    "        automation = RaceWeekendAutomation(pipeline)\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        upcoming = automation.get_race_schedule()\n",
    "        if not upcoming.empty:\n",
    "            next_race = upcoming.iloc[0]\n",
    "            automation.schedule_race_analysis(next_race['raceId'], next_race['date'])\n",
    "            print(f\"Scheduled analyses for {next_race['name']}\")\n",
    "            automation.execute_scheduled_runs()\n",
    "    elif mode == 'backtest':\n",
    "        print(\"Running backtesting...\")\n",
    "        # Import and run backtesting notebook\n",
    "        subprocess.run([\n",
    "            sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "            '--to', 'notebook',\n",
    "            '--execute',\n",
    "            'F1_Backtesting_Framework.ipynb'\n",
    "        ])\n",
    "    else:\n",
    "        # Normal prediction mode\n",
    "        results = pipeline.run(race_id)\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\\\nPipeline completed successfully!\")\n",
    "            print(f\"Results saved to {config.output_dir}\")\n",
    "            \n",
    "            # Track performance\n",
    "            monitor = PerformanceMonitor(config.output_dir)\n",
    "            monitor.load_metrics()\n",
    "            monitor.track_predictions(results.get('predictions', pd.DataFrame()))\n",
    "            monitor.save_metrics()\n",
    "        else:\n",
    "            print(\"\\\\nPipeline failed. Check logs for details.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Run F1 Prize Picks Pipeline')\n",
    "    parser.add_argument('--race-id', type=int, help='Specific race ID to analyze')\n",
    "    parser.add_argument('--backtest', action='store_true', \n",
    "                       help='Run backtesting instead of predictions')\n",
    "    parser.add_argument('--schedule', action='store_true',\n",
    "                       help='Schedule automated race weekend analyses')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.backtest:\n",
    "        run_master_notebook(mode='backtest')\n",
    "    elif args.schedule:\n",
    "        run_master_notebook(mode='schedule')\n",
    "    else:\n",
    "        run_master_notebook(race_id=args.race_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save run script\n",
    "script_path = Path('run_f1_pipeline.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(run_script)\n",
    "\n",
    "# Make it executable on Unix-like systems\n",
    "import os\n",
    "if os.name != 'nt':  # Not Windows\n",
    "    os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"\\nStandalone run script created: {script_path}\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  python run_f1_pipeline.py                  # Run for upcoming race\")\n",
    "print(\"  python run_f1_pipeline.py --race-id 1234   # Run for specific race\")\n",
    "print(\"  python run_f1_pipeline.py --backtest       # Run backtesting\")\n",
    "print(\"  python run_f1_pipeline.py --schedule       # Schedule race weekend automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Components (Using Correct Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:03:24,719 - F1Pipeline - INFO - ✓ Loaded f1db_data_loader\n",
      "2025-07-22 10:03:24,719 - F1Pipeline - WARNING - ✗ F1FeatureStore not available - run F1_Feature_Store.ipynb first\n",
      "2025-07-22 10:03:24,727 - F1Pipeline - WARNING - ✗ IntegratedF1Predictor not available\n",
      "2025-07-22 10:03:24,727 - F1Pipeline - WARNING - ✗ PrizePicksOptimizer not available\n",
      "2025-07-22 10:03:24,727 - F1Pipeline - WARNING - ✗ Explainability components not available\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary components with proper error handling\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Data loading - Using the correct f1db_data_loader.py\n",
    "try:\n",
    "    from f1db_data_loader import F1DBDataLoader, load_f1db_data\n",
    "    logger.info(\"✓ Loaded f1db_data_loader\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"✗ Could not import f1db_data_loader: {e}\")\n",
    "    F1DBDataLoader = None\n",
    "    load_f1db_data = None\n",
    "\n",
    "# Feature engineering\n",
    "try:\n",
    "    # Try to import from the notebook-generated module\n",
    "    from F1_Feature_Store import F1FeatureStore\n",
    "    logger.info(\"✓ Loaded F1FeatureStore\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ F1FeatureStore not available - run F1_Feature_Store.ipynb first\")\n",
    "    F1FeatureStore = None\n",
    "\n",
    "# Model components\n",
    "try:\n",
    "    from F1_Integrated_Driver_Evaluation import IntegratedF1Predictor\n",
    "    logger.info(\"✓ Loaded IntegratedF1Predictor\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ IntegratedF1Predictor not available\")\n",
    "    IntegratedF1Predictor = None\n",
    "\n",
    "# Optimization\n",
    "try:\n",
    "    from F1_Prize_Picks_Optimizer import PrizePicksOptimizer, KellyCriterion, PrizePicksBetTypes\n",
    "    logger.info(\"✓ Loaded PrizePicksOptimizer\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ PrizePicksOptimizer not available\")\n",
    "    PrizePicksOptimizer = None\n",
    "\n",
    "# Explainability\n",
    "try:\n",
    "    from F1_Explainability_Engine import PredictionExplainer, PrizePicksExplainer\n",
    "    logger.info(\"✓ Loaded Explainability components\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ Explainability components not available\")\n",
    "    PredictionExplainer = None\n",
    "    PrizePicksExplainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1PrizePipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrating all components\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = None\n",
    "        self.feature_store = None\n",
    "        self.predictor = None\n",
    "        self.optimizer = None\n",
    "        self.explainer = None\n",
    "        self.results = {}\n",
    "        \n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all pipeline components\"\"\"\n",
    "        logger.info(\"Initializing pipeline components...\")\n",
    "        \n",
    "        # Data loader - Using f1db_data_loader\n",
    "        if F1DBDataLoader:\n",
    "            self.data_loader = F1DBDataLoader(\n",
    "                data_dir=str(self.config.data_dir)  # Changed from base_path\n",
    "            )\n",
    "            logger.info(\"✓ Initialized F1DBDataLoader\")\n",
    "        else:\n",
    "            logger.error(\"✗ F1DBDataLoader not available\")\n",
    "        \n",
    "        # Load saved models if available\n",
    "        try:\n",
    "            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n",
    "            logger.info(\"✓ Loaded integrated predictor\")\n",
    "        except:\n",
    "            logger.warning(\"✗ Could not load integrated predictor\")\n",
    "            self.predictor = None\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if PrizePicksOptimizer:\n",
    "            try:\n",
    "                optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n",
    "                self.optimizer = optimizer_config['optimizer']\n",
    "                logger.info(\"✓ Loaded Prize Picks optimizer\")\n",
    "            except:\n",
    "                logger.warning(\"Creating new optimizer\")\n",
    "                self.optimizer = PrizePicksOptimizer(\n",
    "                    kelly_fraction=self.config.kelly_fraction,\n",
    "                    max_correlation=self.config.max_correlation\n",
    "                )\n",
    "        \n",
    "        # Feature store\n",
    "        if F1FeatureStore:\n",
    "            self.feature_store = F1FeatureStore()\n",
    "            logger.info(\"✓ Initialized Feature Store\")\n",
    "        \n",
    "        # Explainers\n",
    "        if PredictionExplainer:\n",
    "            self.prediction_explainer = PredictionExplainer(None, [])\n",
    "        if PrizePicksExplainer:\n",
    "            self.pp_explainer = PrizePicksExplainer()\n",
    "        \n",
    "        logger.info(\"Pipeline components initialization complete\")\n",
    "    \n",
    "    def load_data(self, force_update=False):\n",
    "        \"\"\"Load and prepare F1 data\"\"\"\n",
    "        logger.info(\"Loading F1 data...\")\n",
    "        \n",
    "        if not self.data_loader:\n",
    "            logger.error(\"Data loader not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Load data using f1db_data_loader\n",
    "        if load_f1db_data:\n",
    "            self.data = load_f1db_data(data_dir=str(self.config.data_dir))  # Changed from base_path\n",
    "        else:\n",
    "            # Fallback to loading core datasets\n",
    "            self.data = self.data_loader.get_core_datasets()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} datasets\")\n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, race_id=None):\n",
    "        \"\"\"Prepare features for prediction\"\"\"\n",
    "        logger.info(\"Preparing features...\")\n",
    "        \n",
    "        # Get upcoming race if no race_id specified\n",
    "        if race_id is None:\n",
    "            races = self.data.get('races', pd.DataFrame())\n",
    "            if not races.empty:\n",
    "                races['date'] = pd.to_datetime(races['date'])\n",
    "                upcoming = races[races['date'] > datetime.now()].iloc[0]\n",
    "                race_id = upcoming['raceId']\n",
    "                logger.info(f\"Preparing for upcoming race: {upcoming['name']}\")\n",
    "            else:\n",
    "                logger.warning(\"No upcoming race found\")\n",
    "                return None\n",
    "        \n",
    "        # Build feature set\n",
    "        if self.feature_store and hasattr(self.feature_store, 'engineer_features'):\n",
    "            features = self.feature_store.engineer_features(self.data)\n",
    "        else:\n",
    "            # Basic feature preparation\n",
    "            features = self._create_basic_features()\n",
    "        \n",
    "        self.results['features'] = features\n",
    "        return features\n",
    "    \n",
    "    def _create_basic_features(self):\n",
    "        \"\"\"Create basic features if feature store not available\"\"\"\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        if results.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Simple feature engineering\n",
    "        driver_stats = results.groupby('driverId').agg({\n",
    "            'positionOrder': ['mean', 'std'],\n",
    "            'points': ['mean', 'sum'],\n",
    "            'grid': 'mean'\n",
    "        })\n",
    "        \n",
    "        driver_stats.columns = ['avg_position', 'position_std', \n",
    "                               'avg_points', 'total_points', 'avg_grid']\n",
    "        \n",
    "        return driver_stats\n",
    "    \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions for all drivers\"\"\"\n",
    "        logger.info(\"Generating predictions...\")\n",
    "        \n",
    "        # Get active drivers\n",
    "        drivers = self.data.get('drivers', pd.DataFrame())\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        \n",
    "        if drivers.empty or results.empty:\n",
    "            logger.error(\"No driver or results data available\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get drivers who raced recently\n",
    "        recent_drivers = results[results['year'] >= 2023]['driverId'].unique()\n",
    "        active_drivers = drivers[drivers['driverId'].isin(recent_drivers)]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for _, driver in active_drivers.iterrows():\n",
    "            # Get driver stats\n",
    "            driver_results = results[results['driverId'] == driver['driverId']].tail(10)\n",
    "            \n",
    "            if len(driver_results) >= 3:\n",
    "                # Calculate probabilities\n",
    "                top10_prob = (driver_results['positionOrder'] <= 10).mean()\n",
    "                top5_prob = (driver_results['positionOrder'] <= 5).mean()\n",
    "                top3_prob = (driver_results['positionOrder'] <= 3).mean()\n",
    "                points_prob = (driver_results['points'] > 0).mean()\n",
    "                \n",
    "                # Adjust with model if available\n",
    "                confidence = 0.7 + 0.05 * len(driver_results) / 10\n",
    "                \n",
    "                predictions.append({\n",
    "                    'driver': driver['surname'],\n",
    "                    'driverId': driver['driverId'],\n",
    "                    'top10_prob': min(0.95, top10_prob * 1.1),\n",
    "                    'top5_prob': min(0.85, top5_prob * 1.1),\n",
    "                    'top3_prob': min(0.70, top3_prob * 1.1),\n",
    "                    'points_prob': min(0.95, points_prob * 1.05),\n",
    "                    'beat_teammate_prob': 0.5,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        self.results['predictions'] = pd.DataFrame(predictions)\n",
    "        logger.info(f\"Generated predictions for {len(predictions)} drivers\")\n",
    "        return self.results['predictions']\n",
    "    \n",
    "    def optimize_picks(self):\n",
    "        \"\"\"Optimize Prize Picks selections\"\"\"\n",
    "        logger.info(\"Optimizing Prize Picks...\")\n",
    "        \n",
    "        if 'predictions' not in self.results or self.results['predictions'].empty:\n",
    "            logger.error(\"No predictions available\")\n",
    "            return None\n",
    "        \n",
    "        if not self.optimizer:\n",
    "            logger.error(\"Optimizer not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Generate all possible picks\n",
    "        all_picks = self.optimizer.generate_all_picks(\n",
    "            self.results['predictions'],\n",
    "            min_edge=self.config.min_edge\n",
    "        )\n",
    "        \n",
    "        if all_picks.empty:\n",
    "            logger.warning(\"No picks with positive edge found\")\n",
    "            return None\n",
    "        \n",
    "        # Optimize portfolio\n",
    "        portfolio = self.optimizer.optimize_portfolio(\n",
    "            all_picks,\n",
    "            bankroll=self.config.bankroll,\n",
    "            constraints=self.config.constraints\n",
    "        )\n",
    "        \n",
    "        self.results['portfolio'] = portfolio\n",
    "        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n",
    "        return portfolio\n",
    "    \n",
    "    def generate_report(self, save_path=None):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        logger.info(\"Generating report...\")\n",
    "        \n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'config': self.config.to_dict(),\n",
    "            'summary': self._generate_summary(),\n",
    "            'predictions': self.results.get('predictions', pd.DataFrame()).to_dict('records'),\n",
    "            'portfolio': self._serialize_portfolio(),\n",
    "            'risk_metrics': self._calculate_risk_metrics()\n",
    "        }\n",
    "        \n",
    "        if save_path is None:\n",
    "            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Report saved to {save_path}\")\n",
    "        return report\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if 'predictions' in self.results and not self.results['predictions'].empty:\n",
    "            summary['n_drivers'] = len(self.results['predictions'])\n",
    "            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n",
    "        \n",
    "        if 'portfolio' in self.results:\n",
    "            portfolio = self.results['portfolio']\n",
    "            summary['n_parlays'] = len(portfolio)\n",
    "            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n",
    "            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n",
    "            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _serialize_portfolio(self):\n",
    "        \"\"\"Serialize portfolio for JSON\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return []\n",
    "        \n",
    "        serialized = []\n",
    "        for parlay in self.results['portfolio']:\n",
    "            parlay_data = {\n",
    "                'n_picks': parlay['n_picks'],\n",
    "                'bet_size': parlay['bet_size'],\n",
    "                'payout': parlay['payout'],\n",
    "                'adjusted_prob': parlay['adjusted_prob'],\n",
    "                'expected_value': parlay['expected_value'],\n",
    "                'kelly_stake': parlay['kelly_stake'],\n",
    "                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n",
    "            }\n",
    "            serialized.append(parlay_data)\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _calculate_risk_metrics(self):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return {}\n",
    "        \n",
    "        portfolio = self.results['portfolio']\n",
    "        \n",
    "        total_exposure = sum(p['bet_size'] for p in portfolio)\n",
    "        \n",
    "        metrics = {\n",
    "            'total_exposure': total_exposure,\n",
    "            'exposure_pct': total_exposure / self.config.bankroll if self.config.bankroll > 0 else 0,\n",
    "            'n_bets': len(portfolio),\n",
    "            'avg_bet_size': total_exposure / len(portfolio) if portfolio else 0,\n",
    "            'max_bet_size': max(p['bet_size'] for p in portfolio) if portfolio else 0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run(self, race_id=None):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        logger.info(\"Starting F1 Prize Picks pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data\n",
    "            self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features\n",
    "            features = self.prepare_features(race_id)\n",
    "            \n",
    "            # Step 3: Generate predictions\n",
    "            predictions = self.generate_predictions()\n",
    "            \n",
    "            # Step 4: Optimize picks\n",
    "            portfolio = self.optimize_picks()\n",
    "            \n",
    "            # Step 5: Generate report\n",
    "            if self.config.save_predictions:\n",
    "                report = self.generate_report()\n",
    "            \n",
    "            logger.info(\"Pipeline completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Pipeline initialization failed. Please check the error messages above.\n"
     ]
    }
   ],
   "source": [
    "# Check if all components are initialized\n",
    "if initialization_success:\n",
    "    # Initialize and run pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    logger.info(\"Pipeline initialized\")\n",
    "    \n",
    "    # Run the pipeline\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    # Display results\n",
    "    if results and 'portfolio' in results:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"F1 PRIZE PICKS RECOMMENDATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        portfolio = results['portfolio']\n",
    "        \n",
    "        for i, parlay in enumerate(portfolio, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"PARLAY {i}: {parlay['n_picks']}-PICK ENTRY\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Bet Amount: ${parlay['bet_size']:.2f}\")\n",
    "            print(f\"Potential Payout: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\")\n",
    "            print(f\"Win Probability: {parlay['adjusted_prob']:.1%}\")\n",
    "            print(f\"Expected Value: ${parlay['expected_value'] * parlay['bet_size']:.2f}\")\n",
    "            print(f\"\\nPicks:\")\n",
    "            \n",
    "            picks = parlay['picks']\n",
    "            if hasattr(picks, 'iterrows'):\n",
    "                for j, (_, pick) in enumerate(picks.iterrows(), 1):\n",
    "                    print(f\"  {j}. {pick['driver']} - {pick['bet_type']}\")\n",
    "                    print(f\"     Edge: +{pick['edge']:.1%}\")\n",
    "        \n",
    "        # Summary\n",
    "        summary = pipeline._generate_summary()\n",
    "        if summary:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Total Wagered: ${summary.get('total_wagered', 0):.2f}\")\n",
    "            print(f\"Expected Profit: ${summary.get('expected_profit', 0):.2f}\")\n",
    "            print(f\"Number of Parlays: {summary.get('n_parlays', 0)}\")\n",
    "            print(f\"Average Win Probability: {summary.get('avg_win_probability', 0):.1%}\")\n",
    "    else:\n",
    "        print(\"\\nNo recommendations generated. Check logs for details.\")\n",
    "else:\n",
    "    print(\"\\n❌ Pipeline initialization failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run_pipeline(bankroll=1000, kelly_fraction=0.25):\n",
    "    \"\"\"\n",
    "    Quick function to run the pipeline with custom parameters\n",
    "    \"\"\"\n",
    "    # Update config\n",
    "    config.bankroll = bankroll\n",
    "    config.kelly_fraction = kelly_fraction\n",
    "    \n",
    "    # Initialize and run\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_all_notebooks_fresh():\n",
    "    \"\"\"\n",
    "    Run all notebooks from scratch\n",
    "    \"\"\"\n",
    "    print(\"Running all notebooks from scratch...\")\n",
    "    print(\"This may take 10-15 minutes...\\n\")\n",
    "    \n",
    "    # Force rerun all components\n",
    "    success = initialize_pipeline_components(force_rerun=True)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✅ All notebooks executed successfully!\")\n",
    "        print(\"Now running the main pipeline...\\n\")\n",
    "        \n",
    "        # Run pipeline\n",
    "        pipeline = F1PrizePipeline(config)\n",
    "        results = pipeline.run()\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"\\n❌ Some notebooks failed. Check the logs.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# results = quick_run_pipeline(bankroll=500, kelly_fraction=0.20)\n",
    "# results = run_all_notebooks_fresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Run Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Master Pipeline Integration notebook:\n",
    "\n",
    "1. **Automatically runs prerequisite notebooks** in the correct order\n",
    "2. **Uses the correct f1db_data_loader.py** (not enhanced version)\n",
    "3. **Handles missing components gracefully** with informative error messages\n",
    "4. **Provides a single entry point** for the entire F1 Prize Picks pipeline\n",
    "\n",
    "### Usage:\n",
    "- **First time**: The notebook will automatically run all required notebooks\n",
    "- **Subsequent runs**: It will skip notebooks whose outputs already exist\n",
    "- **Force fresh run**: Use `run_all_notebooks_fresh()` to rebuild everything\n",
    "\n",
    "### Key Functions:\n",
    "- `initialize_pipeline_components()` - Run required notebooks\n",
    "- `quick_run_pipeline()` - Run with custom parameters\n",
    "- `run_all_notebooks_fresh()` - Rebuild everything from scratch\n",
    "\n",
    "The pipeline is now self-contained and can orchestrate the entire F1 prediction system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
