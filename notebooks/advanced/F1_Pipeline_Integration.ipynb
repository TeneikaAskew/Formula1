{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Pipeline Integration\n",
    "\n",
    "This notebook orchestrates all components of the F1 Prize Picks optimization system into a unified pipeline:\n",
    "- Data loading and feature engineering\n",
    "- Model predictions with driver evaluation\n",
    "- Prize Picks optimization\n",
    "- Explainability and reporting\n",
    "- Automated execution for race weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('F1Pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary components\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Data loading\n",
    "from enhanced_f1db_data_loader import EnhancedF1DBDataLoader, load_f1db_data_enhanced\n",
    "\n",
    "# Feature engineering\n",
    "try:\n",
    "    from F1_Feature_Store import F1FeatureStore\n",
    "except:\n",
    "    logger.warning(\"Feature store module not found\")\n",
    "\n",
    "# Model components\n",
    "try:\n",
    "    from F1_Integrated_Driver_Evaluation import IntegratedF1Predictor\n",
    "except:\n",
    "    logger.warning(\"Integrated predictor module not found\")\n",
    "\n",
    "# Optimization\n",
    "try:\n",
    "    from F1_Prize_Picks_Optimizer import PrizePicksOptimizer, KellyCriterion, PrizePicksBetTypes\n",
    "except:\n",
    "    logger.warning(\"Prize Picks optimizer module not found\")\n",
    "\n",
    "# Explainability\n",
    "try:\n",
    "    from F1_Explainability_Engine import PredictionExplainer, PrizePicksExplainer\n",
    "except:\n",
    "    logger.warning(\"Explainability engine module not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the F1 pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.data_dir = Path('../../data/f1db')\n",
    "        self.model_dir = Path('.')\n",
    "        self.output_dir = Path('pipeline_outputs')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Model settings\n",
    "        self.use_cached_data = True\n",
    "        self.auto_sync = True\n",
    "        self.cache_expiry_hours = 24\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.bankroll = 1000\n",
    "        self.kelly_fraction = 0.25\n",
    "        self.max_correlation = 0.5\n",
    "        self.min_edge = 0.05\n",
    "        self.max_exposure = 0.25\n",
    "        \n",
    "        # Constraints\n",
    "        self.constraints = {\n",
    "            'max_per_driver': 2,\n",
    "            'max_per_type': 3,\n",
    "            'min_avg_edge': 0.08\n",
    "        }\n",
    "        \n",
    "        # Pipeline settings\n",
    "        self.generate_report = True\n",
    "        self.save_predictions = True\n",
    "        self.mlflow_tracking = False  # Set to True when MLflow is configured\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return {\n",
    "            'data_dir': str(self.data_dir),\n",
    "            'model_dir': str(self.model_dir),\n",
    "            'output_dir': str(self.output_dir),\n",
    "            'bankroll': self.bankroll,\n",
    "            'kelly_fraction': self.kelly_fraction,\n",
    "            'max_correlation': self.max_correlation,\n",
    "            'min_edge': self.min_edge,\n",
    "            'max_exposure': self.max_exposure,\n",
    "            'constraints': self.constraints\n",
    "        }\n",
    "    \n",
    "    def save(self, path='pipeline_config.json'):\n",
    "        \"\"\"Save configuration\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path='pipeline_config.json'):\n",
    "        \"\"\"Load configuration\"\"\"\n",
    "        config = cls()\n",
    "        if Path(path).exists():\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for key, value in data.items():\n",
    "                    if hasattr(config, key):\n",
    "                        setattr(config, key, value)\n",
    "        return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = PipelineConfig()\n",
    "config.save()\n",
    "logger.info(f\"Pipeline configuration initialized: {config.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1PrizePipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrating all components\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = None\n",
    "        self.feature_store = None\n",
    "        self.predictor = None\n",
    "        self.optimizer = None\n",
    "        self.explainer = None\n",
    "        self.results = {}\n",
    "        \n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all pipeline components\"\"\"\n",
    "        logger.info(\"Initializing pipeline components...\")\n",
    "        \n",
    "        # Data loader\n",
    "        self.data_loader = EnhancedF1DBDataLoader(\n",
    "            str(self.config.data_dir),\n",
    "            cache_dir=str(self.config.data_dir / 'cache')\n",
    "        )\n",
    "        \n",
    "        # Load saved models if available\n",
    "        try:\n",
    "            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n",
    "            logger.info(\"Loaded integrated predictor\")\n",
    "        except:\n",
    "            logger.warning(\"Could not load integrated predictor\")\n",
    "            self.predictor = None\n",
    "        \n",
    "        try:\n",
    "            optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n",
    "            self.optimizer = optimizer_config['optimizer']\n",
    "            logger.info(\"Loaded Prize Picks optimizer\")\n",
    "        except:\n",
    "            logger.warning(\"Could not load optimizer - creating new one\")\n",
    "            self.optimizer = PrizePicksOptimizer(\n",
    "                kelly_fraction=self.config.kelly_fraction,\n",
    "                max_correlation=self.config.max_correlation\n",
    "            )\n",
    "        \n",
    "        # Feature store\n",
    "        self.feature_store = F1FeatureStore() if 'F1FeatureStore' in globals() else None\n",
    "        \n",
    "        # Explainers\n",
    "        self.prediction_explainer = PredictionExplainer(None, []) if 'PredictionExplainer' in globals() else None\n",
    "        self.pp_explainer = PrizePicksExplainer() if 'PrizePicksExplainer' in globals() else None\n",
    "        \n",
    "        logger.info(\"Pipeline components initialized\")\n",
    "    \n",
    "    def load_data(self, force_update=False):\n",
    "        \"\"\"Load and prepare F1 data\"\"\"\n",
    "        logger.info(\"Loading F1 data...\")\n",
    "        \n",
    "        # Auto sync if enabled\n",
    "        if self.config.auto_sync:\n",
    "            self.data_loader.automated_sync()\n",
    "        \n",
    "        # Load data\n",
    "        self.data = load_f1db_data_enhanced(\n",
    "            data_dir=str(self.config.data_dir),\n",
    "            use_cache=self.config.use_cached_data,\n",
    "            auto_sync=False  # Already synced above\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} datasets\")\n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, race_id=None):\n",
    "        \"\"\"Prepare features for prediction\"\"\"\n",
    "        logger.info(\"Preparing features...\")\n",
    "        \n",
    "        # Get upcoming race if no race_id specified\n",
    "        if race_id is None:\n",
    "            upcoming = self.data_loader.get_upcoming_race()\n",
    "            if upcoming is not None:\n",
    "                race_id = upcoming['raceId']\n",
    "                logger.info(f\"Preparing for upcoming race: {upcoming['name']}\")\n",
    "            else:\n",
    "                logger.warning(\"No upcoming race found\")\n",
    "                return None\n",
    "        \n",
    "        # Build feature set\n",
    "        if self.feature_store and hasattr(self.feature_store, 'base_features'):\n",
    "            features = self.feature_store.get_race_features(race_id)\n",
    "        else:\n",
    "            # Basic feature preparation\n",
    "            results = self.data.get('results', pd.DataFrame())\n",
    "            races = self.data.get('races', pd.DataFrame())\n",
    "            \n",
    "            # Get recent data for feature creation\n",
    "            recent_data = results[results['raceId'] < race_id].tail(1000)\n",
    "            features = self._create_basic_features(recent_data)\n",
    "        \n",
    "        self.results['features'] = features\n",
    "        return features\n",
    "    \n",
    "    def _create_basic_features(self, data):\n",
    "        \"\"\"Create basic features if feature store not available\"\"\"\n",
    "        # Simple feature engineering\n",
    "        driver_stats = data.groupby('driverId').agg({\n",
    "            'positionOrder': ['mean', 'std'],\n",
    "            'points': ['mean', 'sum'],\n",
    "            'grid': 'mean'\n",
    "        })\n",
    "        \n",
    "        driver_stats.columns = ['avg_position', 'position_std', \n",
    "                               'avg_points', 'total_points', 'avg_grid']\n",
    "        \n",
    "        return driver_stats\n",
    "    \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions for all drivers\"\"\"\n",
    "        logger.info(\"Generating predictions...\")\n",
    "        \n",
    "        # Get active drivers\n",
    "        drivers = self.data.get('drivers', pd.DataFrame())\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        \n",
    "        # Get drivers who raced recently\n",
    "        recent_drivers = results[results['year'] >= 2023]['driverId'].unique()\n",
    "        active_drivers = drivers[drivers['driverId'].isin(recent_drivers)]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for _, driver in active_drivers.iterrows():\n",
    "            # Get driver stats\n",
    "            driver_results = results[results['driverId'] == driver['driverId']].tail(10)\n",
    "            \n",
    "            if len(driver_results) >= 3:\n",
    "                # Calculate probabilities\n",
    "                top10_prob = (driver_results['positionOrder'] <= 10).mean()\n",
    "                top5_prob = (driver_results['positionOrder'] <= 5).mean()\n",
    "                top3_prob = (driver_results['positionOrder'] <= 3).mean()\n",
    "                points_prob = (driver_results['points'] > 0).mean()\n",
    "                \n",
    "                # Adjust with model if available\n",
    "                confidence = 0.7 + 0.05 * len(driver_results) / 10\n",
    "                \n",
    "                predictions.append({\n",
    "                    'driver': driver['surname'],\n",
    "                    'driverId': driver['driverId'],\n",
    "                    'top10_prob': min(0.95, top10_prob * 1.1),  # Slight adjustment\n",
    "                    'top5_prob': min(0.85, top5_prob * 1.1),\n",
    "                    'top3_prob': min(0.70, top3_prob * 1.1),\n",
    "                    'points_prob': min(0.95, points_prob * 1.05),\n",
    "                    'beat_teammate_prob': 0.5,  # Default\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        self.results['predictions'] = pd.DataFrame(predictions)\n",
    "        logger.info(f\"Generated predictions for {len(predictions)} drivers\")\n",
    "        return self.results['predictions']\n",
    "    \n",
    "    def optimize_picks(self):\n",
    "        \"\"\"Optimize Prize Picks selections\"\"\"\n",
    "        logger.info(\"Optimizing Prize Picks...\")\n",
    "        \n",
    "        if 'predictions' not in self.results:\n",
    "            logger.error(\"No predictions available\")\n",
    "            return None\n",
    "        \n",
    "        # Generate all possible picks\n",
    "        all_picks = self.optimizer.generate_all_picks(\n",
    "            self.results['predictions'],\n",
    "            min_edge=self.config.min_edge\n",
    "        )\n",
    "        \n",
    "        if all_picks.empty:\n",
    "            logger.warning(\"No picks with positive edge found\")\n",
    "            return None\n",
    "        \n",
    "        # Optimize portfolio\n",
    "        portfolio = self.optimizer.optimize_portfolio(\n",
    "            all_picks,\n",
    "            bankroll=self.config.bankroll,\n",
    "            constraints=self.config.constraints\n",
    "        )\n",
    "        \n",
    "        self.results['portfolio'] = portfolio\n",
    "        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n",
    "        return portfolio\n",
    "    \n",
    "    def generate_explanations(self):\n",
    "        \"\"\"Generate explanations for recommendations\"\"\"\n",
    "        logger.info(\"Generating explanations...\")\n",
    "        \n",
    "        explanations = []\n",
    "        \n",
    "        if 'portfolio' in self.results and self.pp_explainer:\n",
    "            for parlay in self.results['portfolio']:\n",
    "                explanation = self.pp_explainer.explain_parlay(parlay)\n",
    "                explanations.append(explanation)\n",
    "        \n",
    "        self.results['explanations'] = explanations\n",
    "        return explanations\n",
    "    \n",
    "    def generate_report(self, save_path=None):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        logger.info(\"Generating report...\")\n",
    "        \n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'config': self.config.to_dict(),\n",
    "            'summary': self._generate_summary(),\n",
    "            'predictions': self.results.get('predictions', pd.DataFrame()).to_dict('records'),\n",
    "            'portfolio': self._serialize_portfolio(),\n",
    "            'risk_metrics': self._calculate_risk_metrics()\n",
    "        }\n",
    "        \n",
    "        if save_path is None:\n",
    "            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Report saved to {save_path}\")\n",
    "        return report\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if 'predictions' in self.results:\n",
    "            summary['n_drivers'] = len(self.results['predictions'])\n",
    "            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n",
    "        \n",
    "        if 'portfolio' in self.results:\n",
    "            portfolio = self.results['portfolio']\n",
    "            summary['n_parlays'] = len(portfolio)\n",
    "            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n",
    "            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n",
    "            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _serialize_portfolio(self):\n",
    "        \"\"\"Serialize portfolio for JSON\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return []\n",
    "        \n",
    "        serialized = []\n",
    "        for parlay in self.results['portfolio']:\n",
    "            parlay_data = {\n",
    "                'n_picks': parlay['n_picks'],\n",
    "                'bet_size': parlay['bet_size'],\n",
    "                'payout': parlay['payout'],\n",
    "                'adjusted_prob': parlay['adjusted_prob'],\n",
    "                'expected_value': parlay['expected_value'],\n",
    "                'kelly_stake': parlay['kelly_stake'],\n",
    "                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n",
    "            }\n",
    "            serialized.append(parlay_data)\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _calculate_risk_metrics(self):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return {}\n",
    "        \n",
    "        portfolio = self.results['portfolio']\n",
    "        \n",
    "        total_exposure = sum(p['bet_size'] for p in portfolio)\n",
    "        \n",
    "        metrics = {\n",
    "            'total_exposure': total_exposure,\n",
    "            'exposure_pct': total_exposure / self.config.bankroll,\n",
    "            'n_bets': len(portfolio),\n",
    "            'avg_bet_size': total_exposure / len(portfolio) if portfolio else 0,\n",
    "            'max_bet_size': max(p['bet_size'] for p in portfolio) if portfolio else 0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run(self, race_id=None):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        logger.info(\"Starting F1 Prize Picks pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data\n",
    "            self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features\n",
    "            features = self.prepare_features(race_id)\n",
    "            \n",
    "            # Step 3: Generate predictions\n",
    "            predictions = self.generate_predictions()\n",
    "            \n",
    "            # Step 4: Optimize picks\n",
    "            portfolio = self.optimize_picks()\n",
    "            \n",
    "            # Step 5: Generate explanations\n",
    "            if self.config.generate_report:\n",
    "                self.generate_explanations()\n",
    "            \n",
    "            # Step 6: Generate report\n",
    "            if self.config.save_predictions:\n",
    "                report = self.generate_report()\n",
    "            \n",
    "            logger.info(\"Pipeline completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Initialize and run pipeline\n",
    "pipeline = F1PrizePipeline(config)\n",
    "logger.info(\"Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Pipeline for Upcoming Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "results = pipeline.run()\n",
    "\n",
    "# Display results\n",
    "if results and 'portfolio' in results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"F1 PRIZE PICKS RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    portfolio = results['portfolio']\n",
    "    \n",
    "    for i, parlay in enumerate(portfolio, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PARLAY {i}: {parlay['n_picks']}-PICK ENTRY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Bet Amount: ${parlay['bet_size']:.2f}\")\n",
    "        print(f\"Potential Payout: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\")\n",
    "        print(f\"Win Probability: {parlay['adjusted_prob']:.1%}\")\n",
    "        print(f\"Expected Value: ${parlay['expected_value'] * parlay['bet_size']:.2f}\")\n",
    "        print(f\"\\nPicks:\")\n",
    "        \n",
    "        picks = parlay['picks']\n",
    "        if hasattr(picks, 'iterrows'):\n",
    "            for j, (_, pick) in enumerate(picks.iterrows(), 1):\n",
    "                print(f\"  {j}. {pick['driver']} - {pick['bet_type']}\")\n",
    "                print(f\"     Edge: +{pick['edge']:.1%}\")\n",
    "    \n",
    "    # Summary\n",
    "    summary = results.get('summary', {})\n",
    "    if summary:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total Wagered: ${summary.get('total_wagered', 0):.2f}\")\n",
    "        print(f\"Expected Profit: ${summary.get('expected_profit', 0):.2f}\")\n",
    "        print(f\"Number of Parlays: {summary.get('n_parlays', 0)}\")\n",
    "        print(f\"Average Win Probability: {summary.get('avg_win_probability', 0):.1%}\")\n",
    "else:\n",
    "    print(\"\\nNo recommendations generated. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automated Race Weekend Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceWeekendAutomation:\n",
    "    \"\"\"\n",
    "    Automate pipeline execution for race weekends\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: F1PrizePipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.schedule = []\n",
    "    \n",
    "    def get_race_schedule(self):\n",
    "        \"\"\"Get upcoming race schedule\"\"\"\n",
    "        races = self.pipeline.data.get('races', pd.DataFrame())\n",
    "        if races.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get future races\n",
    "        races['date'] = pd.to_datetime(races['date'])\n",
    "        future_races = races[races['date'] > datetime.now()]\n",
    "        \n",
    "        return future_races.sort_values('date')\n",
    "    \n",
    "    def schedule_race_analysis(self, race_id, race_date):\n",
    "        \"\"\"Schedule analysis for a specific race\"\"\"\n",
    "        # Run at different times\n",
    "        schedule_times = [\n",
    "            (race_date - timedelta(days=3), 'Initial Analysis'),\n",
    "            (race_date - timedelta(days=1), 'Pre-Qualifying Update'),\n",
    "            (race_date - timedelta(hours=4), 'Final Predictions')\n",
    "        ]\n",
    "        \n",
    "        for run_time, description in schedule_times:\n",
    "            self.schedule.append({\n",
    "                'race_id': race_id,\n",
    "                'run_time': run_time,\n",
    "                'description': description,\n",
    "                'status': 'scheduled'\n",
    "            })\n",
    "    \n",
    "    def execute_scheduled_runs(self):\n",
    "        \"\"\"Execute scheduled pipeline runs\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        for task in self.schedule:\n",
    "            if task['status'] == 'scheduled' and task['run_time'] <= current_time:\n",
    "                logger.info(f\"Executing {task['description']} for race {task['race_id']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Update config based on timing\n",
    "                    if 'Final' in task['description']:\n",
    "                        self.pipeline.config.kelly_fraction = 0.20  # More conservative\n",
    "                    \n",
    "                    # Run pipeline\n",
    "                    results = self.pipeline.run(task['race_id'])\n",
    "                    \n",
    "                    # Save results with timestamp\n",
    "                    output_name = f\"race_{task['race_id']}_{task['description'].replace(' ', '_')}_{current_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "                    self.pipeline.generate_report(\n",
    "                        self.pipeline.config.output_dir / f\"{output_name}.json\"\n",
    "                    )\n",
    "                    \n",
    "                    task['status'] = 'completed'\n",
    "                    task['completed_at'] = current_time.isoformat()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to execute {task['description']}: {str(e)}\")\n",
    "                    task['status'] = 'failed'\n",
    "                    task['error'] = str(e)\n",
    "    \n",
    "    def generate_weekend_summary(self):\n",
    "        \"\"\"Generate summary of all analyses for a race weekend\"\"\"\n",
    "        completed_tasks = [t for t in self.schedule if t['status'] == 'completed']\n",
    "        \n",
    "        if not completed_tasks:\n",
    "            return None\n",
    "        \n",
    "        summary = {\n",
    "            'race_id': completed_tasks[0]['race_id'],\n",
    "            'analyses_completed': len(completed_tasks),\n",
    "            'final_recommendations': None\n",
    "        }\n",
    "        \n",
    "        # Get final predictions\n",
    "        final_task = next((t for t in completed_tasks if 'Final' in t['description']), None)\n",
    "        if final_task:\n",
    "            # Load the report\n",
    "            report_files = list(self.pipeline.config.output_dir.glob(f\"race_{final_task['race_id']}_Final*.json\"))\n",
    "            if report_files:\n",
    "                with open(report_files[-1], 'r') as f:\n",
    "                    final_report = json.load(f)\n",
    "                    summary['final_recommendations'] = final_report.get('portfolio', [])\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize automation\n",
    "automation = RaceWeekendAutomation(pipeline)\n",
    "\n",
    "# Get upcoming races\n",
    "upcoming_races = automation.get_race_schedule()\n",
    "if not upcoming_races.empty:\n",
    "    print(\"\\nUpcoming Races:\")\n",
    "    print(\"=\" * 60)\n",
    "    for _, race in upcoming_races.head(5).iterrows():\n",
    "        print(f\"{race['date'].strftime('%Y-%m-%d')}: {race['name']} (Round {race['round']})\")\n",
    "        \n",
    "        # Schedule analysis for next race\n",
    "        if _ == 0:  # First race\n",
    "            automation.schedule_race_analysis(race['raceId'], race['date'])\n",
    "    \n",
    "    print(f\"\\nScheduled {len(automation.schedule)} analyses for next race\")\n",
    "else:\n",
    "    print(\"\\nNo upcoming races found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor pipeline and prediction performance\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = []\n",
    "    \n",
    "    def track_predictions(self, predictions, actuals=None):\n",
    "        \"\"\"Track prediction accuracy\"\"\"\n",
    "        metric = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_predictions': len(predictions),\n",
    "            'avg_confidence': predictions['confidence'].mean() if 'confidence' in predictions else 0\n",
    "        }\n",
    "        \n",
    "        if actuals is not None:\n",
    "            # Calculate accuracy metrics\n",
    "            metric['accuracy'] = self._calculate_accuracy(predictions, actuals)\n",
    "        \n",
    "        self.metrics.append(metric)\n",
    "    \n",
    "    def _calculate_accuracy(self, predictions, actuals):\n",
    "        \"\"\"Calculate prediction accuracy\"\"\"\n",
    "        # Implementation depends on actual data format\n",
    "        return 0.0\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"Generate performance report\"\"\"\n",
    "        if not self.metrics:\n",
    "            return None\n",
    "        \n",
    "        report = {\n",
    "            'period': {\n",
    "                'start': self.metrics[0]['timestamp'],\n",
    "                'end': self.metrics[-1]['timestamp']\n",
    "            },\n",
    "            'total_predictions': sum(m['n_predictions'] for m in self.metrics),\n",
    "            'avg_confidence': np.mean([m['avg_confidence'] for m in self.metrics]),\n",
    "            'runs_completed': len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def plot_performance_trends(self):\n",
    "        \"\"\"Plot performance trends over time\"\"\"\n",
    "        if not self.metrics:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        timestamps = [pd.to_datetime(m['timestamp']) for m in self.metrics]\n",
    "        confidences = [m['avg_confidence'] for m in self.metrics]\n",
    "        \n",
    "        ax.plot(timestamps, confidences, marker='o')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Average Confidence')\n",
    "        ax.set_title('Model Confidence Over Time')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize performance monitor\n",
    "monitor = PerformanceMonitor(config.output_dir)\n",
    "\n",
    "# Track current predictions\n",
    "if 'predictions' in results:\n",
    "    monitor.track_predictions(results['predictions'])\n",
    "    \n",
    "    performance_report = monitor.generate_performance_report()\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in performance_report.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Pipeline State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline state for future use\n",
    "pipeline_state = {\n",
    "    'config': config.to_dict(),\n",
    "    'last_run': datetime.now().isoformat(),\n",
    "    'results_summary': pipeline._generate_summary() if hasattr(pipeline, '_generate_summary') else {},\n",
    "    'automation_schedule': automation.schedule if 'automation' in locals() else [],\n",
    "    'performance_metrics': monitor.metrics if 'monitor' in locals() else []\n",
    "}\n",
    "\n",
    "# Save state\n",
    "state_path = config.output_dir / 'pipeline_state.json'\n",
    "with open(state_path, 'w') as f:\n",
    "    json.dump(pipeline_state, f, indent=2)\n",
    "\n",
    "print(f\"\\nPipeline state saved to {state_path}\")\n",
    "\n",
    "# Create run script for easy execution\n",
    "run_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run F1 Prize Picks Pipeline\n",
    "\n",
    "Usage:\n",
    "    python run_pipeline.py              # Run for upcoming race\n",
    "    python run_pipeline.py --race_id 1234  # Run for specific race\n",
    "    python run_pipeline.py --backtest   # Run backtesting\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Add notebook directory to path\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "\n",
    "from F1_Pipeline_Integration import F1PrizePipeline, PipelineConfig\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Run F1 Prize Picks Pipeline')\n",
    "    parser.add_argument('--race_id', type=int, help='Specific race ID to analyze')\n",
    "    parser.add_argument('--config', type=str, default='pipeline_config.json', \n",
    "                       help='Path to configuration file')\n",
    "    parser.add_argument('--backtest', action='store_true', \n",
    "                       help='Run backtesting instead of predictions')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load configuration\n",
    "    config = PipelineConfig.load(args.config)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    \n",
    "    # Run pipeline\n",
    "    if args.backtest:\n",
    "        print(\"Running backtesting...\")\n",
    "        # Import and run backtesting\n",
    "        from F1_Backtesting_Framework import F1BacktestEngine\n",
    "        # Implementation here\n",
    "    else:\n",
    "        results = pipeline.run(args.race_id)\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\nPipeline completed successfully!\")\n",
    "            print(f\"Results saved to {config.output_dir}\")\n",
    "        else:\n",
    "            print(\"\\nPipeline failed. Check logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save run script\n",
    "script_path = Path('run_pipeline.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(run_script)\n",
    "\n",
    "print(f\"\\nRun script created: {script_path}\")\n",
    "print(\"\\nTo run the pipeline:\")\n",
    "print(\"  python run_pipeline.py\")\n",
    "print(\"  python run_pipeline.py --race_id 1234\")\n",
    "print(\"  python run_pipeline.py --backtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The F1 Pipeline Integration successfully orchestrates all components:\n",
    "\n",
    "1. **Data Management**: Automated F1DB synchronization and caching\n",
    "2. **Feature Engineering**: Comprehensive feature store integration\n",
    "3. **Predictions**: Model predictions with driver evaluation\n",
    "4. **Optimization**: Prize Picks portfolio optimization\n",
    "5. **Explainability**: Detailed explanations for recommendations\n",
    "6. **Automation**: Race weekend scheduling and execution\n",
    "\n",
    "### Key Features:\n",
    "- Configurable pipeline with JSON settings\n",
    "- Automated race weekend analysis\n",
    "- Performance monitoring and tracking\n",
    "- Comprehensive reporting\n",
    "- Easy-to-use run script\n",
    "\n",
    "### Usage:\n",
    "1. Configure settings in `pipeline_config.json`\n",
    "2. Run `python run_pipeline.py` for upcoming race\n",
    "3. Check `pipeline_outputs/` for results\n",
    "4. Monitor performance over time\n",
    "\n",
    "The pipeline is now ready for production use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}