{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Pipeline Integration - Master Controller\n",
    "\n",
    "This notebook orchestrates ALL components of the F1 Prize Picks optimization system.\n",
    "It can run other notebooks programmatically to ensure proper initialization of all components.\n",
    "\n",
    "Key Features:\n",
    "- Automatically runs prerequisite notebooks in correct order\n",
    "- Handles missing models/modules gracefully\n",
    "- Uses the correct f1db_data_loader.py\n",
    "- Provides a single entry point for the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 09:38:48,310 - F1Pipeline - INFO - Working directory: c:\\Users\\tenei\\Documents\\GitHub\\Formula1\\notebooks\\advanced\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('F1Pipeline')\n",
    "\n",
    "# Set working directory to notebook location (works on both Windows and Linux)\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name != 'advanced':\n",
    "    # Try to find the advanced directory\n",
    "    if (notebook_dir / 'advanced').exists():\n",
    "        os.chdir(notebook_dir / 'advanced')\n",
    "    elif (notebook_dir / 'notebooks' / 'advanced').exists():\n",
    "        os.chdir(notebook_dir / 'notebooks' / 'advanced')\n",
    "    elif notebook_dir.parent.name == 'notebooks' and notebook_dir.name != 'advanced':\n",
    "        advanced_dir = notebook_dir.parent / 'advanced'\n",
    "        if advanced_dir.exists():\n",
    "            os.chdir(advanced_dir)\n",
    "\n",
    "# Add current directory to Python path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "logger.info(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NotebookRunner:\n",
    "    \"\"\"\n",
    "    Utility to run other Jupyter notebooks programmatically\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.executed_notebooks = []\n",
    "        self.failed_notebooks = []\n",
    "    \n",
    "    def run_notebook(self, notebook_path, timeout=600):\n",
    "        \"\"\"\n",
    "        Execute a Jupyter notebook and return success status\n",
    "        \"\"\"\n",
    "        logger.info(f\"Running notebook: {notebook_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Alternative method: Use papermill if available\n",
    "            try:\n",
    "                import papermill as pm\n",
    "                pm.execute_notebook(\n",
    "                    input_path=notebook_path,\n",
    "                    output_path=notebook_path,\n",
    "                    kernel_name='python3',\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                self.executed_notebooks.append(notebook_path)\n",
    "                return True\n",
    "            except ImportError:\n",
    "                logger.info(\"Papermill not available, using nbconvert method\")\n",
    "            \n",
    "            # Original nbconvert method but without --inplace to avoid modifying the original\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "                    '--to', 'notebook',\n",
    "                    '--execute',\n",
    "                    '--ExecutePreprocessor.timeout=' + str(timeout),\n",
    "                    '--ExecutePreprocessor.allow_errors=True',  # Continue on errors\n",
    "                    '--output', 'temp_' + Path(notebook_path).name,\n",
    "                    notebook_path\n",
    "                ],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            # Check if the notebook created expected outputs\n",
    "            if result.returncode == 0:\n",
    "                # Check if expected files were created\n",
    "                expected_outputs = {\n",
    "                    'F1_Model_Fixes_and_Validation.ipynb': 'f1_position_prediction_model.pkl',\n",
    "                    'F1_Integrated_Driver_Evaluation.ipynb': 'f1_integrated_evaluation_model.pkl',\n",
    "                    'F1_Prize_Picks_Optimizer.ipynb': 'f1_prize_picks_optimizer.pkl'\n",
    "                }\n",
    "                \n",
    "                notebook_name = Path(notebook_path).name\n",
    "                if notebook_name in expected_outputs:\n",
    "                    output_file = expected_outputs[notebook_name]\n",
    "                    if Path(output_file).exists():\n",
    "                        logger.info(f\"✓ Successfully created: {output_file}\")\n",
    "                        self.executed_notebooks.append(notebook_path)\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(f\"⚠ Notebook ran but didn't create expected output: {output_file}\")\n",
    "                        # Still mark as success if it's not a critical notebook\n",
    "                        if 'Evaluation' not in notebook_name:\n",
    "                            self.executed_notebooks.append(notebook_path)\n",
    "                            return True\n",
    "                else:\n",
    "                    # Non-critical notebook\n",
    "                    logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                    self.executed_notebooks.append(notebook_path)\n",
    "                    return True\n",
    "            \n",
    "            logger.error(f\"✗ Failed to execute: {notebook_path}\")\n",
    "            if result.stderr:\n",
    "                logger.error(f\"Error: {result.stderr}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Exception running {notebook_path}: {str(e)}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "        finally:\n",
    "            # Clean up temp files\n",
    "            temp_file = Path('temp_' + Path(notebook_path).name)\n",
    "            if temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "    \n",
    "    def run_notebooks_in_order(self, notebook_list):\n",
    "        \"\"\"\n",
    "        Run a list of notebooks in order\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for notebook in notebook_list:\n",
    "            if Path(notebook).exists():\n",
    "                success = self.run_notebook(notebook)\n",
    "                results.append((notebook, success))\n",
    "                # Don't stop on failure for non-critical notebooks\n",
    "                if not success and any(critical in notebook for critical in ['Model_Fixes', 'Feature_Store']):\n",
    "                    logger.warning(f\"Stopping execution due to failure in critical notebook: {notebook}\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(f\"Notebook not found: {notebook}\")\n",
    "                results.append((notebook, False))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get execution summary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'executed': self.executed_notebooks,\n",
    "            'failed': self.failed_notebooks,\n",
    "            'success_rate': len(self.executed_notebooks) / (len(self.executed_notebooks) + len(self.failed_notebooks)) if (self.executed_notebooks or self.failed_notebooks) else 0\n",
    "        }\n",
    "\n",
    "# Initialize runner\n",
    "runner = NotebookRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookRunner:\n",
    "    \"\"\"\n",
    "    Utility to run other Jupyter notebooks programmatically\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.executed_notebooks = []\n",
    "        self.failed_notebooks = []\n",
    "    \n",
    "    def run_notebook(self, notebook_path, timeout=600):\n",
    "        \"\"\"\n",
    "        Execute a Jupyter notebook and return success status\n",
    "        \"\"\"\n",
    "        logger.info(f\"Running notebook: {notebook_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use nbconvert to execute the notebook\n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "                    '--to', 'notebook',\n",
    "                    '--execute',\n",
    "                    '--ExecutePreprocessor.timeout=' + str(timeout),\n",
    "                    '--inplace',\n",
    "                    '--clear-output',\n",
    "                    notebook_path\n",
    "                ],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                logger.info(f\"✓ Successfully executed: {notebook_path}\")\n",
    "                self.executed_notebooks.append(notebook_path)\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"✗ Failed to execute: {notebook_path}\")\n",
    "                logger.error(f\"Error: {result.stderr}\")\n",
    "                self.failed_notebooks.append(notebook_path)\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Exception running {notebook_path}: {str(e)}\")\n",
    "            self.failed_notebooks.append(notebook_path)\n",
    "            return False\n",
    "    \n",
    "    def run_notebooks_in_order(self, notebook_list):\n",
    "        \"\"\"\n",
    "        Run a list of notebooks in order\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for notebook in notebook_list:\n",
    "            if Path(notebook).exists():\n",
    "                success = self.run_notebook(notebook)\n",
    "                results.append((notebook, success))\n",
    "                if not success:\n",
    "                    logger.warning(f\"Stopping execution due to failure in {notebook}\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(f\"Notebook not found: {notebook}\")\n",
    "                results.append((notebook, False))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get execution summary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'executed': self.executed_notebooks,\n",
    "            'failed': self.failed_notebooks,\n",
    "            'success_rate': len(self.executed_notebooks) / (len(self.executed_notebooks) + len(self.failed_notebooks)) if (self.executed_notebooks or self.failed_notebooks) else 0\n",
    "        }\n",
    "\n",
    "# Initialize runner\n",
    "runner = NotebookRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Required Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 09:38:48,387 - F1Pipeline - INFO - Data directory: C:\\Users\\tenei\\Documents\\GitHub\\Formula1\\data\\f1db\n",
      "2025-07-22 09:38:48,390 - F1Pipeline - INFO - Pipeline configuration initialized\n"
     ]
    }
   ],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the F1 pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths - use relative paths that work on both Windows and Linux\n",
    "        current_dir = Path.cwd()\n",
    "        \n",
    "        # Find the data directory relative to current location\n",
    "        if (current_dir / '../../data/f1db').exists():\n",
    "            self.data_dir = (current_dir / '../../data/f1db').resolve()\n",
    "        elif (current_dir.parent.parent / 'data' / 'f1db').exists():\n",
    "            self.data_dir = (current_dir.parent.parent / 'data' / 'f1db').resolve()\n",
    "        else:\n",
    "            # Try to find it from the root\n",
    "            possible_paths = [\n",
    "                current_dir / 'data' / 'f1db',\n",
    "                current_dir.parent / 'data' / 'f1db',\n",
    "                current_dir.parent.parent / 'data' / 'f1db',\n",
    "                Path('data/f1db'),\n",
    "                Path('../data/f1db'),\n",
    "                Path('../../data/f1db')\n",
    "            ]\n",
    "            for p in possible_paths:\n",
    "                if p.exists():\n",
    "                    self.data_dir = p.resolve()\n",
    "                    break\n",
    "            else:\n",
    "                # Default to relative path\n",
    "                self.data_dir = Path('../../data/f1db').resolve()\n",
    "        \n",
    "        self.model_dir = Path('.')\n",
    "        self.output_dir = Path('pipeline_outputs')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Model settings\n",
    "        self.use_cached_data = True\n",
    "        self.auto_sync = True\n",
    "        self.cache_expiry_hours = 24\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.bankroll = 1000\n",
    "        self.kelly_fraction = 0.25\n",
    "        self.max_correlation = 0.5\n",
    "        self.min_edge = 0.05\n",
    "        self.max_exposure = 0.25\n",
    "        \n",
    "        # Constraints\n",
    "        self.constraints = {\n",
    "            'max_per_driver': 2,\n",
    "            'max_per_type': 3,\n",
    "            'min_avg_edge': 0.08\n",
    "        }\n",
    "        \n",
    "        # Pipeline settings\n",
    "        self.generate_report = True\n",
    "        self.save_predictions = True\n",
    "        self.mlflow_tracking = False\n",
    "        \n",
    "        logger.info(f\"Data directory: {self.data_dir}\")\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return {\n",
    "            'data_dir': str(self.data_dir),\n",
    "            'model_dir': str(self.model_dir),\n",
    "            'output_dir': str(self.output_dir),\n",
    "            'bankroll': self.bankroll,\n",
    "            'kelly_fraction': self.kelly_fraction,\n",
    "            'max_correlation': self.max_correlation,\n",
    "            'min_edge': self.min_edge,\n",
    "            'max_exposure': self.max_exposure,\n",
    "            'constraints': self.constraints\n",
    "        }\n",
    "    \n",
    "    def save(self, path='pipeline_config.json'):\n",
    "        \"\"\"Save configuration\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path='pipeline_config.json'):\n",
    "        \"\"\"Load configuration\"\"\"\n",
    "        config = cls()\n",
    "        if Path(path).exists():\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for key, value in data.items():\n",
    "                    if hasattr(config, key):\n",
    "                        if key.endswith('_dir'):\n",
    "                            setattr(config, key, Path(value))\n",
    "                        else:\n",
    "                            setattr(config, key, value)\n",
    "        return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = PipelineConfig()\n",
    "config.save()\n",
    "logger.info(f\"Pipeline configuration initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Notebook Execution Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the notebooks that need to be run in order\n",
    "NOTEBOOK_PIPELINE = [\n",
    "    {\n",
    "        'name': 'Model Fixes and Validation',\n",
    "        'notebook': 'F1_Model_Fixes_and_Validation.ipynb',\n",
    "        'creates': ['f1_position_prediction_model.pkl'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Feature Store',\n",
    "        'notebook': 'F1_Feature_Store.ipynb',\n",
    "        'creates': ['F1FeatureStore class'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Integrated Driver Evaluation',\n",
    "        'notebook': 'F1_Integrated_Driver_Evaluation.ipynb',\n",
    "        'creates': ['f1_integrated_evaluation_model.pkl'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Prize Picks Optimizer',\n",
    "        'notebook': 'F1_Prize_Picks_Optimizer.ipynb',\n",
    "        'creates': ['f1_prize_picks_optimizer.pkl', 'PrizePicksOptimizer class'],\n",
    "        'required': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Explainability Engine',\n",
    "        'notebook': 'F1_Explainability_Engine.ipynb',\n",
    "        'creates': ['Explainer classes'],\n",
    "        'required': False\n",
    "    },\n",
    "    {\n",
    "        'name': 'MLflow Tracking',\n",
    "        'notebook': 'F1_MLflow_Tracking.ipynb',\n",
    "        'creates': ['MLflow setup'],\n",
    "        'required': False\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize and Run Required Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING F1 PIPELINE COMPONENTS\n",
      "============================================================\n",
      "\n",
      "✓ Skipping: Model Fixes and Validation (outputs exist)\n",
      "✓ Skipping: Feature Store (outputs exist)\n",
      "✓ Skipping: Integrated Driver Evaluation (outputs exist)\n",
      "✓ Skipping: Prize Picks Optimizer (outputs exist)\n",
      "\n",
      "✅ All components already initialized!\n"
     ]
    }
   ],
   "source": [
    "def initialize_pipeline_components(force_rerun=False):\n",
    "    \"\"\"\n",
    "    Initialize all pipeline components by running required notebooks\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INITIALIZING F1 PIPELINE COMPONENTS\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    notebooks_to_run = []\n",
    "    \n",
    "    for component in NOTEBOOK_PIPELINE:\n",
    "        if component['required'] or force_rerun:\n",
    "            # Check if outputs already exist\n",
    "            needs_run = force_rerun\n",
    "            \n",
    "            if not force_rerun:\n",
    "                for output in component['creates']:\n",
    "                    if output.endswith('.pkl') and not Path(output).exists():\n",
    "                        needs_run = True\n",
    "                        break\n",
    "            \n",
    "            if needs_run:\n",
    "                notebooks_to_run.append(component)\n",
    "                print(f\"📋 Will run: {component['name']}\")\n",
    "            else:\n",
    "                print(f\"✓ Skipping: {component['name']} (outputs exist)\")\n",
    "    \n",
    "    if not notebooks_to_run:\n",
    "        print(\"\\n✅ All components already initialized!\")\n",
    "        return True\n",
    "    \n",
    "    # Run the notebooks\n",
    "    print(f\"\\nRunning {len(notebooks_to_run)} notebooks...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for component in notebooks_to_run:\n",
    "        success = runner.run_notebook(component['notebook'])\n",
    "        if not success and component['required']:\n",
    "            print(f\"\\n❌ Failed to initialize {component['name']}\")\n",
    "            return False\n",
    "    \n",
    "    print(\"\\n✅ Pipeline initialization complete!\")\n",
    "    return True\n",
    "\n",
    "# Option to force rerun all notebooks (set to True if needed)\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# Initialize components\n",
    "initialization_success = initialize_pipeline_components(force_rerun=FORCE_RERUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This **Unified Master Pipeline Integration** notebook combines the best of both worlds:\n",
    "\n",
    "### 🎯 **From the Master Version**\n",
    "1. **Automatic notebook execution** - Runs prerequisite notebooks in correct order\n",
    "2. **Component checking** - Verifies which models/files exist\n",
    "3. **Correct data loader** - Uses `f1db_data_loader.py`\n",
    "4. **Smart initialization** - Only runs what's needed\n",
    "\n",
    "### 🚀 **From the Original Version**\n",
    "1. **RaceWeekendAutomation** - Schedules T-3 days, T-1 day, T-4 hours analyses\n",
    "2. **PerformanceMonitor** - Tracks accuracy and plots trends over time\n",
    "3. **Pipeline state management** - Saves complete state for recovery\n",
    "4. **Standalone run script** - Command-line interface for production use\n",
    "\n",
    "### 📋 **Complete Feature Set**\n",
    "- ✅ Runs other notebooks programmatically\n",
    "- ✅ Race weekend automation scheduling\n",
    "- ✅ Performance tracking and visualization\n",
    "- ✅ State persistence and recovery\n",
    "- ✅ Command-line interface\n",
    "- ✅ Backtesting integration\n",
    "- ✅ Proper error handling\n",
    "\n",
    "### 🔧 **Usage Options**\n",
    "\n",
    "**Interactive (Jupyter)**:\n",
    "```python\n",
    "# First time - runs all required notebooks\n",
    "pipeline = F1PrizePipeline(config)\n",
    "results = pipeline.run()\n",
    "\n",
    "# Schedule race weekend\n",
    "automation = RaceWeekendAutomation(pipeline)\n",
    "automation.schedule_race_analysis(race_id, race_date)\n",
    "\n",
    "# Track performance\n",
    "monitor = PerformanceMonitor(config.output_dir)\n",
    "monitor.plot_performance_trends()\n",
    "```\n",
    "\n",
    "**Command Line**:\n",
    "```bash\n",
    "python run_f1_pipeline.py                  # Upcoming race\n",
    "python run_f1_pipeline.py --race-id 1234   # Specific race\n",
    "python run_f1_pipeline.py --schedule       # Schedule automation\n",
    "python run_f1_pipeline.py --backtest       # Run backtesting\n",
    "```\n",
    "\n",
    "### 🏁 **Original Pipeline Status**\n",
    "The original `F1_Pipeline_Integration.ipynb` is now **DEPRECATED**. This master version includes all its functionality plus notebook orchestration. You can safely delete the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 09:39:02,894 - F1Pipeline - INFO - Pipeline configuration initialized\n"
     ]
    }
   ],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the F1 pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.data_dir = Path('/workspace/data/f1db')\n",
    "        self.model_dir = Path('.')\n",
    "        self.output_dir = Path('pipeline_outputs')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Model settings\n",
    "        self.use_cached_data = True\n",
    "        self.auto_sync = True\n",
    "        self.cache_expiry_hours = 24\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.bankroll = 1000\n",
    "        self.kelly_fraction = 0.25\n",
    "        self.max_correlation = 0.5\n",
    "        self.min_edge = 0.05\n",
    "        self.max_exposure = 0.25\n",
    "        \n",
    "        # Constraints\n",
    "        self.constraints = {\n",
    "            'max_per_driver': 2,\n",
    "            'max_per_type': 3,\n",
    "            'min_avg_edge': 0.08\n",
    "        }\n",
    "        \n",
    "        # Pipeline settings\n",
    "        self.generate_report = True\n",
    "        self.save_predictions = True\n",
    "        self.mlflow_tracking = False\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return {\n",
    "            'data_dir': str(self.data_dir),\n",
    "            'model_dir': str(self.model_dir),\n",
    "            'output_dir': str(self.output_dir),\n",
    "            'bankroll': self.bankroll,\n",
    "            'kelly_fraction': self.kelly_fraction,\n",
    "            'max_correlation': self.max_correlation,\n",
    "            'min_edge': self.min_edge,\n",
    "            'max_exposure': self.max_exposure,\n",
    "            'constraints': self.constraints\n",
    "        }\n",
    "    \n",
    "    def save(self, path='pipeline_config.json'):\n",
    "        \"\"\"Save configuration\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path='pipeline_config.json'):\n",
    "        \"\"\"Load configuration\"\"\"\n",
    "        config = cls()\n",
    "        if Path(path).exists():\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for key, value in data.items():\n",
    "                    if hasattr(config, key):\n",
    "                        setattr(config, key, value)\n",
    "        return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = PipelineConfig()\n",
    "config.save()\n",
    "logger.info(f\"Pipeline configuration initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceWeekendAutomation:\n",
    "    \"\"\"\n",
    "    Automate pipeline execution for race weekends\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: F1PrizePipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.schedule = []\n",
    "    \n",
    "    def get_race_schedule(self):\n",
    "        \"\"\"Get upcoming race schedule\"\"\"\n",
    "        races = self.pipeline.data.get('races', pd.DataFrame())\n",
    "        if races.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get future races\n",
    "        races['date'] = pd.to_datetime(races['date'])\n",
    "        future_races = races[races['date'] > datetime.now()]\n",
    "        \n",
    "        return future_races.sort_values('date')\n",
    "    \n",
    "    def schedule_race_analysis(self, race_id, race_date):\n",
    "        \"\"\"Schedule analysis for a specific race\"\"\"\n",
    "        # Run at different times\n",
    "        schedule_times = [\n",
    "            (race_date - timedelta(days=3), 'Initial Analysis'),\n",
    "            (race_date - timedelta(days=1), 'Pre-Qualifying Update'),\n",
    "            (race_date - timedelta(hours=4), 'Final Predictions')\n",
    "        ]\n",
    "        \n",
    "        for run_time, description in schedule_times:\n",
    "            self.schedule.append({\n",
    "                'race_id': race_id,\n",
    "                'run_time': run_time,\n",
    "                'description': description,\n",
    "                'status': 'scheduled'\n",
    "            })\n",
    "    \n",
    "    def execute_scheduled_runs(self):\n",
    "        \"\"\"Execute scheduled pipeline runs\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        for task in self.schedule:\n",
    "            if task['status'] == 'scheduled' and task['run_time'] <= current_time:\n",
    "                logger.info(f\"Executing {task['description']} for race {task['race_id']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Update config based on timing\n",
    "                    if 'Final' in task['description']:\n",
    "                        self.pipeline.config.kelly_fraction = 0.20  # More conservative\n",
    "                    \n",
    "                    # Run pipeline\n",
    "                    results = self.pipeline.run(task['race_id'])\n",
    "                    \n",
    "                    # Save results with timestamp\n",
    "                    output_name = f\"race_{task['race_id']}_{task['description'].replace(' ', '_')}_{current_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "                    self.pipeline.generate_report(\n",
    "                        self.pipeline.config.output_dir / f\"{output_name}.json\"\n",
    "                    )\n",
    "                    \n",
    "                    task['status'] = 'completed'\n",
    "                    task['completed_at'] = current_time.isoformat()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to execute {task['description']}: {str(e)}\")\n",
    "                    task['status'] = 'failed'\n",
    "                    task['error'] = str(e)\n",
    "    \n",
    "    def generate_weekend_summary(self):\n",
    "        \"\"\"Generate summary of all analyses for a race weekend\"\"\"\n",
    "        completed_tasks = [t for t in self.schedule if t['status'] == 'completed']\n",
    "        \n",
    "        if not completed_tasks:\n",
    "            return None\n",
    "        \n",
    "        summary = {\n",
    "            'race_id': completed_tasks[0]['race_id'],\n",
    "            'analyses_completed': len(completed_tasks),\n",
    "            'final_recommendations': None\n",
    "        }\n",
    "        \n",
    "        # Get final predictions\n",
    "        final_task = next((t for t in completed_tasks if 'Final' in t['description']), None)\n",
    "        if final_task:\n",
    "            # Load the report\n",
    "            report_files = list(self.pipeline.config.output_dir.glob(f\"race_{final_task['race_id']}_Final*.json\"))\n",
    "            if report_files:\n",
    "                with open(report_files[-1], 'r') as f:\n",
    "                    final_report = json.load(f)\n",
    "                    summary['final_recommendations'] = final_report.get('portfolio', [])\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Example usage for race weekend automation\n",
    "if 'pipeline' in locals() and hasattr(pipeline, 'data') and pipeline.data:\n",
    "    automation = RaceWeekendAutomation(pipeline)\n",
    "    \n",
    "    # Get upcoming races\n",
    "    upcoming_races = automation.get_race_schedule()\n",
    "    if not upcoming_races.empty:\n",
    "        print(\"\\nUpcoming Races:\")\n",
    "        print(\"=\" * 60)\n",
    "        for idx, race in upcoming_races.head(3).iterrows():\n",
    "            print(f\"{race['date'].strftime('%Y-%m-%d')}: {race['name']} (Round {race['round']})\")\n",
    "            \n",
    "            # Schedule analysis for next race\n",
    "            if idx == upcoming_races.index[0]:  # First race\n",
    "                automation.schedule_race_analysis(race['raceId'], race['date'])\n",
    "        \n",
    "        print(f\"\\nScheduled {len(automation.schedule)} analyses for next race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Monitoring (From Original Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor pipeline and prediction performance\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.metrics = []\n",
    "    \n",
    "    def track_predictions(self, predictions, actuals=None):\n",
    "        \"\"\"Track prediction accuracy\"\"\"\n",
    "        metric = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_predictions': len(predictions),\n",
    "            'avg_confidence': predictions['confidence'].mean() if 'confidence' in predictions else 0\n",
    "        }\n",
    "        \n",
    "        if actuals is not None:\n",
    "            # Calculate accuracy metrics\n",
    "            metric['accuracy'] = self._calculate_accuracy(predictions, actuals)\n",
    "        \n",
    "        self.metrics.append(metric)\n",
    "    \n",
    "    def _calculate_accuracy(self, predictions, actuals):\n",
    "        \"\"\"Calculate prediction accuracy\"\"\"\n",
    "        # Implementation depends on actual data format\n",
    "        # This is a placeholder - implement based on your needs\n",
    "        return 0.0\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"Generate performance report\"\"\"\n",
    "        if not self.metrics:\n",
    "            return None\n",
    "        \n",
    "        report = {\n",
    "            'period': {\n",
    "                'start': self.metrics[0]['timestamp'],\n",
    "                'end': self.metrics[-1]['timestamp']\n",
    "            },\n",
    "            'total_predictions': sum(m['n_predictions'] for m in self.metrics),\n",
    "            'avg_confidence': np.mean([m['avg_confidence'] for m in self.metrics]),\n",
    "            'runs_completed': len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def plot_performance_trends(self):\n",
    "        \"\"\"Plot performance trends over time\"\"\"\n",
    "        if not self.metrics:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        timestamps = [pd.to_datetime(m['timestamp']) for m in self.metrics]\n",
    "        confidences = [m['avg_confidence'] for m in self.metrics]\n",
    "        \n",
    "        ax.plot(timestamps, confidences, marker='o')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Average Confidence')\n",
    "        ax.set_title('Model Confidence Over Time')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save metrics to file\"\"\"\n",
    "        metrics_file = self.output_dir / 'performance_metrics.json'\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        logger.info(f\"Saved performance metrics to {metrics_file}\")\n",
    "    \n",
    "    def load_metrics(self):\n",
    "        \"\"\"Load existing metrics\"\"\"\n",
    "        metrics_file = self.output_dir / 'performance_metrics.json'\n",
    "        if metrics_file.exists():\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                self.metrics = json.load(f)\n",
    "            logger.info(f\"Loaded {len(self.metrics)} historical metrics\")\n",
    "\n",
    "# Initialize performance monitor\n",
    "if 'config' in locals():\n",
    "    monitor = PerformanceMonitor(config.output_dir)\n",
    "    monitor.load_metrics()  # Load any existing metrics\n",
    "    \n",
    "    # Track current predictions if available\n",
    "    if 'pipeline' in locals() and 'predictions' in pipeline.results:\n",
    "        monitor.track_predictions(pipeline.results['predictions'])\n",
    "        monitor.save_metrics()\n",
    "        \n",
    "        performance_report = monitor.generate_performance_report()\n",
    "        print(\"\\nPerformance Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        for key, value in performance_report.items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Pipeline State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline state saved to pipeline_outputs\\pipeline_state.json\n",
      "\n",
      "Standalone run script created: run_f1_pipeline.py\n",
      "\n",
      "Usage examples:\n",
      "  python run_f1_pipeline.py                  # Run for upcoming race\n",
      "  python run_f1_pipeline.py --race-id 1234   # Run for specific race\n",
      "  python run_f1_pipeline.py --backtest       # Run backtesting\n",
      "  python run_f1_pipeline.py --schedule       # Schedule race weekend automation\n"
     ]
    }
   ],
   "source": [
    "# Save complete pipeline state for future use\n",
    "pipeline_state = {\n",
    "    'config': config.to_dict(),\n",
    "    'last_run': datetime.now().isoformat(),\n",
    "    'results_summary': pipeline._generate_summary() if 'pipeline' in locals() else {},\n",
    "    'automation_schedule': automation.schedule if 'automation' in locals() else [],\n",
    "    'performance_metrics': monitor.metrics[-10:] if 'monitor' in locals() else [],  # Last 10 metrics\n",
    "    'notebook_execution': runner.get_summary() if 'runner' in locals() else {}\n",
    "}\n",
    "\n",
    "# Save state\n",
    "state_path = config.output_dir / 'pipeline_state.json'\n",
    "with open(state_path, 'w') as f:\n",
    "    json.dump(pipeline_state, f, indent=2)\n",
    "\n",
    "print(f\"\\nPipeline state saved to {state_path}\")\n",
    "\n",
    "# Create standalone run script for easy execution\n",
    "run_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run F1 Prize Picks Pipeline\n",
    "\n",
    "Usage:\n",
    "    python run_f1_pipeline.py              # Run for upcoming race\n",
    "    python run_f1_pipeline.py --race-id 1234  # Run for specific race\n",
    "    python run_f1_pipeline.py --backtest   # Run backtesting\n",
    "    python run_f1_pipeline.py --schedule   # Schedule race weekend automation\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Add notebook directory to path\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "\n",
    "def run_master_notebook(race_id=None, mode='predict'):\n",
    "    \"\"\"Run the master pipeline notebook\"\"\"\n",
    "    # Convert notebook to script first\n",
    "    subprocess.run([\n",
    "        sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "        '--to', 'script',\n",
    "        'F1_Pipeline_Integration_Master.ipynb'\n",
    "    ])\n",
    "    \n",
    "    # Import and run\n",
    "    from F1_Pipeline_Integration_Master import F1PrizePipeline, PipelineConfig\n",
    "    from F1_Pipeline_Integration_Master import RaceWeekendAutomation, PerformanceMonitor\n",
    "    \n",
    "    # Load configuration\n",
    "    config = PipelineConfig.load()\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    \n",
    "    if mode == 'schedule':\n",
    "        # Run automation\n",
    "        automation = RaceWeekendAutomation(pipeline)\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        upcoming = automation.get_race_schedule()\n",
    "        if not upcoming.empty:\n",
    "            next_race = upcoming.iloc[0]\n",
    "            automation.schedule_race_analysis(next_race['raceId'], next_race['date'])\n",
    "            print(f\"Scheduled analyses for {next_race['name']}\")\n",
    "            automation.execute_scheduled_runs()\n",
    "    elif mode == 'backtest':\n",
    "        print(\"Running backtesting...\")\n",
    "        # Import and run backtesting notebook\n",
    "        subprocess.run([\n",
    "            sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "            '--to', 'notebook',\n",
    "            '--execute',\n",
    "            'F1_Backtesting_Framework.ipynb'\n",
    "        ])\n",
    "    else:\n",
    "        # Normal prediction mode\n",
    "        results = pipeline.run(race_id)\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\\\nPipeline completed successfully!\")\n",
    "            print(f\"Results saved to {config.output_dir}\")\n",
    "            \n",
    "            # Track performance\n",
    "            monitor = PerformanceMonitor(config.output_dir)\n",
    "            monitor.load_metrics()\n",
    "            monitor.track_predictions(results.get('predictions', pd.DataFrame()))\n",
    "            monitor.save_metrics()\n",
    "        else:\n",
    "            print(\"\\\\nPipeline failed. Check logs for details.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Run F1 Prize Picks Pipeline')\n",
    "    parser.add_argument('--race-id', type=int, help='Specific race ID to analyze')\n",
    "    parser.add_argument('--backtest', action='store_true', \n",
    "                       help='Run backtesting instead of predictions')\n",
    "    parser.add_argument('--schedule', action='store_true',\n",
    "                       help='Schedule automated race weekend analyses')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.backtest:\n",
    "        run_master_notebook(mode='backtest')\n",
    "    elif args.schedule:\n",
    "        run_master_notebook(mode='schedule')\n",
    "    else:\n",
    "        run_master_notebook(race_id=args.race_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save run script\n",
    "script_path = Path('run_f1_pipeline.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(run_script)\n",
    "\n",
    "# Make it executable on Unix-like systems\n",
    "import os\n",
    "if os.name != 'nt':  # Not Windows\n",
    "    os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"\\nStandalone run script created: {script_path}\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  python run_f1_pipeline.py                  # Run for upcoming race\")\n",
    "print(\"  python run_f1_pipeline.py --race-id 1234   # Run for specific race\")\n",
    "print(\"  python run_f1_pipeline.py --backtest       # Run backtesting\")\n",
    "print(\"  python run_f1_pipeline.py --schedule       # Schedule race weekend automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Race Weekend Automation (From Original Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Components (Using Correct Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 09:39:03,087 - F1Pipeline - INFO - ✓ Loaded f1db_data_loader\n",
      "2025-07-22 09:39:03,099 - F1Pipeline - WARNING - ✗ F1FeatureStore not available - run F1_Feature_Store.ipynb first\n",
      "2025-07-22 09:39:03,104 - F1Pipeline - WARNING - ✗ IntegratedF1Predictor not available\n",
      "2025-07-22 09:39:03,104 - F1Pipeline - WARNING - ✗ PrizePicksOptimizer not available\n",
      "2025-07-22 09:39:03,104 - F1Pipeline - WARNING - ✗ Explainability components not available\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary components with proper error handling\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Data loading - Using the correct f1db_data_loader.py\n",
    "try:\n",
    "    from f1db_data_loader import F1DBDataLoader, load_f1db_data\n",
    "    logger.info(\"✓ Loaded f1db_data_loader\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"✗ Could not import f1db_data_loader: {e}\")\n",
    "    F1DBDataLoader = None\n",
    "    load_f1db_data = None\n",
    "\n",
    "# Feature engineering\n",
    "try:\n",
    "    # Try to import from the notebook-generated module\n",
    "    from F1_Feature_Store import F1FeatureStore\n",
    "    logger.info(\"✓ Loaded F1FeatureStore\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ F1FeatureStore not available - run F1_Feature_Store.ipynb first\")\n",
    "    F1FeatureStore = None\n",
    "\n",
    "# Model components\n",
    "try:\n",
    "    from F1_Integrated_Driver_Evaluation import IntegratedF1Predictor\n",
    "    logger.info(\"✓ Loaded IntegratedF1Predictor\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ IntegratedF1Predictor not available\")\n",
    "    IntegratedF1Predictor = None\n",
    "\n",
    "# Optimization\n",
    "try:\n",
    "    from F1_Prize_Picks_Optimizer import PrizePicksOptimizer, KellyCriterion, PrizePicksBetTypes\n",
    "    logger.info(\"✓ Loaded PrizePicksOptimizer\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ PrizePicksOptimizer not available\")\n",
    "    PrizePicksOptimizer = None\n",
    "\n",
    "# Explainability\n",
    "try:\n",
    "    from F1_Explainability_Engine import PredictionExplainer, PrizePicksExplainer\n",
    "    logger.info(\"✓ Loaded Explainability components\")\n",
    "except ImportError:\n",
    "    logger.warning(\"✗ Explainability components not available\")\n",
    "    PredictionExplainer = None\n",
    "    PrizePicksExplainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1PrizePipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrating all components\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = None\n",
    "        self.feature_store = None\n",
    "        self.predictor = None\n",
    "        self.optimizer = None\n",
    "        self.explainer = None\n",
    "        self.results = {}\n",
    "        \n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all pipeline components\"\"\"\n",
    "        logger.info(\"Initializing pipeline components...\")\n",
    "        \n",
    "        # Data loader - Using f1db_data_loader\n",
    "        if F1DBDataLoader:\n",
    "            self.data_loader = F1DBDataLoader(\n",
    "                base_path=str(self.config.data_dir)\n",
    "            )\n",
    "            logger.info(\"✓ Initialized F1DBDataLoader\")\n",
    "        else:\n",
    "            logger.error(\"✗ F1DBDataLoader not available\")\n",
    "        \n",
    "        # Load saved models if available\n",
    "        try:\n",
    "            self.predictor = joblib.load(self.config.model_dir / 'f1_integrated_evaluation_model.pkl')\n",
    "            logger.info(\"✓ Loaded integrated predictor\")\n",
    "        except:\n",
    "            logger.warning(\"✗ Could not load integrated predictor\")\n",
    "            self.predictor = None\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if PrizePicksOptimizer:\n",
    "            try:\n",
    "                optimizer_config = joblib.load(self.config.model_dir / 'f1_prize_picks_optimizer.pkl')\n",
    "                self.optimizer = optimizer_config['optimizer']\n",
    "                logger.info(\"✓ Loaded Prize Picks optimizer\")\n",
    "            except:\n",
    "                logger.warning(\"Creating new optimizer\")\n",
    "                self.optimizer = PrizePicksOptimizer(\n",
    "                    kelly_fraction=self.config.kelly_fraction,\n",
    "                    max_correlation=self.config.max_correlation\n",
    "                )\n",
    "        \n",
    "        # Feature store\n",
    "        if F1FeatureStore:\n",
    "            self.feature_store = F1FeatureStore()\n",
    "            logger.info(\"✓ Initialized Feature Store\")\n",
    "        \n",
    "        # Explainers\n",
    "        if PredictionExplainer:\n",
    "            self.prediction_explainer = PredictionExplainer(None, [])\n",
    "        if PrizePicksExplainer:\n",
    "            self.pp_explainer = PrizePicksExplainer()\n",
    "        \n",
    "        logger.info(\"Pipeline components initialization complete\")\n",
    "    \n",
    "    def load_data(self, force_update=False):\n",
    "        \"\"\"Load and prepare F1 data\"\"\"\n",
    "        logger.info(\"Loading F1 data...\")\n",
    "        \n",
    "        if not self.data_loader:\n",
    "            logger.error(\"Data loader not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Load data using f1db_data_loader\n",
    "        if load_f1db_data:\n",
    "            self.data = load_f1db_data(base_path=str(self.config.data_dir))\n",
    "        else:\n",
    "            # Fallback to manual loading\n",
    "            self.data = self.data_loader.load_all_data()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} datasets\")\n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, race_id=None):\n",
    "        \"\"\"Prepare features for prediction\"\"\"\n",
    "        logger.info(\"Preparing features...\")\n",
    "        \n",
    "        # Get upcoming race if no race_id specified\n",
    "        if race_id is None:\n",
    "            races = self.data.get('races', pd.DataFrame())\n",
    "            if not races.empty:\n",
    "                races['date'] = pd.to_datetime(races['date'])\n",
    "                upcoming = races[races['date'] > datetime.now()].iloc[0]\n",
    "                race_id = upcoming['raceId']\n",
    "                logger.info(f\"Preparing for upcoming race: {upcoming['name']}\")\n",
    "            else:\n",
    "                logger.warning(\"No upcoming race found\")\n",
    "                return None\n",
    "        \n",
    "        # Build feature set\n",
    "        if self.feature_store and hasattr(self.feature_store, 'engineer_features'):\n",
    "            features = self.feature_store.engineer_features(self.data)\n",
    "        else:\n",
    "            # Basic feature preparation\n",
    "            features = self._create_basic_features()\n",
    "        \n",
    "        self.results['features'] = features\n",
    "        return features\n",
    "    \n",
    "    def _create_basic_features(self):\n",
    "        \"\"\"Create basic features if feature store not available\"\"\"\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        if results.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Simple feature engineering\n",
    "        driver_stats = results.groupby('driverId').agg({\n",
    "            'positionOrder': ['mean', 'std'],\n",
    "            'points': ['mean', 'sum'],\n",
    "            'grid': 'mean'\n",
    "        })\n",
    "        \n",
    "        driver_stats.columns = ['avg_position', 'position_std', \n",
    "                               'avg_points', 'total_points', 'avg_grid']\n",
    "        \n",
    "        return driver_stats\n",
    "    \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions for all drivers\"\"\"\n",
    "        logger.info(\"Generating predictions...\")\n",
    "        \n",
    "        # Get active drivers\n",
    "        drivers = self.data.get('drivers', pd.DataFrame())\n",
    "        results = self.data.get('results', pd.DataFrame())\n",
    "        \n",
    "        if drivers.empty or results.empty:\n",
    "            logger.error(\"No driver or results data available\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get drivers who raced recently\n",
    "        recent_drivers = results[results['year'] >= 2023]['driverId'].unique()\n",
    "        active_drivers = drivers[drivers['driverId'].isin(recent_drivers)]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for _, driver in active_drivers.iterrows():\n",
    "            # Get driver stats\n",
    "            driver_results = results[results['driverId'] == driver['driverId']].tail(10)\n",
    "            \n",
    "            if len(driver_results) >= 3:\n",
    "                # Calculate probabilities\n",
    "                top10_prob = (driver_results['positionOrder'] <= 10).mean()\n",
    "                top5_prob = (driver_results['positionOrder'] <= 5).mean()\n",
    "                top3_prob = (driver_results['positionOrder'] <= 3).mean()\n",
    "                points_prob = (driver_results['points'] > 0).mean()\n",
    "                \n",
    "                # Adjust with model if available\n",
    "                confidence = 0.7 + 0.05 * len(driver_results) / 10\n",
    "                \n",
    "                predictions.append({\n",
    "                    'driver': driver['surname'],\n",
    "                    'driverId': driver['driverId'],\n",
    "                    'top10_prob': min(0.95, top10_prob * 1.1),\n",
    "                    'top5_prob': min(0.85, top5_prob * 1.1),\n",
    "                    'top3_prob': min(0.70, top3_prob * 1.1),\n",
    "                    'points_prob': min(0.95, points_prob * 1.05),\n",
    "                    'beat_teammate_prob': 0.5,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        self.results['predictions'] = pd.DataFrame(predictions)\n",
    "        logger.info(f\"Generated predictions for {len(predictions)} drivers\")\n",
    "        return self.results['predictions']\n",
    "    \n",
    "    def optimize_picks(self):\n",
    "        \"\"\"Optimize Prize Picks selections\"\"\"\n",
    "        logger.info(\"Optimizing Prize Picks...\")\n",
    "        \n",
    "        if 'predictions' not in self.results or self.results['predictions'].empty:\n",
    "            logger.error(\"No predictions available\")\n",
    "            return None\n",
    "        \n",
    "        if not self.optimizer:\n",
    "            logger.error(\"Optimizer not initialized\")\n",
    "            return None\n",
    "        \n",
    "        # Generate all possible picks\n",
    "        all_picks = self.optimizer.generate_all_picks(\n",
    "            self.results['predictions'],\n",
    "            min_edge=self.config.min_edge\n",
    "        )\n",
    "        \n",
    "        if all_picks.empty:\n",
    "            logger.warning(\"No picks with positive edge found\")\n",
    "            return None\n",
    "        \n",
    "        # Optimize portfolio\n",
    "        portfolio = self.optimizer.optimize_portfolio(\n",
    "            all_picks,\n",
    "            bankroll=self.config.bankroll,\n",
    "            constraints=self.config.constraints\n",
    "        )\n",
    "        \n",
    "        self.results['portfolio'] = portfolio\n",
    "        logger.info(f\"Optimized portfolio with {len(portfolio)} parlays\")\n",
    "        return portfolio\n",
    "    \n",
    "    def generate_report(self, save_path=None):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        logger.info(\"Generating report...\")\n",
    "        \n",
    "        report = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'config': self.config.to_dict(),\n",
    "            'summary': self._generate_summary(),\n",
    "            'predictions': self.results.get('predictions', pd.DataFrame()).to_dict('records'),\n",
    "            'portfolio': self._serialize_portfolio(),\n",
    "            'risk_metrics': self._calculate_risk_metrics()\n",
    "        }\n",
    "        \n",
    "        if save_path is None:\n",
    "            save_path = self.config.output_dir / f\"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Report saved to {save_path}\")\n",
    "        return report\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if 'predictions' in self.results and not self.results['predictions'].empty:\n",
    "            summary['n_drivers'] = len(self.results['predictions'])\n",
    "            summary['avg_confidence'] = self.results['predictions']['confidence'].mean()\n",
    "        \n",
    "        if 'portfolio' in self.results:\n",
    "            portfolio = self.results['portfolio']\n",
    "            summary['n_parlays'] = len(portfolio)\n",
    "            summary['total_wagered'] = sum(p['bet_size'] for p in portfolio)\n",
    "            summary['expected_profit'] = sum(p['expected_value'] * p['bet_size'] for p in portfolio)\n",
    "            summary['avg_win_probability'] = np.mean([p['adjusted_prob'] for p in portfolio])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _serialize_portfolio(self):\n",
    "        \"\"\"Serialize portfolio for JSON\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return []\n",
    "        \n",
    "        serialized = []\n",
    "        for parlay in self.results['portfolio']:\n",
    "            parlay_data = {\n",
    "                'n_picks': parlay['n_picks'],\n",
    "                'bet_size': parlay['bet_size'],\n",
    "                'payout': parlay['payout'],\n",
    "                'adjusted_prob': parlay['adjusted_prob'],\n",
    "                'expected_value': parlay['expected_value'],\n",
    "                'kelly_stake': parlay['kelly_stake'],\n",
    "                'picks': parlay['picks'].to_dict('records') if hasattr(parlay['picks'], 'to_dict') else parlay['picks']\n",
    "            }\n",
    "            serialized.append(parlay_data)\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _calculate_risk_metrics(self):\n",
    "        \"\"\"Calculate risk metrics for portfolio\"\"\"\n",
    "        if 'portfolio' not in self.results:\n",
    "            return {}\n",
    "        \n",
    "        portfolio = self.results['portfolio']\n",
    "        \n",
    "        total_exposure = sum(p['bet_size'] for p in portfolio)\n",
    "        \n",
    "        metrics = {\n",
    "            'total_exposure': total_exposure,\n",
    "            'exposure_pct': total_exposure / self.config.bankroll if self.config.bankroll > 0 else 0,\n",
    "            'n_bets': len(portfolio),\n",
    "            'avg_bet_size': total_exposure / len(portfolio) if portfolio else 0,\n",
    "            'max_bet_size': max(p['bet_size'] for p in portfolio) if portfolio else 0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run(self, race_id=None):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        logger.info(\"Starting F1 Prize Picks pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data\n",
    "            self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features\n",
    "            features = self.prepare_features(race_id)\n",
    "            \n",
    "            # Step 3: Generate predictions\n",
    "            predictions = self.generate_predictions()\n",
    "            \n",
    "            # Step 4: Optimize picks\n",
    "            portfolio = self.optimize_picks()\n",
    "            \n",
    "            # Step 5: Generate report\n",
    "            if self.config.save_predictions:\n",
    "                report = self.generate_report()\n",
    "            \n",
    "            logger.info(\"Pipeline completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run the Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Pipeline initialization failed. Please check the error messages above.\n"
     ]
    }
   ],
   "source": [
    "# Check if all components are initialized\n",
    "if initialization_success:\n",
    "    # Initialize and run pipeline\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    logger.info(\"Pipeline initialized\")\n",
    "    \n",
    "    # Run the pipeline\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    # Display results\n",
    "    if results and 'portfolio' in results:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"F1 PRIZE PICKS RECOMMENDATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        portfolio = results['portfolio']\n",
    "        \n",
    "        for i, parlay in enumerate(portfolio, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"PARLAY {i}: {parlay['n_picks']}-PICK ENTRY\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Bet Amount: ${parlay['bet_size']:.2f}\")\n",
    "            print(f\"Potential Payout: ${parlay['bet_size'] * parlay['payout']:.2f} ({parlay['payout']}x)\")\n",
    "            print(f\"Win Probability: {parlay['adjusted_prob']:.1%}\")\n",
    "            print(f\"Expected Value: ${parlay['expected_value'] * parlay['bet_size']:.2f}\")\n",
    "            print(f\"\\nPicks:\")\n",
    "            \n",
    "            picks = parlay['picks']\n",
    "            if hasattr(picks, 'iterrows'):\n",
    "                for j, (_, pick) in enumerate(picks.iterrows(), 1):\n",
    "                    print(f\"  {j}. {pick['driver']} - {pick['bet_type']}\")\n",
    "                    print(f\"     Edge: +{pick['edge']:.1%}\")\n",
    "        \n",
    "        # Summary\n",
    "        summary = pipeline._generate_summary()\n",
    "        if summary:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Total Wagered: ${summary.get('total_wagered', 0):.2f}\")\n",
    "            print(f\"Expected Profit: ${summary.get('expected_profit', 0):.2f}\")\n",
    "            print(f\"Number of Parlays: {summary.get('n_parlays', 0)}\")\n",
    "            print(f\"Average Win Probability: {summary.get('avg_win_probability', 0):.1%}\")\n",
    "    else:\n",
    "        print(\"\\nNo recommendations generated. Check logs for details.\")\n",
    "else:\n",
    "    print(\"\\n❌ Pipeline initialization failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run_pipeline(bankroll=1000, kelly_fraction=0.25):\n",
    "    \"\"\"\n",
    "    Quick function to run the pipeline with custom parameters\n",
    "    \"\"\"\n",
    "    # Update config\n",
    "    config.bankroll = bankroll\n",
    "    config.kelly_fraction = kelly_fraction\n",
    "    \n",
    "    # Initialize and run\n",
    "    pipeline = F1PrizePipeline(config)\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_all_notebooks_fresh():\n",
    "    \"\"\"\n",
    "    Run all notebooks from scratch\n",
    "    \"\"\"\n",
    "    print(\"Running all notebooks from scratch...\")\n",
    "    print(\"This may take 10-15 minutes...\\n\")\n",
    "    \n",
    "    # Force rerun all components\n",
    "    success = initialize_pipeline_components(force_rerun=True)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✅ All notebooks executed successfully!\")\n",
    "        print(\"Now running the main pipeline...\\n\")\n",
    "        \n",
    "        # Run pipeline\n",
    "        pipeline = F1PrizePipeline(config)\n",
    "        results = pipeline.run()\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"\\n❌ Some notebooks failed. Check the logs.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# results = quick_run_pipeline(bankroll=500, kelly_fraction=0.20)\n",
    "# results = run_all_notebooks_fresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Master Pipeline Integration notebook:\n",
    "\n",
    "1. **Automatically runs prerequisite notebooks** in the correct order\n",
    "2. **Uses the correct f1db_data_loader.py** (not enhanced version)\n",
    "3. **Handles missing components gracefully** with informative error messages\n",
    "4. **Provides a single entry point** for the entire F1 Prize Picks pipeline\n",
    "\n",
    "### Usage:\n",
    "- **First time**: The notebook will automatically run all required notebooks\n",
    "- **Subsequent runs**: It will skip notebooks whose outputs already exist\n",
    "- **Force fresh run**: Use `run_all_notebooks_fresh()` to rebuild everything\n",
    "\n",
    "### Key Functions:\n",
    "- `initialize_pipeline_components()` - Run required notebooks\n",
    "- `quick_run_pipeline()` - Run with custom parameters\n",
    "- `run_all_notebooks_fresh()` - Rebuild everything from scratch\n",
    "\n",
    "The pipeline is now self-contained and can orchestrate the entire F1 prediction system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
