{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Prize Picks MLflow Experiment Tracking\n",
    "\n",
    "This notebook sets up MLflow tracking for the F1 Prize Picks optimization pipeline to manage experiments, track metrics, and version models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up MLflow tracking URI\n",
    "MLFLOW_DIR = Path('/app/notebooks/advanced/mlflow')\n",
    "MLFLOW_DIR.mkdir(exist_ok=True)\n",
    "mlflow.set_tracking_uri(f'file://{MLFLOW_DIR.absolute()}')\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1MLflowTracker:\n",
    "    \"\"\"Manages MLflow experiment tracking for F1 predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name=\"f1_prize_picks\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.client = MlflowClient()\n",
    "        self._setup_experiment()\n",
    "        \n",
    "    def _setup_experiment(self):\n",
    "        \"\"\"Create or get experiment\"\"\"\n",
    "        try:\n",
    "            self.experiment = self.client.get_experiment_by_name(self.experiment_name)\n",
    "            if self.experiment is None:\n",
    "                self.experiment_id = self.client.create_experiment(\n",
    "                    self.experiment_name,\n",
    "                    artifact_location=str(MLFLOW_DIR / \"artifacts\"),\n",
    "                    tags={\n",
    "                        \"project\": \"F1 Prize Picks\",\n",
    "                        \"team\": \"Data Science\",\n",
    "                        \"version\": \"1.0\"\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                self.experiment_id = self.experiment.experiment_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up experiment: {e}\")\n",
    "            self.experiment_id = \"0\"\n",
    "    \n",
    "    def start_run(self, run_name=None, tags=None):\n",
    "        \"\"\"Start a new MLflow run\"\"\"\n",
    "        if run_name is None:\n",
    "            run_name = f\"f1_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        mlflow.start_run(run_name=run_name)\n",
    "        \n",
    "        if tags:\n",
    "            mlflow.set_tags(tags)\n",
    "            \n",
    "        return mlflow.active_run().info.run_id\n",
    "    \n",
    "    def log_model_training(self, model, X_train, y_train, X_val, y_val, \n",
    "                          model_type=\"random_forest\", feature_names=None):\n",
    "        \"\"\"Log model training metrics and artifacts\"\"\"\n",
    "        # Log parameters\n",
    "        params = model.get_params()\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log dataset info\n",
    "        mlflow.log_param(\"train_samples\", len(X_train))\n",
    "        mlflow.log_param(\"val_samples\", len(X_val))\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Calculate and log metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        train_pred = model.predict(X_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        \n",
    "        metrics = {\n",
    "            \"train_accuracy\": accuracy_score(y_train, train_pred),\n",
    "            \"val_accuracy\": accuracy_score(y_val, val_pred),\n",
    "            \"train_precision\": precision_score(y_train, train_pred, average='weighted'),\n",
    "            \"val_precision\": precision_score(y_val, val_pred, average='weighted'),\n",
    "            \"train_recall\": recall_score(y_train, train_pred, average='weighted'),\n",
    "            \"val_recall\": recall_score(y_val, val_pred, average='weighted'),\n",
    "            \"train_f1\": f1_score(y_train, train_pred, average='weighted'),\n",
    "            \"val_f1\": f1_score(y_val, val_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log feature importance if available\n",
    "        if hasattr(model, 'feature_importances_') and feature_names:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Save as artifact\n",
    "            importance_path = \"/tmp/feature_importance.csv\"\n",
    "            importance_df.to_csv(importance_path, index=False)\n",
    "            mlflow.log_artifact(importance_path)\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train, train_pred)\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            f\"{model_type}_model\",\n",
    "            signature=signature\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def log_prediction_results(self, predictions_df, actual_results=None):\n",
    "        \"\"\"Log prediction results and performance\"\"\"\n",
    "        # Log prediction statistics\n",
    "        mlflow.log_metric(\"n_predictions\", len(predictions_df))\n",
    "        \n",
    "        if 'probability' in predictions_df.columns:\n",
    "            mlflow.log_metric(\"avg_confidence\", predictions_df['probability'].mean())\n",
    "            mlflow.log_metric(\"std_confidence\", predictions_df['probability'].std())\n",
    "        \n",
    "        # If we have actual results, calculate performance\n",
    "        if actual_results is not None:\n",
    "            merged = predictions_df.merge(actual_results, on=['race_id', 'driver_id'])\n",
    "            accuracy = (merged['predicted'] == merged['actual']).mean()\n",
    "            mlflow.log_metric(\"prediction_accuracy\", accuracy)\n",
    "        \n",
    "        # Save predictions as artifact\n",
    "        predictions_path = \"/tmp/predictions.csv\"\n",
    "        predictions_df.to_csv(predictions_path, index=False)\n",
    "        mlflow.log_artifact(predictions_path)\n",
    "    \n",
    "    def log_betting_performance(self, bets_df, bankroll_history):\n",
    "        \"\"\"Log betting performance metrics\"\"\"\n",
    "        # Calculate key metrics\n",
    "        total_bets = len(bets_df)\n",
    "        winning_bets = (bets_df['payout'] > 0).sum()\n",
    "        total_wagered = bets_df['stake'].sum()\n",
    "        total_payout = bets_df['payout'].sum()\n",
    "        roi = (total_payout - total_wagered) / total_wagered if total_wagered > 0 else 0\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"total_bets\": total_bets,\n",
    "            \"winning_bets\": winning_bets,\n",
    "            \"win_rate\": winning_bets / total_bets if total_bets > 0 else 0,\n",
    "            \"total_wagered\": total_wagered,\n",
    "            \"total_payout\": total_payout,\n",
    "            \"roi\": roi,\n",
    "            \"final_bankroll\": bankroll_history[-1] if bankroll_history else 0\n",
    "        })\n",
    "        \n",
    "        # Log Sharpe ratio if we have enough data\n",
    "        if len(bankroll_history) > 10:\n",
    "            returns = pd.Series(bankroll_history).pct_change().dropna()\n",
    "            sharpe = returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0\n",
    "            mlflow.log_metric(\"sharpe_ratio\", sharpe)\n",
    "        \n",
    "        # Save betting history\n",
    "        bets_path = \"/tmp/betting_history.csv\"\n",
    "        bets_df.to_csv(bets_path, index=False)\n",
    "        mlflow.log_artifact(bets_path)\n",
    "        \n",
    "        # Save bankroll evolution\n",
    "        bankroll_df = pd.DataFrame({\n",
    "            'period': range(len(bankroll_history)),\n",
    "            'bankroll': bankroll_history\n",
    "        })\n",
    "        bankroll_path = \"/tmp/bankroll_history.csv\"\n",
    "        bankroll_df.to_csv(bankroll_path, index=False)\n",
    "        mlflow.log_artifact(bankroll_path)\n",
    "    \n",
    "    def end_run(self):\n",
    "        \"\"\"End the current MLflow run\"\"\"\n",
    "        mlflow.end_run()\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = F1MLflowTracker()\n",
    "print(f\"Experiment ID: {tracker.experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Model Wrapper for Prize Picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrizePicksMLflowModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Custom MLflow model for Prize Picks predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict, feature_columns, optimizer_config):\n",
    "        self.models_dict = models_dict\n",
    "        self.feature_columns = feature_columns\n",
    "        self.optimizer_config = optimizer_config\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Generate Prize Picks recommendations\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for prop_type, model in self.models_dict.items():\n",
    "            if prop_type in model_input.columns:\n",
    "                # Get features for this prop type\n",
    "                features = model_input[self.feature_columns[prop_type]]\n",
    "                \n",
    "                # Make predictions\n",
    "                probs = model.predict_proba(features)[:, 1]\n",
    "                \n",
    "                # Apply optimizer logic\n",
    "                threshold = self.optimizer_config.get('threshold', 0.55)\n",
    "                high_confidence = probs > threshold\n",
    "                \n",
    "                prop_predictions = pd.DataFrame({\n",
    "                    'prop_type': prop_type,\n",
    "                    'probability': probs,\n",
    "                    'recommended': high_confidence,\n",
    "                    'kelly_fraction': self._calculate_kelly(probs)\n",
    "                })\n",
    "                \n",
    "                predictions.append(prop_predictions)\n",
    "        \n",
    "        return pd.concat(predictions, ignore_index=True)\n",
    "    \n",
    "    def _calculate_kelly(self, probs):\n",
    "        \"\"\"Calculate Kelly fraction for bet sizing\"\"\"\n",
    "        # Simplified Kelly calculation\n",
    "        odds = 2.0  # Assume even money for Prize Picks\n",
    "        edge = probs * odds - 1\n",
    "        kelly = np.where(edge > 0, edge / (odds - 1), 0)\n",
    "        return np.clip(kelly * 0.25, 0, 0.1)  # Conservative Kelly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Tracking Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_full_pipeline_run(pipeline_config, results):\n",
    "    \"\"\"Track a complete pipeline run in MLflow\"\"\"\n",
    "    \n",
    "    # Start run\n",
    "    run_id = tracker.start_run(\n",
    "        run_name=f\"pipeline_{pipeline_config.get('race_id', 'test')}\",\n",
    "        tags={\n",
    "            \"pipeline_version\": pipeline_config.get('version', '1.0'),\n",
    "            \"environment\": pipeline_config.get('environment', 'development'),\n",
    "            \"race_id\": str(pipeline_config.get('race_id', 'N/A'))\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Log configuration\n",
    "        mlflow.log_params({\n",
    "            \"optimizer_strategy\": pipeline_config.get('optimizer_strategy', 'conservative'),\n",
    "            \"kelly_fraction\": pipeline_config.get('kelly_fraction', 0.25),\n",
    "            \"min_edge\": pipeline_config.get('min_edge', 0.05),\n",
    "            \"max_picks_per_parlay\": pipeline_config.get('max_picks_per_parlay', 6),\n",
    "            \"bankroll\": pipeline_config.get('bankroll', 100)\n",
    "        })\n",
    "        \n",
    "        # Log results\n",
    "        if 'predictions' in results:\n",
    "            tracker.log_prediction_results(results['predictions'])\n",
    "        \n",
    "        if 'betting_results' in results:\n",
    "            tracker.log_betting_performance(\n",
    "                results['betting_results'],\n",
    "                results.get('bankroll_history', [])\n",
    "            )\n",
    "        \n",
    "        # Log performance metrics\n",
    "        if 'metrics' in results:\n",
    "            mlflow.log_metrics(results['metrics'])\n",
    "        \n",
    "        # Save pipeline artifacts\n",
    "        if 'report' in results:\n",
    "            report_path = \"/tmp/pipeline_report.json\"\n",
    "            with open(report_path, 'w') as f:\n",
    "                json.dump(results['report'], f, indent=2)\n",
    "            mlflow.log_artifact(report_path)\n",
    "        \n",
    "        print(f\"Run logged successfully: {run_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error logging run: {e}\")\n",
    "        mlflow.set_tag(\"error\", str(e))\n",
    "    finally:\n",
    "        tracker.end_run()\n",
    "    \n",
    "    return run_id\n",
    "\n",
    "# Example usage\n",
    "example_config = {\n",
    "    'race_id': 'test_race_2024',\n",
    "    'version': '1.0',\n",
    "    'optimizer_strategy': 'moderate',\n",
    "    'kelly_fraction': 0.25,\n",
    "    'bankroll': 1000\n",
    "}\n",
    "\n",
    "example_results = {\n",
    "    'predictions': pd.DataFrame({\n",
    "        'driver_id': [1, 2, 3],\n",
    "        'prop_type': ['winner', 'podium', 'points'],\n",
    "        'probability': [0.65, 0.72, 0.58]\n",
    "    }),\n",
    "    'metrics': {\n",
    "        'avg_confidence': 0.65,\n",
    "        'n_high_value_picks': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Track example run\n",
    "# run_id = track_full_pipeline_run(example_config, example_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Registry and Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ModelRegistry:\n",
    "    \"\"\"Manage model versions in MLflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = MlflowClient()\n",
    "    \n",
    "    def register_model(self, run_id, model_name, model_type=\"winner_prediction\"):\n",
    "        \"\"\"Register a model from a run\"\"\"\n",
    "        model_uri = f\"runs:/{run_id}/{model_type}_model\"\n",
    "        \n",
    "        try:\n",
    "            # Register the model\n",
    "            mv = mlflow.register_model(model_uri, model_name)\n",
    "            \n",
    "            # Add description and tags\n",
    "            self.client.update_model_version(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                description=f\"F1 {model_type} model trained on {datetime.now().date()}\"\n",
    "            )\n",
    "            \n",
    "            self.client.set_model_version_tag(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                key=\"model_type\",\n",
    "                value=model_type\n",
    "            )\n",
    "            \n",
    "            print(f\"Model registered: {model_name} v{mv.version}\")\n",
    "            return mv\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error registering model: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def promote_model(self, model_name, version, stage=\"Production\"):\n",
    "        \"\"\"Promote a model version to a stage\"\"\"\n",
    "        try:\n",
    "            self.client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=version,\n",
    "                stage=stage,\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(f\"Model {model_name} v{version} promoted to {stage}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error promoting model: {e}\")\n",
    "    \n",
    "    def get_latest_model(self, model_name, stage=\"Production\"):\n",
    "        \"\"\"Get the latest model version for a stage\"\"\"\n",
    "        try:\n",
    "            versions = self.client.get_latest_versions(model_name, stages=[stage])\n",
    "            if versions:\n",
    "                return versions[0]\n",
    "            else:\n",
    "                print(f\"No {stage} version found for {model_name}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting model: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_production_model(self, model_name):\n",
    "        \"\"\"Load the production version of a model\"\"\"\n",
    "        model_version = self.get_latest_model(model_name, \"Production\")\n",
    "        if model_version:\n",
    "            model_uri = f\"models:/{model_name}/{model_version.version}\"\n",
    "            return mlflow.sklearn.load_model(model_uri)\n",
    "        return None\n",
    "\n",
    "# Initialize registry\n",
    "registry = F1ModelRegistry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_experiments(experiment_name=\"f1_prize_picks\", metric=\"val_accuracy\"):\n",
    "    \"\"\"Compare runs within an experiment\"\"\"\n",
    "    \n",
    "    # Get experiment\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        print(f\"Experiment {experiment_name} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Search runs\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{metric} DESC\"],\n",
    "        max_results=20\n",
    "    )\n",
    "    \n",
    "    if runs.empty:\n",
    "        print(\"No runs found\")\n",
    "        return None\n",
    "    \n",
    "    # Display key metrics\n",
    "    display_cols = [\n",
    "        'run_id', 'start_time', f'metrics.{metric}',\n",
    "        'metrics.val_f1', 'metrics.roi', 'metrics.sharpe_ratio',\n",
    "        'params.optimizer_strategy', 'params.kelly_fraction'\n",
    "    ]\n",
    "    \n",
    "    # Filter existing columns\n",
    "    display_cols = [col for col in display_cols if col in runs.columns]\n",
    "    \n",
    "    comparison_df = runs[display_cols].head(10)\n",
    "    \n",
    "    # Plot performance\n",
    "    if f'metrics.{metric}' in runs.columns:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(runs['start_time'], runs[f'metrics.{metric}'], 'o-')\n",
    "        plt.xlabel('Run Time')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} Over Time')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare experiments\n",
    "# comparison = compare_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlflow_callback():\n",
    "    \"\"\"Create callback for pipeline integration\"\"\"\n",
    "    \n",
    "    def mlflow_callback(stage, data):\n",
    "        \"\"\"Callback to log pipeline stages to MLflow\"\"\"\n",
    "        \n",
    "        if stage == \"data_loaded\":\n",
    "            mlflow.log_metric(\"n_races\", len(data.get('races', [])))\n",
    "            mlflow.log_metric(\"n_drivers\", len(data.get('drivers', [])))\n",
    "            \n",
    "        elif stage == \"features_computed\":\n",
    "            features_df = data.get('features')\n",
    "            if features_df is not None:\n",
    "                mlflow.log_metric(\"n_features\", features_df.shape[1])\n",
    "                mlflow.log_metric(\"n_samples\", features_df.shape[0])\n",
    "                \n",
    "        elif stage == \"predictions_made\":\n",
    "            predictions = data.get('predictions')\n",
    "            if predictions is not None:\n",
    "                mlflow.log_metric(\"n_predictions\", len(predictions))\n",
    "                if 'probability' in predictions.columns:\n",
    "                    mlflow.log_metric(\"avg_confidence\", predictions['probability'].mean())\n",
    "                    \n",
    "        elif stage == \"optimization_complete\":\n",
    "            picks = data.get('optimized_picks')\n",
    "            if picks:\n",
    "                mlflow.log_metric(\"n_recommended_picks\", len(picks))\n",
    "                mlflow.log_metric(\"total_expected_value\", \n",
    "                                sum(p.get('expected_value', 0) for p in picks))\n",
    "                \n",
    "        elif stage == \"pipeline_complete\":\n",
    "            # Log final artifacts\n",
    "            if 'report_path' in data:\n",
    "                mlflow.log_artifact(data['report_path'])\n",
    "            \n",
    "            # Log summary metrics\n",
    "            summary = data.get('summary', {})\n",
    "            for key, value in summary.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    mlflow.log_metric(f\"summary_{key}\", value)\n",
    "    \n",
    "    return mlflow_callback\n",
    "\n",
    "# Example: Integrate with pipeline\n",
    "print(\"MLflow callback created for pipeline integration\")\n",
    "print(\"Use: pipeline.add_callback(create_mlflow_callback())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MLflow UI Launch Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create launch script for MLflow UI\n",
    "launch_script = \"\"\"#!/bin/bash\n",
    "# Launch MLflow UI for F1 Prize Picks experiments\n",
    "\n",
    "echo \"Starting MLflow UI...\"\n",
    "echo \"Access at: http://localhost:5000\"\n",
    "echo \"Press Ctrl+C to stop\"\n",
    "\n",
    "cd /app/notebooks/advanced\n",
    "mlflow ui --backend-store-uri file:///app/notebooks/advanced/mlflow --port 5000\n",
    "\"\"\"\n",
    "\n",
    "with open('/app/notebooks/advanced/mlflow/launch_ui.sh', 'w') as f:\n",
    "    f.write(launch_script)\n",
    "\n",
    "import os\n",
    "os.chmod('/app/notebooks/advanced/mlflow/launch_ui.sh', 0o755)\n",
    "\n",
    "print(\"MLflow UI launch script created at:\")\n",
    "print(\"/app/notebooks/advanced/mlflow/launch_ui.sh\")\n",
    "print(\"\\nTo start MLflow UI, run:\")\n",
    "print(\"bash /app/notebooks/advanced/mlflow/launch_ui.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has set up comprehensive MLflow tracking for the F1 Prize Picks pipeline:\n",
    "\n",
    "1. **Experiment Tracking**: Automated logging of model training, predictions, and betting performance\n",
    "2. **Model Registry**: Version control and lifecycle management for models\n",
    "3. **Custom Model Wrapper**: Prize Picks specific model implementation\n",
    "4. **Pipeline Integration**: Callbacks for seamless tracking during pipeline execution\n",
    "5. **Experiment Comparison**: Tools to analyze and compare different runs\n",
    "6. **MLflow UI**: Launch script for visual experiment tracking\n",
    "\n",
    "### Usage in Pipeline:\n",
    "\n",
    "```python\n",
    "# In your pipeline code\n",
    "from F1_MLflow_Tracking import F1MLflowTracker, create_mlflow_callback\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = F1MLflowTracker()\n",
    "\n",
    "# Add callback to pipeline\n",
    "pipeline.add_callback(create_mlflow_callback())\n",
    "\n",
    "# Track run\n",
    "run_id = tracker.start_run()\n",
    "# ... run pipeline ...\n",
    "tracker.end_run()\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Integrate MLflow tracking into the main pipeline\n",
    "2. Set up automated model promotion based on performance\n",
    "3. Create dashboards for monitoring betting performance\n",
    "4. Implement A/B testing for different strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}