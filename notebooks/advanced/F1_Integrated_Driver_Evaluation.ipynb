{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Integrated Driver Evaluation System\n",
    "\n",
    "This notebook integrates the driver evaluation system with the fixed prediction models to create a unified system for Prize Picks optimization.\n",
    "\n",
    "## Key Integration Points:\n",
    "1. **Age-Performance Curves**: Adjust predictions based on driver development phase\n",
    "2. **Constructor Compatibility**: Factor team dynamics into race predictions\n",
    "3. **Consistency Metrics**: Weight predictions by driver reliability\n",
    "4. **Long-term Value**: Identify betting opportunities based on driver trajectories"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Define module exports - this will be populated after the class is defined\n__all__ = []",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\n# plt.style.use('seaborn-darkgrid') # Original style - may not work on all systems\n# Safe plotting style setup\ntry:\n    import seaborn as sns\n    sns.set_theme()  # Modern seaborn initialization\nexcept:\n    try:\n        plt.style.use('ggplot')  # Fallback style\n    except:\n        pass  # Use default style\nsns.set_palette('husl')"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading F1 data with enhanced loader...\n",
      "Already have latest F1DB data (version v2025.12.0)\n",
      "Already have latest F1DB data (version v2025.12.0)\n",
      "Loading 14 CSV files...\n",
      "  ✓ Loaded circuits: 77 rows\n",
      "  ✓ Loaded constructors: 212 rows\n",
      "  ✓ Loaded constructor_results: 12445 rows\n",
      "  ✓ Loaded constructor_standings: 13211 rows\n",
      "  ✓ Loaded drivers: 859 rows\n",
      "  ✓ Loaded driver_standings: 34469 rows\n",
      "  ✓ Loaded lap_times: 568576 rows\n",
      "  ✓ Loaded pit_stops: 10766 rows\n",
      "  ✓ Loaded qualifying: 10134 rows\n",
      "  ✓ Loaded races: 1125 rows\n",
      "  ✓ Loaded results: 26399 rows\n",
      "  ✓ Loaded seasons: 75 rows\n",
      "  ✓ Loaded sprint_results: 280 rows\n",
      "  ✓ Loaded status: 139 rows\n",
      "  → Mapped results to constructor_results\n",
      "\n",
      "Loaded 14 datasets\n",
      "Results shape: (26399, 18)\n"
     ]
    }
   ],
   "source": [
    "# Load data using enhanced loader\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from f1db_data_loader import load_f1db_data, F1DBDataLoader\n",
    "\n",
    "print(\"Loading F1 data with enhanced loader...\")\n",
    "f1_data = load_f1db_data(data_dir='../../data')\n",
    "\n",
    "# Initialize enhanced loader for additional functionality\n",
    "loader = F1DBDataLoader('../../data/f1db')\n",
    "\n",
    "# Get core datasets\n",
    "results = f1_data.get('results', pd.DataFrame())\n",
    "races = f1_data.get('races', pd.DataFrame())\n",
    "drivers = f1_data.get('drivers', pd.DataFrame())\n",
    "constructors = f1_data.get('constructors', pd.DataFrame())\n",
    "qualifying = f1_data.get('qualifying', pd.DataFrame())\n",
    "driver_standings = f1_data.get('driver_standings', pd.DataFrame())\n",
    "constructor_standings = f1_data.get('constructor_standings', pd.DataFrame())\n",
    "\n",
    "print(f\"\\nLoaded {len(f1_data)} datasets\")\n",
    "print(f\"Results shape: {results.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Integrated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (26399, 29)\n",
      "Date range: 1950-05-13 00:00:00 to 2024-05-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Merge core data with driver information\n",
    "df = results.merge(races[['raceId', 'year', 'round', 'circuitId', 'date']], on='raceId')\n",
    "df = df.merge(drivers[['driverId', 'driverRef', 'surname', 'dob', 'nationality']], on='driverId')\n",
    "df = df.merge(constructors[['constructorId', 'constructorRef', 'name']], \n",
    "              on='constructorId', suffixes=('_race', '_constructor'))\n",
    "\n",
    "# Calculate driver age at race time\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "df['driver_age'] = (df['date'] - df['dob']).dt.days / 365.25\n",
    "\n",
    "# Sort chronologically\n",
    "df = df.sort_values(['date', 'raceId'])\n",
    "\n",
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Driver Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated metrics for 34 drivers\n",
      "\n",
      "Top 10 drivers by average points:\n",
      "       surname  avg_points  consistency_score  current_age\n",
      "14  Verstappen       17.85               0.15        26.60\n",
      "2     Hamilton       14.82               0.18        39.32\n",
      "9      Leclerc        9.85               0.14        26.55\n",
      "18       Pérez        9.34               0.15        34.27\n",
      "0        Sainz        7.76               0.15        29.68\n",
      "3       Bottas        7.50               0.14        34.69\n",
      "1       Norris        6.24               0.17        24.48\n",
      "33     Bearman        6.00               1.00        18.84\n",
      "24      Alonso        5.46               0.17        42.77\n",
      "8       Vettel        4.41               0.16        35.38\n"
     ]
    }
   ],
   "source": [
    "def calculate_enhanced_driver_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive driver metrics including evaluation scores\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Focus on recent seasons for relevance\n",
    "    recent_df = df[df['year'] >= 2019].copy()\n",
    "    \n",
    "    for driver_id in recent_df['driverId'].unique():\n",
    "        driver_data = recent_df[recent_df['driverId'] == driver_id]\n",
    "        \n",
    "        # Basic information\n",
    "        driver_info = {\n",
    "            'driverId': driver_id,\n",
    "            'driverRef': driver_data['driverRef'].iloc[0],\n",
    "            'surname': driver_data['surname'].iloc[0],\n",
    "            'current_age': driver_data['driver_age'].iloc[-1] if len(driver_data) > 0 else 0,\n",
    "            'races_completed': len(driver_data),\n",
    "            'years_active': driver_data['year'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Performance metrics\n",
    "        driver_info['avg_position'] = driver_data['positionOrder'].mean()\n",
    "        driver_info['avg_points'] = driver_data['points'].mean()\n",
    "        driver_info['total_points'] = driver_data['points'].sum()\n",
    "        driver_info['podium_rate'] = (driver_data['positionOrder'] <= 3).mean()\n",
    "        driver_info['top10_rate'] = (driver_data['positionOrder'] <= 10).mean()\n",
    "        driver_info['dnf_rate'] = (driver_data['statusId'] > 1).mean()\n",
    "        driver_info['wins'] = (driver_data['positionOrder'] == 1).sum()\n",
    "        driver_info['win_rate'] = (driver_data['positionOrder'] == 1).mean()\n",
    "        \n",
    "        # Consistency metrics\n",
    "        driver_info['position_std'] = driver_data['positionOrder'].std()\n",
    "        driver_info['points_std'] = driver_data['points'].std()\n",
    "        driver_info['consistency_score'] = 1 / (1 + driver_info['position_std']) if driver_info['position_std'] > 0 else 1\n",
    "        \n",
    "        # Recent form (last 10 races)\n",
    "        recent_races = driver_data.sort_values('date').tail(10)\n",
    "        if len(recent_races) > 0:\n",
    "            driver_info['recent_avg_position'] = recent_races['positionOrder'].mean()\n",
    "            driver_info['recent_avg_points'] = recent_races['points'].mean()\n",
    "            driver_info['recent_form_trend'] = driver_info['avg_position'] - driver_info['recent_avg_position']\n",
    "        else:\n",
    "            driver_info['recent_avg_position'] = driver_info['avg_position']\n",
    "            driver_info['recent_avg_points'] = driver_info['avg_points']\n",
    "            driver_info['recent_form_trend'] = 0\n",
    "        \n",
    "        # Track diversity\n",
    "        driver_info['track_diversity'] = driver_data['circuitId'].nunique()\n",
    "        driver_info['avg_tracks_per_season'] = driver_info['track_diversity'] / max(1, driver_info['years_active'])\n",
    "        \n",
    "        # Constructor performance\n",
    "        current_constructor = driver_data['constructorId'].iloc[-1] if len(driver_data) > 0 else None\n",
    "        driver_info['current_constructor'] = current_constructor\n",
    "        driver_info['constructor_changes'] = driver_data['constructorId'].nunique()\n",
    "        \n",
    "        metrics.append(driver_info)\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Calculate driver metrics\n",
    "driver_metrics = calculate_enhanced_driver_metrics(df)\n",
    "print(f\"\\nCalculated metrics for {len(driver_metrics)} drivers\")\n",
    "print(\"\\nTop 10 drivers by average points:\")\n",
    "print(driver_metrics.nlargest(10, 'avg_points')[['surname', 'avg_points', 'consistency_score', 'current_age']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Age-Performance Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_age_performance_model(df, driver_metrics):\n    \"\"\"\n    Model the relationship between age and performance\n    \"\"\"\n    # Create age bins for analysis\n    age_bins = [18, 22, 25, 28, 32, 36, 45]\n    age_labels = ['18-22', '23-25', '26-28', '29-32', '33-36', '37+']\n    \n    df['age_group'] = pd.cut(df['driver_age'], bins=age_bins, labels=age_labels, include_lowest=True)\n    \n    # Calculate performance by age group\n    age_performance = df.groupby('age_group').agg({\n        'positionOrder': ['mean', 'std'],\n        'points': ['mean', 'std'],\n        'driverId': 'nunique'\n    }).round(2)\n    \n    # Peak performance window\n    peak_age_range = (26, 32)\n    \n    # Calculate development scores\n    development_scores = []\n    \n    for _, driver in driver_metrics.iterrows():\n        age = driver['current_age']\n        \n        # Development phase\n        if age < peak_age_range[0]:\n            development_phase = 'pre-peak'\n            years_to_peak = peak_age_range[0] - age\n            # Young drivers have high potential\n            age_factor = 1.1 + (0.05 * min(years_to_peak, 4))\n        elif age <= peak_age_range[1]:\n            development_phase = 'peak'\n            years_to_peak = 0\n            age_factor = 1.2  # Peak performance\n        else:\n            development_phase = 'post-peak'\n            years_past_peak = age - peak_age_range[1]\n            # Gradual decline\n            age_factor = 1.0 - (0.03 * min(years_past_peak, 10))\n        \n        # Experience bonus\n        experience_factor = min(1.2, 1 + (driver['races_completed'] / 100))\n        \n        # Combined development score\n        development_score = age_factor * experience_factor * driver['consistency_score']\n        \n        development_scores.append({\n            'driverId': driver['driverId'],\n            'surname': driver['surname'],\n            'current_age': age,\n            'development_phase': development_phase,\n            'age_factor': age_factor,\n            'experience_factor': experience_factor,\n            'development_score': development_score\n        })\n    \n    development_df = pd.DataFrame(development_scores)\n    \n    # Visualize age-performance relationship\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Age distribution with performance\n    scatter = ax1.scatter(development_df['current_age'], \n                         development_df['development_score'],\n                         c=development_df['age_factor'], \n                         cmap='RdYlGn', s=100, alpha=0.7)\n    \n    # Add peak performance zone\n    ax1.axvspan(peak_age_range[0], peak_age_range[1], alpha=0.2, color='green', label='Peak Years')\n    \n    ax1.set_xlabel('Current Age')\n    ax1.set_ylabel('Development Score')\n    ax1.set_title('Driver Development Scores by Age')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Add colorbar\n    cbar = plt.colorbar(scatter, ax=ax1)\n    cbar.set_label('Age Factor')\n    \n    # Age group performance\n    age_perf_mean = age_performance['points']['mean']\n    ax2.bar(range(len(age_perf_mean)), age_perf_mean)\n    ax2.set_xticks(range(len(age_perf_mean)))\n    ax2.set_xticklabels(age_labels)\n    ax2.set_xlabel('Age Group')\n    ax2.set_ylabel('Average Points per Race')\n    ax2.set_title('Performance by Age Group')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return development_df, age_performance\n\n# Create age-performance model\ndevelopment_df, age_performance = create_age_performance_model(df, driver_metrics)\nprint(\"\\nAge Group Performance Summary:\")\nprint(age_performance)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Constructor Compatibility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 drivers by constructor compatibility:\n",
      "       surname  compatibility_score  team_performance_match  races_together\n",
      "13  Giovinazzi                1.000                   0.995              60\n",
      "16    Grosjean                1.000                   0.992              98\n",
      "17   Magnussen                1.000                   0.984             129\n",
      "25  Schumacher                1.000                   0.991              44\n",
      "12   Räikkönen                0.998                   0.945              58\n",
      "19      Stroll                0.998                   0.939              72\n",
      "31    Sargeant                0.971                   0.904              27\n",
      "10       Kvyat                0.969                   0.918              17\n",
      "15       Gasly                0.963                   0.852              28\n",
      "26     Mazepin                0.963                   0.965              22\n"
     ]
    }
   ],
   "source": [
    "def calculate_constructor_compatibility(df, driver_metrics):\n",
    "    \"\"\"\n",
    "    Calculate how well drivers fit with their current constructors\n",
    "    \"\"\"\n",
    "    compatibility_scores = []\n",
    "    \n",
    "    # Calculate constructor profiles\n",
    "    constructor_profiles = df.groupby('constructorId').agg({\n",
    "        'points': ['mean', 'sum'],\n",
    "        'positionOrder': 'mean',\n",
    "        'statusId': lambda x: (x > 1).mean()  # DNF rate\n",
    "    }).round(3)\n",
    "    \n",
    "    for _, driver in driver_metrics.iterrows():\n",
    "        driver_id = driver['driverId']\n",
    "        constructor_id = driver['current_constructor']\n",
    "        \n",
    "        if pd.isna(constructor_id) or constructor_id not in constructor_profiles.index:\n",
    "            compatibility_scores.append({\n",
    "                'driverId': driver_id,\n",
    "                'surname': driver['surname'],\n",
    "                'compatibility_score': 0.5,  # Neutral if no data\n",
    "                'team_performance_match': 0.5,\n",
    "                'reliability_match': 0.5\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get constructor profile\n",
    "        constructor_avg_points = constructor_profiles.loc[constructor_id, ('points', 'mean')]\n",
    "        constructor_avg_position = constructor_profiles.loc[constructor_id, ('positionOrder', 'mean')]\n",
    "        constructor_dnf_rate = constructor_profiles.loc[constructor_id, ('statusId', '<lambda>')]\n",
    "        \n",
    "        # Performance match (closer is better)\n",
    "        points_diff = abs(driver['avg_points'] - constructor_avg_points / 2)  # Assume 2 drivers\n",
    "        performance_match = 1 / (1 + points_diff / 10)\n",
    "        \n",
    "        # Reliability match\n",
    "        reliability_match = 1 - abs(driver['dnf_rate'] - constructor_dnf_rate)\n",
    "        \n",
    "        # Experience with constructor\n",
    "        driver_constructor_races = df[(df['driverId'] == driver_id) & \n",
    "                                     (df['constructorId'] == constructor_id)]\n",
    "        races_together = len(driver_constructor_races)\n",
    "        experience_bonus = min(0.2, races_together / 100)  # Max 20% bonus\n",
    "        \n",
    "        # Overall compatibility\n",
    "        compatibility = (\n",
    "            0.5 * performance_match +\n",
    "            0.3 * reliability_match +\n",
    "            0.2 * (1 + experience_bonus)\n",
    "        )\n",
    "        \n",
    "        compatibility_scores.append({\n",
    "            'driverId': driver_id,\n",
    "            'surname': driver['surname'],\n",
    "            'constructorId': constructor_id,\n",
    "            'compatibility_score': min(1.0, compatibility),\n",
    "            'team_performance_match': performance_match,\n",
    "            'reliability_match': reliability_match,\n",
    "            'races_together': races_together\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(compatibility_scores)\n",
    "\n",
    "# Calculate compatibility scores\n",
    "compatibility_df = calculate_constructor_compatibility(df, driver_metrics)\n",
    "print(\"\\nTop 10 drivers by constructor compatibility:\")\n",
    "print(compatibility_df.nlargest(10, 'compatibility_score')[[\n",
    "    'surname', 'compatibility_score', 'team_performance_match', 'races_together'\n",
    "]].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrated Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class IntegratedF1Predictor:\n    \"\"\"\n    Combines fixed prediction models with driver evaluation metrics\n    \"\"\"\n    def __init__(self, base_model_path=None):\n        self.base_model = None\n        self.scaler = None\n        self.feature_columns = None\n        \n        if base_model_path:\n            self.load_base_model(base_model_path)\n    \n    def load_base_model(self, model_path):\n        \"\"\"Load the fixed prediction model\"\"\"\n        try:\n            model_artifacts = joblib.load(model_path)\n            self.base_model = model_artifacts['model']\n            self.scaler = model_artifacts['scaler']\n            self.feature_columns = model_artifacts['feature_columns']\n            print(f\"Loaded model: {model_artifacts['model_name']}\")\n        except FileNotFoundError:\n            print(\"Base model not found. Training new model...\")\n            self._train_default_model()\n    \n    def _train_default_model(self):\n        \"\"\"Train a default model if no saved model exists\"\"\"\n        # Simplified training for demonstration\n        self.base_model = RandomForestClassifier(\n            n_estimators=100,\n            max_depth=8,\n            min_samples_split=50,\n            random_state=42\n        )\n        self.scaler = StandardScaler()\n        print(\"Initialized default model\")\n    \n    def predict_with_evaluation(self, race_features, driver_evaluation, compatibility):\n        \"\"\"\n        Make predictions incorporating driver evaluation metrics\n        \"\"\"\n        predictions = []\n        \n        for idx, features in race_features.iterrows():\n            driver_id = features.get('driverId')\n            \n            # Get base prediction (if model is loaded and trained)\n            base_prob = 0.5  # Default probability\n            \n            if self.base_model is not None and self.feature_columns is not None and self.scaler is not None:\n                try:\n                    # Check if model is fitted\n                    if hasattr(self.base_model, 'n_features_in_'):\n                        X = features[self.feature_columns].values.reshape(1, -1)\n                        X_scaled = self.scaler.transform(X)\n                        base_prob = self.base_model.predict_proba(X_scaled)[0, 1]\n                except:\n                    base_prob = 0.5  # Default if prediction fails\n            \n            # Get driver evaluation metrics\n            driver_eval = driver_evaluation[driver_evaluation['driverId'] == driver_id]\n            if driver_eval.empty:\n                age_factor = 1.0\n                development_score = 1.0\n            else:\n                age_factor = driver_eval.iloc[0]['age_factor']\n                development_score = driver_eval.iloc[0]['development_score']\n            \n            # Get compatibility score\n            driver_compat = compatibility[compatibility['driverId'] == driver_id]\n            if driver_compat.empty:\n                compat_score = 0.5\n            else:\n                compat_score = driver_compat.iloc[0]['compatibility_score']\n            \n            # Adjust prediction based on evaluation metrics\n            # Weight: 60% base model, 20% age factor, 20% compatibility\n            adjusted_prob = (\n                0.6 * base_prob +\n                0.2 * (base_prob * age_factor) +\n                0.2 * (base_prob * compat_score)\n            )\n            \n            # Ensure probability is in valid range\n            adjusted_prob = np.clip(adjusted_prob, 0.01, 0.99)\n            \n            predictions.append({\n                'driverId': driver_id,\n                'base_probability': base_prob,\n                'age_factor': age_factor,\n                'compatibility_score': compat_score,\n                'development_score': development_score,\n                'adjusted_probability': adjusted_prob,\n                'confidence': self._calculate_confidence(base_prob, age_factor, compat_score)\n            })\n        \n        return pd.DataFrame(predictions)\n    \n    def _calculate_confidence(self, base_prob, age_factor, compat_score):\n        \"\"\"Calculate prediction confidence\"\"\"\n        # Higher confidence when all factors align\n        factor_variance = np.std([base_prob, age_factor, compat_score])\n        confidence = 1 - (2 * factor_variance)  # Lower variance = higher confidence\n        return np.clip(confidence, 0.1, 0.9)\n\n# Initialize integrated predictor\npredictor = IntegratedF1Predictor()\n\n# Try to load the fixed model from Phase 1\ntry:\n    predictor.load_base_model('f1_model_fixed_top10.pkl')\nexcept:\n    print(\"Using default model configuration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prize Picks Value Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def identify_prize_picks_value(predictions, driver_metrics, development_df):\n    \"\"\"\n    Identify high-value Prize Picks opportunities based on integrated analysis\n    \"\"\"\n    # Merge all driver information\n    value_analysis = predictions.merge(\n        driver_metrics[['driverId', 'surname', 'avg_points', 'consistency_score', 'recent_form_trend']], \n        on='driverId'\n    )\n    value_analysis = value_analysis.merge(\n        development_df[['driverId', 'development_phase']], \n        on='driverId'\n    )\n    \n    # Calculate value scores\n    value_analysis['value_score'] = (\n        value_analysis['adjusted_probability'] * \n        value_analysis['consistency_score'] * \n        value_analysis['confidence']\n    )\n    \n    # Categorize opportunities\n    value_analysis['opportunity_type'] = 'Standard'\n    \n    # Young talent (pre-peak with good form)\n    young_talent = (\n        (value_analysis['development_phase'] == 'pre-peak') & \n        (value_analysis['recent_form_trend'] > 0)\n    )\n    value_analysis.loc[young_talent, 'opportunity_type'] = 'Rising Star'\n    \n    # Peak performers with high consistency\n    peak_performers = (\n        (value_analysis['development_phase'] == 'peak') & \n        (value_analysis['consistency_score'] > 0.7)\n    )\n    value_analysis.loc[peak_performers, 'opportunity_type'] = 'Safe Bet'\n    \n    # Value picks (good probability but potentially overlooked)\n    value_picks = (\n        (value_analysis['adjusted_probability'] > 0.6) & \n        (value_analysis['avg_points'] < 10)\n    )\n    value_analysis.loc[value_picks, 'opportunity_type'] = 'Hidden Value'\n    \n    # Sort by value score\n    value_analysis = value_analysis.sort_values('value_score', ascending=False)\n    \n    # Visualize opportunities\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Value vs Probability scatter\n    opportunity_colors = {\n        'Rising Star': 'green',\n        'Safe Bet': 'blue',\n        'Hidden Value': 'orange',\n        'Standard': 'gray'\n    }\n    \n    for opp_type, color in opportunity_colors.items():\n        mask = value_analysis['opportunity_type'] == opp_type\n        ax1.scatter(\n            value_analysis[mask]['adjusted_probability'],\n            value_analysis[mask]['value_score'],\n            c=color, label=opp_type, alpha=0.7, s=100\n        )\n    \n    ax1.set_xlabel('Adjusted Probability')\n    ax1.set_ylabel('Value Score')\n    ax1.set_title('Prize Picks Value Opportunities')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Top opportunities by type\n    opportunity_counts = value_analysis['opportunity_type'].value_counts()\n    ax2.bar(opportunity_counts.index, opportunity_counts.values)\n    ax2.set_xlabel('Opportunity Type')\n    ax2.set_ylabel('Count')\n    ax2.set_title('Distribution of Betting Opportunities')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return value_analysis\n\n# Create sample predictions for demonstration\nsample_features = pd.DataFrame({\n    'driverId': driver_metrics['driverId'].head(20),\n    'grid': np.random.randint(1, 20, 20),\n    'avg_position_3': np.random.uniform(5, 15, 20)\n})\n\n# Generate predictions\npredictions = predictor.predict_with_evaluation(\n    sample_features, \n    development_df,\n    compatibility_df\n)\n\n# Identify value opportunities\nvalue_opportunities = identify_prize_picks_value(\n    predictions,\n    driver_metrics,\n    development_df\n)\n\nprint(\"\\nTop 10 Prize Picks Value Opportunities:\")\nprint(value_opportunities.head(10)[[\n    'surname', 'adjusted_probability', 'value_score', \n    'opportunity_type', 'development_phase', 'confidence'\n]].round(3))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def generate_race_predictions(race_id=None):\n    \"\"\"\n    Generate predictions for a specific race or upcoming race\n    \"\"\"\n    # Get upcoming race if no race_id specified\n    if race_id is None:\n        # Find the most recent or next race\n        races['date'] = pd.to_datetime(races['date'])\n        upcoming = races[races['date'] > datetime.now()]\n        if not upcoming.empty:\n            race_info = upcoming.iloc[0]\n            print(f\"\\nGenerating predictions for: {race_info['name']} ({race_info['date']})\")\n        else:\n            print(\"No upcoming race found. Using most recent race.\")\n            recent_races = races.sort_values('date', ascending=False)\n            race_info = recent_races.iloc[0]\n    else:\n        race_info = races[races['raceId'] == race_id].iloc[0]\n    \n    # Get drivers likely to participate\n    # For demonstration, use top 20 active drivers\n    active_drivers = driver_metrics.nlargest(20, 'races_completed')\n    \n    # Create race-specific features\n    race_predictions = []\n    \n    for _, driver in active_drivers.iterrows():\n        # Get driver's historical performance at this circuit\n        circuit_history = df[\n            (df['driverId'] == driver['driverId']) & \n            (df['circuitId'] == race_info.get('circuitId', 0))\n        ]\n        \n        circuit_avg = circuit_history['positionOrder'].mean() if len(circuit_history) > 0 else 15\n        \n        # Create feature set\n        features = pd.DataFrame([{\n            'driverId': driver['driverId'],\n            'grid': 10,  # Placeholder\n            'circuit_avg': circuit_avg,\n            'recent_form': driver['recent_avg_position']\n        }])\n        \n        # Get predictions\n        pred = predictor.predict_with_evaluation(\n            features,\n            development_df,\n            compatibility_df\n        )\n        \n        if not pred.empty:\n            pred_data = pred.iloc[0]\n            race_predictions.append({\n                'driver': driver['surname'],\n                'team': constructors[\n                    constructors['constructorId'] == driver['current_constructor']\n                ]['name'].iloc[0] if driver['current_constructor'] in constructors['constructorId'].values else 'Unknown',\n                'top10_probability': pred_data['adjusted_probability'],\n                'confidence': pred_data['confidence'],\n                'age_factor': pred_data['age_factor'],\n                'recent_form': driver['recent_form_trend']\n            })\n    \n    predictions_df = pd.DataFrame(race_predictions)\n    predictions_df = predictions_df.sort_values('top10_probability', ascending=False)\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    y_pos = np.arange(len(predictions_df.head(15)))\n    bars = ax.barh(y_pos, predictions_df.head(15)['top10_probability'])\n    \n    # Color by confidence using colormap directly from matplotlib.cm\n    norm = Normalize(vmin=0, vmax=1)\n    # Access colormap directly from cm module\n    colors = cm.RdYlGn(norm(predictions_df.head(15)['confidence']))\n    for bar, color in zip(bars, colors):\n        bar.set_color(color)\n    \n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(predictions_df.head(15)['driver'])\n    ax.set_xlabel('Top 10 Probability')\n    ax.set_title(f'Integrated Predictions - {race_info.get(\"name\", \"Upcoming Race\")}')\n    ax.grid(True, alpha=0.3, axis='x')\n    \n    # Add confidence indicator\n    sm = cm.ScalarMappable(cmap=cm.RdYlGn, norm=norm)\n    sm.set_array([])\n    cbar = plt.colorbar(sm, ax=ax)\n    cbar.set_label('Confidence Level')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return predictions_df\n\n# Generate predictions\nrace_predictions = generate_race_predictions()\n\nprint(\"\\nTop Prize Picks Recommendations:\")\nprint(\"=\" * 60)\nfor idx, pred in race_predictions.head(5).iterrows():\n    print(f\"\\n{pred['driver']} ({pred['team']})\")\n    print(f\"  Top 10 Probability: {pred['top10_probability']:.1%}\")\n    print(f\"  Confidence: {pred['confidence']:.1%}\")\n    print(f\"  Age Factor: {pred['age_factor']:.2f}\")\n    print(f\"  Recent Form Trend: {pred['recent_form']:+.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_race_predictions(race_id=None):\n    \"\"\"\n    Generate predictions for a specific race or upcoming race\n    \"\"\"\n    # Get upcoming race if no race_id specified\n    if race_id is None:\n        # Find the most recent or next race\n        races['date'] = pd.to_datetime(races['date'])\n        upcoming = races[races['date'] > datetime.now()]\n        if not upcoming.empty:\n            race_info = upcoming.iloc[0]\n            print(f\"\\nGenerating predictions for: {race_info['name']} ({race_info['date']})\")\n        else:\n            print(\"No upcoming race found. Using most recent race.\")\n            recent_races = races.sort_values('date', ascending=False)\n            race_info = recent_races.iloc[0]\n    else:\n        race_info = races[races['raceId'] == race_id].iloc[0]\n    \n    # Get drivers likely to participate\n    # For demonstration, use top 20 active drivers\n    active_drivers = driver_metrics.nlargest(20, 'races_completed')\n    \n    # Create race-specific features\n    race_predictions = []\n    \n    for _, driver in active_drivers.iterrows():\n        # Get driver's historical performance at this circuit\n        circuit_history = df[\n            (df['driverId'] == driver['driverId']) & \n            (df['circuitId'] == race_info.get('circuitId', 0))\n        ]\n        \n        circuit_avg = circuit_history['positionOrder'].mean() if len(circuit_history) > 0 else 15\n        \n        # Create feature set\n        features = pd.DataFrame([{\n            'driverId': driver['driverId'],\n            'grid': 10,  # Placeholder\n            'circuit_avg': circuit_avg,\n            'recent_form': driver['recent_avg_position']\n        }])\n        \n        # Get predictions\n        pred = predictor.predict_with_evaluation(\n            features,\n            development_df,\n            compatibility_df\n        )\n        \n        if not pred.empty:\n            pred_data = pred.iloc[0]\n            race_predictions.append({\n                'driver': driver['surname'],\n                'team': constructors[\n                    constructors['constructorId'] == driver['current_constructor']\n                ]['name'].iloc[0] if driver['current_constructor'] in constructors['constructorId'].values else 'Unknown',\n                'top10_probability': pred_data['adjusted_probability'],\n                'confidence': pred_data['confidence'],\n                'age_factor': pred_data['age_factor'],\n                'recent_form': driver['recent_form_trend']\n            })\n    \n    predictions_df = pd.DataFrame(race_predictions)\n    predictions_df = predictions_df.sort_values('top10_probability', ascending=False)\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    y_pos = np.arange(len(predictions_df.head(15)))\n    bars = ax.barh(y_pos, predictions_df.head(15)['top10_probability'])\n    \n    # Color by confidence using colormap directly from matplotlib.cm\n    norm = Normalize(vmin=0, vmax=1)\n    # Access colormap directly from cm module instead of plt.get_cmap\n    colors = cm.RdYlGn(norm(predictions_df.head(15)['confidence']))\n    for bar, color in zip(bars, colors):\n        bar.set_color(color)\n    \n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(predictions_df.head(15)['driver'])\n    ax.set_xlabel('Top 10 Probability')\n    ax.set_title(f'Integrated Predictions - {race_info.get(\"name\", \"Upcoming Race\")}')\n    ax.grid(True, alpha=0.3, axis='x')\n    \n    # Add confidence indicator\n    sm = cm.ScalarMappable(cmap=cm.RdYlGn, norm=norm)\n    sm.set_array([])\n    cbar = plt.colorbar(sm, ax=ax)\n    cbar.set_label('Confidence Level')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return predictions_df\n\n# Generate predictions\nrace_predictions = generate_race_predictions()\n\nprint(\"\\nTop Prize Picks Recommendations:\")\nprint(\"=\" * 60)\nfor idx, pred in race_predictions.head(5).iterrows():\n    print(f\"\\n{pred['driver']} ({pred['team']})\")\n    print(f\"  Top 10 Probability: {pred['top10_probability']:.1%}\")\n    print(f\"  Confidence: {pred['confidence']:.1%}\")\n    print(f\"  Age Factor: {pred['age_factor']:.2f}\")\n    print(f\"  Recent Form Trend: {pred['recent_form']:+.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Integrated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the integrated components\nintegrated_model = {\n    'predictor': predictor,\n    'driver_metrics': driver_metrics,\n    'development_scores': development_df,\n    'compatibility_scores': compatibility_df,\n    'age_performance': age_performance,\n    'metadata': {\n        'created_date': datetime.now().isoformat(),\n        'n_drivers': len(driver_metrics),\n        'data_range': f\"{df['date'].min()} to {df['date'].max()}\"\n    }\n}\n\n# Save integrated model\njoblib.dump(integrated_model, 'f1_integrated_evaluation_model.pkl')\nprint(\"\\nIntegrated model saved successfully!\")\nprint(f\"Components: {list(integrated_model.keys())}\")\n\n# Update module exports after class is defined\n__all__ = ['IntegratedF1Predictor']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Integrated Driver Evaluation System combines:\n",
    "\n",
    "1. **Fixed Prediction Models**: Properly validated models without overfitting\n",
    "2. **Age-Performance Curves**: Adjust predictions based on driver development phase\n",
    "3. **Constructor Compatibility**: Factor in team dynamics and fit\n",
    "4. **Consistency Metrics**: Weight predictions by driver reliability\n",
    "5. **Value Identification**: Find overlooked betting opportunities\n",
    "\n",
    "### Key Insights for Prize Picks:\n",
    "- **Rising Stars**: Young drivers (pre-peak) showing improvement\n",
    "- **Safe Bets**: Peak-age drivers with high consistency\n",
    "- **Hidden Value**: Good probability but potentially lower odds\n",
    "- **Team Changes**: New driver-constructor pairings may offer value\n",
    "\n",
    "This integrated approach provides more nuanced predictions than pure statistical models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}