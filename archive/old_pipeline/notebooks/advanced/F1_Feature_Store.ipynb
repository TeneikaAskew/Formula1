{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Feature Store\n",
    "\n",
    "This notebook develops a comprehensive feature store for F1 predictions, including:\n",
    "- Weather data integration (simulated)\n",
    "- Momentum indicators\n",
    "- Track characteristics\n",
    "- Team strategy patterns\n",
    "- Advanced performance metrics\n",
    "\n",
    "The feature store provides a centralized, reusable set of features for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "# plt.style.use('seaborn-darkgrid') # Original style - may not work on all systems\n",
    "# Safe plotting style setup\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_theme()  # Modern seaborn initialization\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('ggplot')  # Fallback style\n",
    "    except:\n",
    "        pass  # Use default style\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup imports with robust path handling\nimport sys\nimport os\nfrom pathlib import Path\n\n# Determine the notebook location and add necessary paths\ntry:\n    # In Jupyter notebooks, __file__ might not be defined\n    notebook_dir = Path.cwd()\n    \n    # Check if we're in the advanced directory\n    if 'advanced' in str(notebook_dir):\n        # We're in the advanced directory\n        if str(notebook_dir) not in sys.path:\n            sys.path.insert(0, str(notebook_dir))\n    else:\n        # Add the advanced directory to path\n        workspace_root = notebook_dir\n        \n        # Navigate to find the advanced directory\n        possible_paths = [\n            notebook_dir / 'notebooks' / 'advanced',  # From workspace root\n            notebook_dir / 'advanced',  # From notebooks directory\n            notebook_dir.parent / 'advanced',  # If we're in a sibling directory\n            notebook_dir.parent / 'notebooks' / 'advanced',  # From other locations\n        ]\n        \n        for path in possible_paths:\n            if path.exists() and str(path) not in sys.path:\n                sys.path.insert(0, str(path))\n                break\n                \nexcept Exception as e:\n    print(f\"Path setup warning: {e}\")\n    # Fallback to simple path addition\n    sys.path.append('.')\n\n# Import the required modules\nfrom f1db_data_loader import load_f1db_data\nfrom f1_ml import fix_column_mappings, merge_race_data\n\n# Import feature engineering functions from f1_ml package\nfrom f1_ml.features import (\n    F1FeatureStore,\n    create_track_features,\n    simulate_weather_features,\n    create_momentum_features,\n    create_strategy_features,\n    create_advanced_metrics\n)"
  },
  {
   "cell_type": "code",
   "source": "# Load F1 data\nprint(\"Loading F1DB data...\")\ndata = load_f1db_data()\n\n# Apply column mappings to ensure compatibility\nprint(\"\\nApplying column mappings...\")\ndata = fix_column_mappings(data)\n\n# Extract individual dataframes for compatibility with existing code\nresults = data.get('results', pd.DataFrame())\nraces = data.get('races', pd.DataFrame())\ndrivers = data.get('drivers', pd.DataFrame())\nconstructors = data.get('constructors', pd.DataFrame())\ncircuits = data.get('circuits', pd.DataFrame())\npit_stops = data.get('pit_stops', pd.DataFrame())\nlap_times = data.get('lap_times', pd.DataFrame())\ndriver_standings = data.get('driver_standings', pd.DataFrame())\n\n# Print summary\nprint(f\"\\nData loaded successfully:\")\nprint(f\"  Results: {len(results)} records\")\nprint(f\"  Races: {len(races)} records\")\nprint(f\"  Drivers: {len(drivers)} records\")\nprint(f\"  Constructors: {len(constructors)} records\")\nprint(f\"  Circuits: {len(circuits)} records\")\nprint(f\"  Pit stops: {len(pit_stops)} records\")\nprint(f\"  Lap times: {len(lap_times)} records\")\nprint(f\"  Driver standings: {len(driver_standings)} records\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create base dataframe with all race results\n# Option 1: Use merge_race_data for automatic merging (recommended)\ndf_base = merge_race_data(data)\n\n# Option 2: Manual merging (if you need more control)\n# df_base = results.merge(races[['raceId', 'year', 'round', 'circuitId', 'date']], on='raceId')\n# df_base = df_base.merge(drivers[['driverId', 'driverRef', 'surname', 'code', 'dob']], on='driverId')\n# df_base = df_base.merge(constructors[['constructorId', 'constructorRef', 'name']], \n#                        on='constructorId', suffixes=('_race', '_constructor'))\n# df_base = df_base.merge(circuits[['circuitId', 'circuitRef', 'location', 'country', 'lat', 'lng']], \n#                        on='circuitId')\n\n# Convert dates if not already done by merge_race_data\nif 'date' in df_base.columns and df_base['date'].dtype == 'object':\n    df_base['date'] = pd.to_datetime(df_base['date'])\n    \nif 'dob' in df_base.columns and df_base['dob'].dtype == 'object':\n    df_base['dob'] = pd.to_datetime(df_base['dob'])\n    \n# Calculate driver age if not already present\nif 'driver_age' not in df_base.columns and 'date' in df_base.columns and 'dob' in df_base.columns:\n    df_base['driver_age'] = (df_base['date'] - df_base['dob']).dt.days / 365.25\n\n# Sort by date\ndf_base = df_base.sort_values(['date', 'raceId', 'positionOrder'])\n\nprint(f\"Base dataframe shape: {df_base.shape}\")\nprint(f\"Date range: {df_base['date'].min()} to {df_base['date'].max()}\")\n\n# Show available columns\nprint(f\"\\nAvailable columns: {sorted(df_base.columns.tolist())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base dataframe with all race results\n",
    "df_base = results.merge(races[['raceId', 'year', 'round', 'circuitId', 'date']], on='raceId')\n",
    "df_base = df_base.merge(drivers[['driverId', 'driverRef', 'surname', 'code', 'dob']], on='driverId')\n",
    "df_base = df_base.merge(constructors[['constructorId', 'constructorRef', 'name']], \n",
    "                       on='constructorId', suffixes=('_race', '_constructor'))\n",
    "df_base = df_base.merge(circuits[['circuitId', 'circuitRef', 'location', 'country', 'lat', 'lng']], \n",
    "                       on='circuitId')\n",
    "\n",
    "# Convert dates\n",
    "df_base['date'] = pd.to_datetime(df_base['date'])\n",
    "df_base['dob'] = pd.to_datetime(df_base['dob'])\n",
    "df_base['driver_age'] = (df_base['date'] - df_base['dob']).dt.days / 365.25\n",
    "\n",
    "# Sort by date\n",
    "df_base = df_base.sort_values(['date', 'raceId', 'positionOrder'])\n",
    "\n",
    "print(f\"Base dataframe shape: {df_base.shape}\")\n",
    "print(f\"Date range: {df_base['date'].min()} to {df_base['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Track Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create track features using imported function\ntrack_features = create_track_features(df_base, circuits)\n\nprint(\"Track features created:\")\nprint(track_features[['circuitRef', 'is_street_circuit', 'is_high_speed', \n                      'is_technical', 'overtaking_difficulty']].head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weather Features (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate weather features using imported function\nweather_features = simulate_weather_features(df_base)\n\n# Visualize weather distribution\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0, 0].hist(weather_features['rain_probability'], bins=20, edgecolor='black')\naxes[0, 0].set_title('Rain Probability Distribution')\naxes[0, 0].set_xlabel('Probability')\n\naxes[0, 1].hist(weather_features['temperature'], bins=20, edgecolor='black')\naxes[0, 1].set_title('Temperature Distribution')\naxes[0, 1].set_xlabel('Temperature (°C)')\n\naxes[1, 0].scatter(weather_features['temperature'], weather_features['humidity'], \n                  alpha=0.5, c=weather_features['is_wet_race'], cmap='coolwarm')\naxes[1, 0].set_xlabel('Temperature (°C)')\naxes[1, 0].set_ylabel('Humidity (%)')\naxes[1, 0].set_title('Temperature vs Humidity (color = wet race)')\n\nwet_race_pct = weather_features.groupby(pd.cut(weather_features['rain_probability'], \n                                               bins=5))['is_wet_race'].mean()\naxes[1, 1].bar(range(len(wet_race_pct)), wet_race_pct.values)\naxes[1, 1].set_xlabel('Rain Probability Bins')\naxes[1, 1].set_ylabel('Actual Wet Race %')\naxes[1, 1].set_title('Rain Probability vs Actual Wet Races')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nWeather features generated for {len(weather_features)} races\")\nprint(f\"Wet race percentage: {weather_features['is_wet_race'].mean():.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Momentum and Form Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create momentum features using imported function\ndf_momentum = create_momentum_features(df_base, windows=[3, 5, 10], driver_standings=driver_standings)\n\n# Visualize momentum trends\nrecent_data = df_momentum[df_momentum['year'] >= 2020]\ntop_drivers = recent_data.groupby('driverId')['points'].sum().nlargest(10).index\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\nfor driver_id in top_drivers[:5]:\n    driver_data = recent_data[recent_data['driverId'] == driver_id].sort_values('date')\n    driver_name = driver_data['surname'].iloc[0]\n    \n    ax.plot(driver_data['date'], \n           driver_data['points_momentum_5'].rolling(3).mean(), \n           label=driver_name, linewidth=2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Points Momentum (5-race average)')\nax.set_title('Driver Momentum Trends (Top 5 Drivers)')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMomentum features created:\")\nmomentum_cols = [col for col in df_momentum.columns if 'momentum' in col or 'trend' in col]\nprint(f\"Total momentum features: {len(momentum_cols)}\")\nprint(f\"Sample features: {momentum_cols[:5]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy Pattern Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create strategy features using imported function\ndf_strategy = create_strategy_features(df_momentum, pit_stops, lap_times)\n\nprint(\"\\nStrategy features created\")\nstrategy_cols = ['n_pit_stops', 'avg_pit_time', 'constructor_avg_pit_time', \n                'lap_consistency_score', 'strategy_type']\navailable_cols = [col for col in strategy_cols if col in df_strategy.columns]\nprint(f\"Available strategy features: {available_cols}\")\n\n# Visualize strategy preferences if available\nif 'strategy_type' in df_strategy.columns:\n    # Calculate constructor strategy preferences\n    strategy_prefs = df_strategy.groupby(['constructorId', 'strategy_type']).size().unstack(fill_value=0)\n    strategy_prefs = strategy_prefs.div(strategy_prefs.sum(axis=1), axis=0)\n    \n    # Visualize strategy preferences\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    top_constructors = df_strategy.groupby('constructorId').size().nlargest(10).index\n    strategy_prefs_top = strategy_prefs.loc[top_constructors]\n    \n    strategy_prefs_top.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_xlabel('Constructor ID')\n    ax.set_ylabel('Strategy Preference %')\n    ax.set_title('Pit Stop Strategy Preferences by Constructor')\n    ax.legend(title='Strategy Type')\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create advanced metrics using imported function\ndf_advanced, h2h_records = create_advanced_metrics(df_strategy, drivers)\n\nprint(\"\\nAdvanced metrics created:\")\nadvanced_cols = ['era_adjusted_points', 'era_adjusted_position', 'teammate_position_diff', \n                'clutch_factor', 'start_performance']\nprint(f\"New advanced features: {[col for col in advanced_cols if col in df_advanced.columns]}\")\n\n# Visualize advanced metrics\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Era-adjusted performance over time\nera_performance = df_advanced.groupby('year')['era_adjusted_points'].mean()\naxes[0, 0].plot(era_performance.index, era_performance.values)\naxes[0, 0].set_xlabel('Year')\naxes[0, 0].set_ylabel('Era-Adjusted Points')\naxes[0, 0].set_title('Average Era-Adjusted Performance Over Time')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Clutch factor distribution\nclutch_stats = df_advanced.groupby('driverId').agg({\n    'clutch_points': 'mean',\n    'points': 'mean'\n})\nclutch_stats['clutch_factor'] = clutch_stats['clutch_points'] / (clutch_stats['points'] + 0.1)\naxes[0, 1].hist(clutch_stats['clutch_factor'].dropna(), bins=20, edgecolor='black')\naxes[0, 1].set_xlabel('Clutch Factor')\naxes[0, 1].set_ylabel('Number of Drivers')\naxes[0, 1].set_title('Distribution of Driver Clutch Factors')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Teammate performance comparison\nrecent_teammate_diff = df_advanced[df_advanced['year'] >= 2020].groupby('driverId')['teammate_position_diff'].mean()\ntop_teammates = recent_teammate_diff.nsmallest(10)\n\naxes[1, 0].barh(range(len(top_teammates)), top_teammates.values)\naxes[1, 0].set_yticks(range(len(top_teammates)))\naxes[1, 0].set_yticklabels([drivers[drivers['driverId'] == d]['surname'].iloc[0] \n                            for d in top_teammates.index])\naxes[1, 0].set_xlabel('Average Position Difference vs Teammate')\naxes[1, 0].set_title('Top 10 Drivers vs Teammates (2020+)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Head-to-head matrix\nif not h2h_records.empty:\n    # Create matrix for visualization\n    h2h_pivot = h2h_records.pivot_table(\n        values='driver1_wins', \n        index='driver1', \n        columns='driver2',\n        aggfunc='sum'\n    )\n    \n    # Get driver names\n    driver_names = {d: drivers[drivers['driverId'] == d]['surname'].iloc[0] \n                   for d in h2h_pivot.index if d in drivers['driverId'].values}\n    \n    h2h_pivot.index = [driver_names.get(d, f'Driver{d}') for d in h2h_pivot.index]\n    h2h_pivot.columns = [driver_names.get(d, f'Driver{d}') for d in h2h_pivot.columns]\n    \n    sns.heatmap(h2h_pivot.fillna(0), annot=True, fmt='.0f', cmap='RdYlGn', \n               ax=axes[1, 1], cbar_kws={'label': 'Wins'})\n    axes[1, 1].set_title('Head-to-Head Records (Top Drivers)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Store Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import F1FeatureStore from the f1_ml package\nfrom f1_ml.features import F1FeatureStore\n\n# Create and populate feature store\nfeature_store = F1FeatureStore()\nall_features = feature_store.build_feature_store(df_advanced, track_features, weather_features)\n\nprint(\"\\nFeature Store Summary:\")\nprint(\"=\" * 50)\nprint(f\"Total records: {len(all_features):,}\")\nprint(f\"Total features: {len(all_features.columns)}\")\nprint(f\"Date range: {all_features['date'].min()} to {all_features['date'].max()}\")\nprint(f\"\\nFeature groups:\")\nfor group, metadata in feature_store.feature_metadata.items():\n    print(f\"  {group}: {metadata['count']} features\")\n\n# Save feature store\nfeature_store.save_feature_store('f1_feature_store.parquet')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_quality(feature_store):\n",
    "    \"\"\"\n",
    "    Analyze feature quality and usefulness\n",
    "    \"\"\"\n",
    "    df = feature_store.base_features\n",
    "    \n",
    "    # Get numeric features only\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Calculate feature statistics\n",
    "    feature_stats = []\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        if feature in ['raceId', 'driverId', 'constructorId', 'circuitId']:  # Skip IDs\n",
    "            continue\n",
    "            \n",
    "        stats = {\n",
    "            'feature': feature,\n",
    "            'missing_pct': df[feature].isna().mean() * 100,\n",
    "            'unique_values': df[feature].nunique(),\n",
    "            'std_dev': df[feature].std(),\n",
    "            'skewness': df[feature].skew(),\n",
    "            'kurtosis': df[feature].kurtosis()\n",
    "        }\n",
    "        \n",
    "        # Correlation with target (position)\n",
    "        if 'positionOrder' in df.columns:\n",
    "            stats['correlation_with_position'] = df[feature].corr(df['positionOrder'])\n",
    "        \n",
    "        feature_stats.append(stats)\n",
    "    \n",
    "    feature_quality_df = pd.DataFrame(feature_stats)\n",
    "    \n",
    "    # Identify potential issues\n",
    "    print(\"\\nFeature Quality Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # High missing data\n",
    "    high_missing = feature_quality_df[feature_quality_df['missing_pct'] > 20]\n",
    "    if not high_missing.empty:\n",
    "        print(\"\\nFeatures with >20% missing data:\")\n",
    "        print(high_missing[['feature', 'missing_pct']].round(1))\n",
    "    \n",
    "    # Low variance features\n",
    "    low_variance = feature_quality_df[feature_quality_df['std_dev'] < 0.01]\n",
    "    if not low_variance.empty:\n",
    "        print(\"\\nLow variance features (might not be useful):\")\n",
    "        print(low_variance[['feature', 'std_dev']])\n",
    "    \n",
    "    # Highly correlated with target\n",
    "    if 'correlation_with_position' in feature_quality_df.columns:\n",
    "        high_corr = feature_quality_df.nlargest(10, 'correlation_with_position')\n",
    "        print(\"\\nTop 10 features correlated with position:\")\n",
    "        print(high_corr[['feature', 'correlation_with_position']].round(3))\n",
    "    \n",
    "    # Visualize feature distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Missing data\n",
    "    missing_data = feature_quality_df.nlargest(15, 'missing_pct')\n",
    "    axes[0, 0].barh(missing_data['feature'], missing_data['missing_pct'])\n",
    "    axes[0, 0].set_xlabel('Missing %')\n",
    "    axes[0, 0].set_title('Features with Most Missing Data')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature importance proxy (absolute correlation)\n",
    "    if 'correlation_with_position' in feature_quality_df.columns:\n",
    "        feature_quality_df['abs_correlation'] = feature_quality_df['correlation_with_position'].abs()\n",
    "        top_corr = feature_quality_df.nlargest(15, 'abs_correlation')\n",
    "        axes[0, 1].barh(top_corr['feature'], top_corr['abs_correlation'])\n",
    "        axes[0, 1].set_xlabel('Absolute Correlation with Position')\n",
    "        axes[0, 1].set_title('Most Predictive Features')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Skewness distribution\n",
    "    axes[1, 0].hist(feature_quality_df['skewness'].dropna(), bins=30, edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Skewness')\n",
    "    axes[1, 0].set_ylabel('Number of Features')\n",
    "    axes[1, 0].set_title('Feature Skewness Distribution')\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--', label='No skew')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature count by group\n",
    "    group_counts = pd.Series({group: metadata['count'] \n",
    "                             for group, metadata in feature_store.feature_metadata.items()})\n",
    "    axes[1, 1].pie(group_counts.values, labels=group_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Feature Distribution by Group')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_quality_df\n",
    "\n",
    "# Analyze feature quality\n",
    "feature_quality = analyze_feature_quality(feature_store)\n",
    "\n",
    "# Get recommended feature set\n",
    "recommended_features = feature_store.get_feature_set(\n",
    "    ['basic', 'driver', 'track', 'weather', 'momentum', 'advanced']\n",
    ")\n",
    "print(f\"\\nRecommended feature set: {len(recommended_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The F1 Feature Store provides:\n",
    "\n",
    "1. **Track Characteristics**: Circuit type, overtaking difficulty, historical performance\n",
    "2. **Weather Features**: Simulated weather conditions affecting race outcomes\n",
    "3. **Momentum Indicators**: Recent form, consistency, championship pressure\n",
    "4. **Strategy Patterns**: Pit stop timing, tire strategies, team preferences\n",
    "5. **Advanced Metrics**: Era-adjusted performance, clutch factor, head-to-head records\n",
    "\n",
    "### Key Insights:\n",
    "- Weather significantly impacts race strategies and outcomes\n",
    "- Momentum features capture driver form better than static averages\n",
    "- Track characteristics strongly influence overtaking opportunities\n",
    "- Team strategy patterns are predictable and vary by constructor\n",
    "\n",
    "The feature store is saved as a Parquet file for efficient storage and quick loading in production models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}