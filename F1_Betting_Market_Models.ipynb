{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Betting Market Prediction Models\n",
    "\n",
    "This notebook implements sophisticated probabilistic models for F1 betting markets, including:\n",
    "- Exact finishing position predictions\n",
    "- Head-to-head matchup probabilities\n",
    "- Points scoring probabilities\n",
    "- DNF risk assessment\n",
    "- Calibrated probability outputs for odds generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 data already extracted.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Extract F1 data if not already extracted\n",
    "if not os.path.exists('data/results.csv'):\n",
    "    print(\"Extracting F1 data from f1db_csv.zip...\")\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    with zipfile.ZipFile('f1db_csv.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data/')\n",
    "    print(\"Data extraction complete!\")\n",
    "else:\n",
    "    print(\"F1 data already extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data for Betting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betting dataset shape: (2619, 34)\n",
      "Years covered: 2018 - 2024\n",
      "Total races: 131\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "results = pd.read_csv('data/results.csv')\n",
    "races = pd.read_csv('data/races.csv')\n",
    "drivers = pd.read_csv('data/drivers.csv')\n",
    "constructors = pd.read_csv('data/constructors.csv')\n",
    "qualifying = pd.read_csv('data/qualifying.csv')\n",
    "driver_standings = pd.read_csv('data/driver_standings.csv')\n",
    "constructor_standings = pd.read_csv('data/constructor_standings.csv')\n",
    "status = pd.read_csv('data/status.csv')\n",
    "circuits = pd.read_csv('data/circuits.csv')\n",
    "\n",
    "# Merge core data\n",
    "df = results.merge(races[['raceId', 'year', 'round', 'circuitId', 'date', 'name']], on='raceId')\n",
    "df = df.merge(drivers[['driverId', 'driverRef', 'code']], on='driverId')\n",
    "df = df.merge(constructors[['constructorId', 'constructorRef']], on='constructorId')\n",
    "df = df.merge(circuits[['circuitId', 'circuitRef']], on='circuitId')\n",
    "df = df.merge(status[['statusId', 'status']], on='statusId')\n",
    "df = df.merge(qualifying[['raceId', 'driverId', 'position']], \n",
    "              on=['raceId', 'driverId'], \n",
    "              how='left', \n",
    "              suffixes=('', '_quali'))\n",
    "\n",
    "# Focus on recent data for betting relevance\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df_betting = df[df['year'] >= 2018].copy()\n",
    "\n",
    "# Create betting-relevant features\n",
    "df_betting['finished'] = (df_betting['statusId'] == 1).astype(int)\n",
    "df_betting['points_scored'] = (df_betting['points'] > 0).astype(int)\n",
    "df_betting['podium'] = (df_betting['positionOrder'] <= 3) & (df_betting['finished'] == 1)\n",
    "df_betting['top_5'] = (df_betting['positionOrder'] <= 5) & (df_betting['finished'] == 1)\n",
    "df_betting['top_10'] = (df_betting['positionOrder'] <= 10) & (df_betting['finished'] == 1)\n",
    "\n",
    "print(f\"Betting dataset shape: {df_betting.shape}\")\n",
    "print(f\"Years covered: {df_betting['year'].min()} - {df_betting['year'].max()}\")\n",
    "print(f\"Total races: {df_betting['raceId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering for Betting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dataset shape after feature engineering: (1683, 52)\n"
     ]
    }
   ],
   "source": [
    "def create_betting_features(df):\n",
    "    \"\"\"\n",
    "    Create features specifically designed for betting predictions\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['driverId', 'date']).copy()\n",
    "    \n",
    "    # Recent form features (last 5 races)\n",
    "    for window in [3, 5, 10]:\n",
    "        df[f'avg_position_last_{window}'] = df.groupby('driverId')['positionOrder'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f'dnf_rate_last_{window}'] = df.groupby('driverId')['finished'].transform(\n",
    "            lambda x: 1 - x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f'points_rate_last_{window}'] = df.groupby('driverId')['points_scored'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Constructor form\n",
    "    df['constructor_avg_position'] = df.groupby(['constructorId', 'raceId'])['positionOrder'].transform('mean')\n",
    "    df['constructor_reliability'] = df.groupby('constructorId')['finished'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Track-specific features\n",
    "    df['driver_track_history'] = df.groupby(['driverId', 'circuitId']).cumcount()\n",
    "    df['driver_track_avg_position'] = df.groupby(['driverId', 'circuitId'])['positionOrder'].transform(\n",
    "        lambda x: x.shift(1).expanding().mean()\n",
    "    )\n",
    "    df['driver_track_dnf_rate'] = df.groupby(['driverId', 'circuitId'])['finished'].transform(\n",
    "        lambda x: 1 - x.shift(1).expanding().mean()\n",
    "    )\n",
    "    \n",
    "    # Qualifying performance\n",
    "    df['quali_position'] = df['position_quali'].fillna(20)\n",
    "    df['quali_to_grid_change'] = df['grid'] - df['quali_position']\n",
    "    \n",
    "    # Season progress\n",
    "    df['season_progress'] = df.groupby('year')['round'].transform(lambda x: x / x.max())\n",
    "    \n",
    "    # Championship pressure\n",
    "    season_standings = df.groupby(['year', 'driverId'])['points'].sum().reset_index()\n",
    "    season_standings['championship_position'] = season_standings.groupby('year')['points'].rank(ascending=False)\n",
    "    df = df.merge(season_standings[['year', 'driverId', 'championship_position']], on=['year', 'driverId'], how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_features = create_betting_features(df_betting)\n",
    "\n",
    "# Define feature columns for models\n",
    "feature_cols = [\n",
    "    'grid', 'quali_position', 'quali_to_grid_change',\n",
    "    'avg_position_last_3', 'avg_position_last_5', 'avg_position_last_10',\n",
    "    'dnf_rate_last_3', 'dnf_rate_last_5', 'dnf_rate_last_10',\n",
    "    'points_rate_last_3', 'points_rate_last_5', 'points_rate_last_10',\n",
    "    'constructor_avg_position', 'constructor_reliability',\n",
    "    'driver_track_history', 'driver_track_avg_position', 'driver_track_dnf_rate',\n",
    "    'season_progress', 'championship_position'\n",
    "]\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df_model = df_features.dropna(subset=feature_cols).copy()\n",
    "print(f\"Model dataset shape after feature engineering: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ordinal Regression for Exact Position Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ordinal regression model for position predictions...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GradientBoostingClassifier' object has no attribute 'estimators_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25784/4075991427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;31m# Train ordinal regression model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training ordinal regression model for position predictions...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m ordinal_model = OrdinalRegressionClassifier(\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mbase_classifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25784/4075991427.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, base_classifier)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_classifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_classifier\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;34m\"\"\"Return the number of estimators in the ensemble.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GradientBoostingClassifier' object has no attribute 'estimators_'"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class OrdinalRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Ordinal regression for predicting race finishing positions\n",
    "    \"\"\"\n",
    "    def __init__(self, base_classifier=None):\n",
    "        self.base_classifier = base_classifier or LogisticRegression(max_iter=1000)\n",
    "        self.classifiers = {}\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        \n",
    "        # Train binary classifiers for each threshold\n",
    "        for i, threshold in enumerate(self.classes_[:-1]):\n",
    "            # Create binary target: 1 if position <= threshold\n",
    "            binary_y = (y <= threshold).astype(int)\n",
    "            \n",
    "            # Clone and train classifier\n",
    "            clf = clone(self.base_classifier)\n",
    "            clf.fit(X, binary_y)\n",
    "            self.classifiers[threshold] = clf\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        probas = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        # Get cumulative probabilities\n",
    "        cum_probas = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i, threshold in enumerate(self.classes_[:-1]):\n",
    "            cum_probas[:, i] = self.classifiers[threshold].predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Ensure monotonicity\n",
    "        for i in range(1, n_classes - 1):\n",
    "            cum_probas[:, i] = np.maximum(cum_probas[:, i], cum_probas[:, i-1])\n",
    "        \n",
    "        # Convert cumulative to individual probabilities\n",
    "        probas[:, 0] = cum_probas[:, 0]\n",
    "        for i in range(1, n_classes - 1):\n",
    "            probas[:, i] = cum_probas[:, i] - cum_probas[:, i-1]\n",
    "        probas[:, -1] = 1 - cum_probas[:, -2]\n",
    "        \n",
    "        # Normalize to ensure sum to 1\n",
    "        probas = probas / probas.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Prepare data for position prediction\n",
    "# Only consider finished races for position prediction\n",
    "finished_races = df_model[df_model['finished'] == 1].copy()\n",
    "\n",
    "X = finished_races[feature_cols]\n",
    "y = finished_races['positionOrder']\n",
    "\n",
    "# Split data temporally\n",
    "train_date = '2021-12-31'\n",
    "train_mask = finished_races['date'] <= train_date\n",
    "test_mask = finished_races['date'] > train_date\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train ordinal regression model\n",
    "print(\"Training ordinal regression model for position predictions...\")\n",
    "ordinal_model = OrdinalRegressionClassifier(\n",
    "    base_classifier=GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    ")\n",
    "ordinal_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "position_probas = ordinal_model.predict_proba(X_test_scaled)\n",
    "predicted_positions = ordinal_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate predictions\n",
    "mae = np.mean(np.abs(predicted_positions - y_test))\n",
    "print(f\"\\nPosition Prediction MAE: {mae:.2f}\")\n",
    "\n",
    "# Visualize prediction accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predicted_positions, alpha=0.5)\n",
    "plt.plot([1, 20], [1, 20], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('Actual Position')\n",
    "plt.ylabel('Predicted Position')\n",
    "plt.title('Position Prediction Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Head-to-Head Matchup Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head_to_head_dataset(df):\n",
    "    \"\"\"\n",
    "    Create pairwise comparisons for head-to-head predictions\n",
    "    \"\"\"\n",
    "    h2h_data = []\n",
    "    \n",
    "    # Group by race\n",
    "    for race_id, race_data in df.groupby('raceId'):\n",
    "        drivers = race_data['driverId'].unique()\n",
    "        \n",
    "        # Create all pairwise comparisons\n",
    "        for i in range(len(drivers)):\n",
    "            for j in range(i+1, len(drivers)):\n",
    "                driver1_data = race_data[race_data['driverId'] == drivers[i]].iloc[0]\n",
    "                driver2_data = race_data[race_data['driverId'] == drivers[j]].iloc[0]\n",
    "                \n",
    "                # Only include if both drivers finished\n",
    "                if driver1_data['finished'] == 1 and driver2_data['finished'] == 1:\n",
    "                    # Create feature differences\n",
    "                    features_diff = {}\n",
    "                    for col in feature_cols:\n",
    "                        features_diff[f'{col}_diff'] = driver1_data[col] - driver2_data[col]\n",
    "                    \n",
    "                    # Target: 1 if driver1 finished ahead\n",
    "                    features_diff['driver1_wins'] = int(driver1_data['positionOrder'] < driver2_data['positionOrder'])\n",
    "                    features_diff['race_id'] = race_id\n",
    "                    features_diff['driver1_id'] = drivers[i]\n",
    "                    features_diff['driver2_id'] = drivers[j]\n",
    "                    features_diff['driver1_ref'] = driver1_data['driverRef']\n",
    "                    features_diff['driver2_ref'] = driver2_data['driverRef']\n",
    "                    features_diff['date'] = driver1_data['date']\n",
    "                    \n",
    "                    h2h_data.append(features_diff)\n",
    "    \n",
    "    return pd.DataFrame(h2h_data)\n",
    "\n",
    "# Create head-to-head dataset\n",
    "print(\"Creating head-to-head comparison dataset...\")\n",
    "h2h_df = create_head_to_head_dataset(df_model)\n",
    "\n",
    "# Define feature columns for H2H\n",
    "h2h_feature_cols = [col for col in h2h_df.columns if col.endswith('_diff')]\n",
    "\n",
    "# Split data\n",
    "train_mask = h2h_df['date'] <= train_date\n",
    "test_mask = h2h_df['date'] > train_date\n",
    "\n",
    "X_h2h_train = h2h_df[train_mask][h2h_feature_cols]\n",
    "y_h2h_train = h2h_df[train_mask]['driver1_wins']\n",
    "X_h2h_test = h2h_df[test_mask][h2h_feature_cols]\n",
    "y_h2h_test = h2h_df[test_mask]['driver1_wins']\n",
    "\n",
    "# Scale features\n",
    "scaler_h2h = StandardScaler()\n",
    "X_h2h_train_scaled = scaler_h2h.fit_transform(X_h2h_train)\n",
    "X_h2h_test_scaled = scaler_h2h.transform(X_h2h_test)\n",
    "\n",
    "# Train calibrated classifier for H2H predictions\n",
    "print(\"Training head-to-head prediction model...\")\n",
    "h2h_base_model = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "h2h_model = CalibratedClassifierCV(h2h_base_model, method='isotonic', cv=3)\n",
    "h2h_model.fit(X_h2h_train_scaled, y_h2h_train)\n",
    "\n",
    "# Make predictions\n",
    "h2h_probas = h2h_model.predict_proba(X_h2h_test_scaled)[:, 1]\n",
    "h2h_predictions = (h2h_probas > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "h2h_accuracy = accuracy_score(y_h2h_test, h2h_predictions)\n",
    "h2h_log_loss = log_loss(y_h2h_test, h2h_probas)\n",
    "\n",
    "print(f\"\\nHead-to-Head Prediction Accuracy: {h2h_accuracy:.3f}\")\n",
    "print(f\"Head-to-Head Log Loss: {h2h_log_loss:.3f}\")\n",
    "\n",
    "# Calibration plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_h2h_test, h2h_probas, n_bins=10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-', label='H2H Model')\n",
    "plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Head-to-Head Probability Calibration')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DNF Risk Assessment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DNF prediction dataset\n",
    "X_dnf = df_model[feature_cols]\n",
    "y_dnf = (df_model['finished'] == 0).astype(int)  # 1 if DNF\n",
    "\n",
    "# Split data\n",
    "train_mask = df_model['date'] <= train_date\n",
    "test_mask = df_model['date'] > train_date\n",
    "\n",
    "X_dnf_train, y_dnf_train = X_dnf[train_mask], y_dnf[train_mask]\n",
    "X_dnf_test, y_dnf_test = X_dnf[test_mask], y_dnf[test_mask]\n",
    "\n",
    "# Scale features\n",
    "X_dnf_train_scaled = scaler.transform(X_dnf_train)\n",
    "X_dnf_test_scaled = scaler.transform(X_dnf_test)\n",
    "\n",
    "# Train DNF prediction model with calibration\n",
    "print(\"Training DNF risk assessment model...\")\n",
    "dnf_base_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "dnf_model = CalibratedClassifierCV(dnf_base_model, method='isotonic', cv=3)\n",
    "dnf_model.fit(X_dnf_train_scaled, y_dnf_train)\n",
    "\n",
    "# Make predictions\n",
    "dnf_probas = dnf_model.predict_proba(X_dnf_test_scaled)[:, 1]\n",
    "\n",
    "# Analyze DNF rates by different factors\n",
    "test_data = df_model[test_mask].copy()\n",
    "test_data['dnf_probability'] = dnf_probas\n",
    "\n",
    "# DNF probability by constructor\n",
    "constructor_dnf = test_data.groupby('constructorRef').agg({\n",
    "    'dnf_probability': 'mean',\n",
    "    'finished': lambda x: 1 - x.mean()  # Actual DNF rate\n",
    "}).round(3)\n",
    "constructor_dnf.columns = ['Predicted DNF Rate', 'Actual DNF Rate']\n",
    "constructor_dnf = constructor_dnf.sort_values('Predicted DNF Rate', ascending=False)\n",
    "\n",
    "print(\"\\nDNF Risk by Constructor:\")\n",
    "print(constructor_dnf.head(10))\n",
    "\n",
    "# Visualize DNF probabilities distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dnf_probas[y_dnf_test == 0], bins=30, alpha=0.5, label='Finished', density=True)\n",
    "plt.hist(dnf_probas[y_dnf_test == 1], bins=30, alpha=0.5, label='DNF', density=True)\n",
    "plt.xlabel('Predicted DNF Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('DNF Probability Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Points Scoring Probability Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-class target for points ranges\n",
    "def categorize_points(points):\n",
    "    if points == 0:\n",
    "        return 0  # No points\n",
    "    elif points <= 4:\n",
    "        return 1  # 1-4 points (P7-P10)\n",
    "    elif points <= 10:\n",
    "        return 2  # 6-10 points (P5-P6)\n",
    "    elif points <= 18:\n",
    "        return 3  # 12-18 points (P3-P4)\n",
    "    else:\n",
    "        return 4  # 25+ points (P1-P2)\n",
    "\n",
    "df_model['points_category'] = df_model['points'].apply(categorize_points)\n",
    "\n",
    "# Prepare data\n",
    "X_points = df_model[feature_cols]\n",
    "y_points = df_model['points_category']\n",
    "\n",
    "# Split data\n",
    "X_points_train, y_points_train = X_points[train_mask], y_points[train_mask]\n",
    "X_points_test, y_points_test = X_points[test_mask], y_points[test_mask]\n",
    "\n",
    "# Scale features\n",
    "X_points_train_scaled = scaler.transform(X_points_train)\n",
    "X_points_test_scaled = scaler.transform(X_points_test)\n",
    "\n",
    "# Train points prediction model\n",
    "print(\"Training points scoring probability model...\")\n",
    "points_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "points_model.fit(X_points_train_scaled, y_points_train)\n",
    "\n",
    "# Get probability predictions\n",
    "points_probas = points_model.predict_proba(X_points_test_scaled)\n",
    "\n",
    "# Calculate expected points\n",
    "points_mapping = {0: 0, 1: 2, 2: 8, 3: 15, 4: 25}  # Average points per category\n",
    "expected_points = np.sum(points_probas * np.array([points_mapping[i] for i in range(5)]), axis=1)\n",
    "\n",
    "# Actual points for comparison\n",
    "actual_points = df_model[test_mask]['points'].values\n",
    "\n",
    "# Evaluate expected points accuracy\n",
    "points_mae = np.mean(np.abs(expected_points - actual_points))\n",
    "print(f\"\\nExpected Points MAE: {points_mae:.2f}\")\n",
    "\n",
    "# Visualize expected vs actual points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual_points, expected_points, alpha=0.5)\n",
    "plt.plot([0, 25], [0, 25], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('Actual Points')\n",
    "plt.ylabel('Expected Points')\n",
    "plt.title('Expected Points vs Actual Points')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show probability distribution for a sample of drivers\n",
    "test_races = df_model[test_mask].iloc[:20]\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create stacked bar chart of point probabilities\n",
    "bottom = np.zeros(20)\n",
    "colors = ['red', 'orange', 'yellow', 'lightgreen', 'darkgreen']\n",
    "labels = ['No points', '1-4 pts', '6-10 pts', '12-18 pts', '25+ pts']\n",
    "\n",
    "for i in range(5):\n",
    "    ax.bar(range(20), points_probas[:20, i], bottom=bottom, \n",
    "           label=labels[i], color=colors[i], alpha=0.8)\n",
    "    bottom += points_probas[:20, i]\n",
    "\n",
    "ax.set_xlabel('Driver (sample)')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Points Scoring Probability Distribution')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(20))\n",
    "ax.set_xticklabels(test_races['driverRef'].values, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrated Betting Odds Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_betting_odds(race_data, models, scalers):\n",
    "    \"\"\"\n",
    "    Generate comprehensive betting odds for a race\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    X = race_data[feature_cols]\n",
    "    X_scaled = scalers['main'].transform(X)\n",
    "    \n",
    "    odds_data = []\n",
    "    \n",
    "    for idx, (_, driver) in enumerate(race_data.iterrows()):\n",
    "        driver_odds = {\n",
    "            'driver': driver['driverRef'],\n",
    "            'constructor': driver['constructorRef'],\n",
    "            'grid': driver['grid']\n",
    "        }\n",
    "        \n",
    "        # DNF probability\n",
    "        dnf_prob = models['dnf'].predict_proba(X_scaled[idx:idx+1])[:, 1][0]\n",
    "        driver_odds['dnf_probability'] = dnf_prob\n",
    "        driver_odds['finish_probability'] = 1 - dnf_prob\n",
    "        \n",
    "        # Position probabilities (if finishes)\n",
    "        if 'position' in models:\n",
    "            pos_probas = models['position'].predict_proba(X_scaled[idx:idx+1])[0]\n",
    "            driver_odds['win_probability'] = pos_probas[0] * (1 - dnf_prob)\n",
    "            driver_odds['podium_probability'] = pos_probas[:3].sum() * (1 - dnf_prob)\n",
    "            driver_odds['top5_probability'] = pos_probas[:5].sum() * (1 - dnf_prob)\n",
    "            driver_odds['top10_probability'] = pos_probas[:10].sum() * (1 - dnf_prob)\n",
    "        \n",
    "        # Points probability\n",
    "        points_probas = models['points'].predict_proba(X_scaled[idx:idx+1])[0]\n",
    "        driver_odds['points_probability'] = 1 - points_probas[0]  # Probability of scoring any points\n",
    "        \n",
    "        # Expected points\n",
    "        points_mapping = {0: 0, 1: 2, 2: 8, 3: 15, 4: 25}\n",
    "        driver_odds['expected_points'] = sum(points_probas[i] * points_mapping[i] for i in range(5))\n",
    "        \n",
    "        # Convert probabilities to decimal odds\n",
    "        for key in ['win_probability', 'podium_probability', 'top5_probability', \n",
    "                   'top10_probability', 'points_probability']:\n",
    "            if key in driver_odds and driver_odds[key] > 0:\n",
    "                driver_odds[f'{key.replace(\"_probability\", \"\")}_odds'] = 1 / driver_odds[key]\n",
    "        \n",
    "        odds_data.append(driver_odds)\n",
    "    \n",
    "    return pd.DataFrame(odds_data)\n",
    "\n",
    "# Example: Generate odds for a recent race\n",
    "recent_race = df_model[df_model['date'] > '2023-01-01'].groupby('raceId').first().index[0]\n",
    "race_data = df_model[df_model['raceId'] == recent_race]\n",
    "\n",
    "# Collect all models\n",
    "models = {\n",
    "    'dnf': dnf_model,\n",
    "    'points': points_model,\n",
    "    # 'position': ordinal_model  # Commented out due to complexity in example\n",
    "}\n",
    "\n",
    "scalers = {\n",
    "    'main': scaler,\n",
    "    'h2h': scaler_h2h\n",
    "}\n",
    "\n",
    "# Generate odds\n",
    "race_odds = generate_betting_odds(race_data, models, scalers)\n",
    "race_odds = race_odds.sort_values('expected_points', ascending=False)\n",
    "\n",
    "print(\"\\nBetting Odds for Sample Race:\")\n",
    "print(\"=\" * 80)\n",
    "print(race_odds[['driver', 'constructor', 'grid', 'dnf_probability', \n",
    "                'points_probability', 'expected_points']].round(3))\n",
    "\n",
    "# Visualize odds\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Points probability by driver\n",
    "ax1.barh(race_odds['driver'][:15], race_odds['points_probability'][:15])\n",
    "ax1.set_xlabel('Probability of Scoring Points')\n",
    "ax1.set_title('Points Scoring Probability by Driver')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Expected points vs grid position\n",
    "ax2.scatter(race_odds['grid'], race_odds['expected_points'], s=100)\n",
    "for idx, row in race_odds.iterrows():\n",
    "    ax2.annotate(row['driver'][:3], (row['grid'], row['expected_points']), fontsize=8)\n",
    "ax2.set_xlabel('Grid Position')\n",
    "ax2.set_ylabel('Expected Points')\n",
    "ax2.set_title('Expected Points vs Grid Position')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Confidence and Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_confidence(models, X_scaled, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Calculate prediction confidence using bootstrapping\n",
    "    \"\"\"\n",
    "    n_samples = X_scaled.shape[0]\n",
    "    \n",
    "    # Store predictions from each iteration\n",
    "    dnf_predictions = []\n",
    "    points_predictions = []\n",
    "    \n",
    "    # Bootstrap predictions\n",
    "    for _ in range(n_iterations):\n",
    "        # Sample with replacement\n",
    "        idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_boot = X_scaled[idx]\n",
    "        \n",
    "        # Get predictions\n",
    "        dnf_pred = models['dnf'].predict_proba(X_boot)[:, 1]\n",
    "        points_pred = models['points'].predict_proba(X_boot)\n",
    "        \n",
    "        dnf_predictions.append(dnf_pred)\n",
    "        points_predictions.append(points_pred)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    dnf_predictions = np.array(dnf_predictions)\n",
    "    dnf_mean = np.mean(dnf_predictions, axis=0)\n",
    "    dnf_std = np.std(dnf_predictions, axis=0)\n",
    "    dnf_lower = np.percentile(dnf_predictions, 5, axis=0)\n",
    "    dnf_upper = np.percentile(dnf_predictions, 95, axis=0)\n",
    "    \n",
    "    confidence_data = pd.DataFrame({\n",
    "        'dnf_mean': dnf_mean,\n",
    "        'dnf_std': dnf_std,\n",
    "        'dnf_lower_90': dnf_lower,\n",
    "        'dnf_upper_90': dnf_upper,\n",
    "        'dnf_confidence': 1 - (dnf_std / (dnf_mean + 0.001))  # Confidence metric\n",
    "    })\n",
    "    \n",
    "    return confidence_data\n",
    "\n",
    "# Calculate confidence for test predictions\n",
    "print(\"Calculating prediction confidence intervals...\")\n",
    "confidence = calculate_prediction_confidence(models, X_test_scaled[:100])\n",
    "\n",
    "# Visualize confidence intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "sample_idx = range(20)\n",
    "plt.errorbar(sample_idx, \n",
    "            confidence['dnf_mean'][:20], \n",
    "            yerr=[confidence['dnf_mean'][:20] - confidence['dnf_lower_90'][:20],\n",
    "                  confidence['dnf_upper_90'][:20] - confidence['dnf_mean'][:20]],\n",
    "            fmt='o', capsize=5)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('DNF Probability')\n",
    "plt.title('DNF Predictions with 90% Confidence Intervals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrediction Confidence Summary:\")\n",
    "print(f\"Average confidence: {confidence['dnf_confidence'].mean():.3f}\")\n",
    "print(f\"Min confidence: {confidence['dnf_confidence'].min():.3f}\")\n",
    "print(f\"Max confidence: {confidence['dnf_confidence'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features of the Betting Market Models:\n",
    "\n",
    "1. **Ordinal Regression**: Properly handles the ordered nature of finishing positions\n",
    "\n",
    "2. **Head-to-Head Predictions**: Direct driver comparison probabilities for matchup betting\n",
    "\n",
    "3. **DNF Risk Assessment**: Calibrated probabilities for non-finish outcomes\n",
    "\n",
    "4. **Points Scoring Model**: Multi-class prediction for different points brackets\n",
    "\n",
    "5. **Probability Calibration**: Ensures predicted probabilities match actual frequencies\n",
    "\n",
    "6. **Integrated Odds Generation**: Combines all models to produce comprehensive betting odds\n",
    "\n",
    "7. **Uncertainty Quantification**: Provides confidence intervals for predictions\n",
    "\n",
    "These models provide the foundation for a professional betting platform with accurate, calibrated probabilities suitable for odds generation and risk management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
